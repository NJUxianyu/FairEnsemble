{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32b2ef04814b284c19f71e623e98022efd322732"
   },
   "source": [
    "# Adult Census Income Analysis - Decision TREE, Random Forest, CV, Tuning the model with Ensemble Techniques(Baaging , ADAboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7462e6028fbfac80717afb34961109db9f1d26b4"
   },
   "source": [
    "### A stable and optimized model to predict the income of a given population, which is labelled as <= 50K and >50K. The attributes (predictors) are age, working class type, marital status, gender, race etc.\n",
    "#### Following are the steps, \n",
    "#### 1.clean and prepare the data,\n",
    "#### 2.Analyze Data,\n",
    "#### 3.Label Encoding,\n",
    "#### 4.Build a decision tree and Random forest with default hyperparameters,\n",
    "#### 5.Build several classifier models to compare, cross validate and for voting classifier model\n",
    "#### 6.choose the optimal hyperparameters using grid search cross-validation.\n",
    "#### 7.Build optimized Random forest model with tuned hyperparameters from grid search model\n",
    "#### 8.Increase Accuracy by Applying Ensemble technique BAGGING to our tuned random forest model\n",
    "#### 9.Increase Accuracy by Applying Ensemble technique ADABOOST to our tuned random forest model\n",
    "####  I hope you enjoy this notebook and find it useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce6da2377aff6dd18a2a76b7d0b67e732e00bfcb"
   },
   "source": [
    "## Clean & Analyze Data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7361e3b94dfc782419e36e24e47650f6bee7acbf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "import matplotlib.patches as patches\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "#from packages import *\n",
    "#from ml_fairness import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "39f4cd7d56fe0b06e47bfc6cf2c950feaad7e8e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ffa2817a5eb614c43310643022ddbd8a7afa8d42"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "04d567bee2cf3de195c43c3a9ecae60f33b93f38"
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv(\"../../Data/adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0ef0457d40b0333ae9687a364eb00ea6f386bf67"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90         ?   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "2   66         ?  186061  Some-college             10        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "0                  ?  Not-in-family  White  Female             0   \n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                  ?      Unmarried  Black  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "0          4356              40  United-States  <=50K  \n",
       "1          4356              18  United-States  <=50K  \n",
       "2          4356              40  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "59ecb2faafed022b16c91590939620ae684e868f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education.num   32561 non-null  int64 \n",
      " 5   marital.status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital.gain    32561 non-null  int64 \n",
      " 11  capital.loss    32561 non-null  int64 \n",
      " 12  hours.per.week  32561 non-null  int64 \n",
      " 13  native.country  32561 non-null  object\n",
      " 14  income          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a62594b2ac936da01d921ac9ee380394803c7e60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education.num     0\n",
       "marital.status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital.gain      0\n",
       "capital.loss      0\n",
       "hours.per.week    0\n",
       "native.country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "de0c8f4d41821ac89947f1ce11c69bf410424321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workclass         1836\n",
       "education            0\n",
       "marital.status       0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "native.country     583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select all categorical variables\n",
    "df_categorical = data.select_dtypes(include=['object'])\n",
    "\n",
    "# checking whether any other columns contain a \"?\"\n",
    "df_categorical.apply(lambda x: x==\"?\", axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "801d05ea01a07ece2c0bbe4b7f95160e9fbefbc3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               1836\n",
       "workclass         1836\n",
       "fnlwgt            1836\n",
       "education         1836\n",
       "education.num     1836\n",
       "marital.status    1836\n",
       "occupation        1836\n",
       "relationship      1836\n",
       "race              1836\n",
       "sex               1836\n",
       "capital.gain      1836\n",
       "capital.loss      1836\n",
       "hours.per.week    1836\n",
       "native.country    1836\n",
       "income            1836\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['workclass'] == '?' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4a990ca11cc0710e10062a60b16781df33255cb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               1843\n",
       "workclass         1843\n",
       "fnlwgt            1843\n",
       "education         1843\n",
       "education.num     1843\n",
       "marital.status    1843\n",
       "occupation        1843\n",
       "relationship      1843\n",
       "race              1843\n",
       "sex               1843\n",
       "capital.gain      1843\n",
       "capital.loss      1843\n",
       "hours.per.week    1843\n",
       "native.country    1843\n",
       "income            1843\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['occupation'] == '?' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "3141dc44e649ffc20f10424b190ac9c8d6f88b7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               583\n",
       "workclass         583\n",
       "fnlwgt            583\n",
       "education         583\n",
       "education.num     583\n",
       "marital.status    583\n",
       "occupation        583\n",
       "relationship      583\n",
       "race              583\n",
       "sex               583\n",
       "capital.gain      583\n",
       "capital.loss      583\n",
       "hours.per.week    583\n",
       "native.country    583\n",
       "income            583\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['native.country'] == '?' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a6ed9e88a758c17b6aa9e2dd8d472fd09f9dac26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005638647461687295"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1836/32561)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51a8e4950eb27e6854f8cf9c6d83f212a1d83b2"
   },
   "source": [
    " ### Missing Value % is very insignificant  so we will drop those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "539c79a31706c11cc52d70f431401e52d896dc68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               32561\n",
       "workclass         32561\n",
       "fnlwgt            32561\n",
       "education         32561\n",
       "education.num     32561\n",
       "marital.status    32561\n",
       "occupation        32561\n",
       "relationship      32561\n",
       "race              32561\n",
       "sex               32561\n",
       "capital.gain      32561\n",
       "capital.loss      32561\n",
       "hours.per.week    32561\n",
       "native.country    32561\n",
       "income            32561\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3429d7ada61be066e494c6acd19cffdbfa035e29"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"workclass\"] != \"?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1349aa0ed7099010bfd8e96ffbdb858d2277ea9f"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"occupation\"] != \"?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "8a0b26a32092dd60ce36765d66eb2f5ed68f76c9"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"native.country\"] != \"?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4d67c1faab29f37a890aaa4b8d69a6d1524fe964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               30162\n",
       "workclass         30162\n",
       "fnlwgt            30162\n",
       "education         30162\n",
       "education.num     30162\n",
       "marital.status    30162\n",
       "occupation        30162\n",
       "relationship      30162\n",
       "race              30162\n",
       "sex               30162\n",
       "capital.gain      30162\n",
       "capital.loss      30162\n",
       "hours.per.week    30162\n",
       "native.country    30162\n",
       "income            30162\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "377e5040e9ec46e875f1648a388dda15a282b15e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "5   34   Private  216864       HS-grad              9       Divorced   \n",
       "6   38   Private  150601          10th              6      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "5      Other-service      Unmarried  White  Female             0   \n",
       "6       Adm-clerical      Unmarried  White    Male             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "1          4356              18  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  \n",
       "5          3770              45  United-States  <=50K  \n",
       "6          3770              40  United-States  <=50K  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "8973966358496bc69ded01402417e797e9a8abec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<=50K', '>50K'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"income\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "71ead6f769e3b2412645c5446f9607c176beca4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "5   34   Private  216864       HS-grad              9       Divorced   \n",
       "6   38   Private  150601          10th              6      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "5      Other-service      Unmarried  White  Female             0   \n",
       "6       Adm-clerical      Unmarried  White    Male             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country  income  \n",
       "1          4356              18  United-States       0  \n",
       "3          3900              40  United-States       0  \n",
       "4          3900              40  United-States       0  \n",
       "5          3770              45  United-States       0  \n",
       "6          3770              40  United-States       0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"income\"] = data[\"income\"].map({'<=50K' : 0, '>50K': 1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "8c3cd40f8cd1c4b1d6663f9029c645e2246718b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"income\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16c4b44756d205c485138c9e988e00099b56567c"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "8930628f17b35db2be301b97ac1e34acaa2688f5"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "catogorical_data = data.select_dtypes(include =['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ea714a2426659b7a2bdb7ee382f7a2843a8390c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Private</td>\n",
       "      <td>10th</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  workclass     education marital.status         occupation   relationship  \\\n",
       "1   Private       HS-grad        Widowed    Exec-managerial  Not-in-family   \n",
       "3   Private       7th-8th       Divorced  Machine-op-inspct      Unmarried   \n",
       "4   Private  Some-college      Separated     Prof-specialty      Own-child   \n",
       "5   Private       HS-grad       Divorced      Other-service      Unmarried   \n",
       "6   Private          10th      Separated       Adm-clerical      Unmarried   \n",
       "\n",
       "    race     sex native.country  \n",
       "1  White  Female  United-States  \n",
       "3  White  Female  United-States  \n",
       "4  White  Female  United-States  \n",
       "5  White  Female  United-States  \n",
       "6  White    Male  United-States  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catogorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "abc005c18d8499593ecbf512f18aabf5ffd8cfe8"
   },
   "outputs": [],
   "source": [
    "catogorical_data = catogorical_data.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "1e64759b9f1ad053ed3c27db18e262a289e41565"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass  education  marital.status  occupation  relationship  race  sex  \\\n",
       "1          2         11               6           3             1     4    0   \n",
       "3          2          5               0           6             4     4    0   \n",
       "4          2         15               5           9             3     4    0   \n",
       "5          2         11               0           7             4     4    0   \n",
       "6          2          0               5           0             4     4    1   \n",
       "\n",
       "   native.country  \n",
       "1              38  \n",
       "3              38  \n",
       "4              38  \n",
       "5              38  \n",
       "6              38  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catogorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "7a051dcff726e3ad29d40cf202ae20631aef2041"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>132870</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>140359</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>264663</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>216864</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>150601</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education.num  capital.gain  capital.loss  hours.per.week  \\\n",
       "1   82  132870              9             0          4356              18   \n",
       "3   54  140359              4             0          3900              40   \n",
       "4   41  264663             10             0          3900              40   \n",
       "5   34  216864              9             0          3770              45   \n",
       "6   38  150601              6             0          3770              40   \n",
       "\n",
       "   income  workclass  education  marital.status  occupation  relationship  \\\n",
       "1       0          2         11               6           3             1   \n",
       "3       0          2          5               0           6             4   \n",
       "4       0          2         15               5           9             3   \n",
       "5       0          2         11               0           7             4   \n",
       "6       0          2          0               5           0             4   \n",
       "\n",
       "   race  sex  native.country  \n",
       "1     4    0              38  \n",
       "3     4    0              38  \n",
       "4     4    0              38  \n",
       "5     4    0              38  \n",
       "6     4    1              38  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(catogorical_data.columns, axis=1)\n",
    "data = pd.concat([data, catogorical_data], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "0c387fad9e66ca47ac40bde78712ca65bf17f5c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30162 entries, 1 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   age             30162 non-null  int64\n",
      " 1   fnlwgt          30162 non-null  int64\n",
      " 2   education.num   30162 non-null  int64\n",
      " 3   capital.gain    30162 non-null  int64\n",
      " 4   capital.loss    30162 non-null  int64\n",
      " 5   hours.per.week  30162 non-null  int64\n",
      " 6   income          30162 non-null  int64\n",
      " 7   workclass       30162 non-null  int32\n",
      " 8   education       30162 non-null  int32\n",
      " 9   marital.status  30162 non-null  int32\n",
      " 10  occupation      30162 non-null  int32\n",
      " 11  relationship    30162 non-null  int32\n",
      " 12  race            30162 non-null  int32\n",
      " 13  sex             30162 non-null  int32\n",
      " 14  native.country  30162 non-null  int32\n",
      "dtypes: int32(8), int64(7)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "3c8381e875346a9cf9e71fc42a2d94763a73af22"
   },
   "outputs": [],
   "source": [
    "data['income'] = data['income'].astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bedbd44b9962d3c451c83333728b1785e435f046"
   },
   "source": [
    "## Decision Tree Model with Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "b1e9480c1274e73df82082ce231507a88bbf0dc6"
   },
   "outputs": [],
   "source": [
    "x=data.drop('income',axis=1)\n",
    "y=data['income']\n",
    "#Train & Test split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state= 476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "67fbd003d313f5b35823f3bdcca716c1a363579d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "model_tree = tree.fit(x_train,y_train)\n",
    "model_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "d3b0b401d7de935334802a6b40810e106fc9d0b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "4831f4b65f4bf77df61c12d363e2069b747093e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Desicion Tree is  0.8125759752458835\n"
     ]
    }
   ],
   "source": [
    "model_tree = tree.fit(x_train,y_train)\n",
    "pred_tree = tree.predict(x_test)\n",
    "a1 = accuracy_score(y_test,pred_tree)\n",
    "print(\"The Accuracy of Desicion Tree is \", a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "2dca916976e4efa778d2caeb647bc79c303b6942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5931,  857],\n",
       "       [ 839, 1422]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "cb90ece9466f7d446f9921aab016c29d1783e6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      6788\n",
      "           1       0.62      0.63      0.63      2261\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      9049\n",
      "   macro avg       0.75      0.75      0.75      9049\n",
      "weighted avg       0.81      0.81      0.81      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5f89d14d398ea7ea5a4f469525466c98a76cd3"
   },
   "source": [
    "## Random Forest Model with Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "598c1a77316eb07c6995f05f33208f33bf7cfce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Random Forest is  0.8450657531218919\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "model_rf = rf.fit(x_train,y_train)\n",
    "pred_rf = rf.predict(x_test)\n",
    "a2 = accuracy_score(y_test, pred_rf)\n",
    "print(\"The Accuracy of Random Forest is \", a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50a9d2bb57b7b8803b1bd7c2068192cdda0831e9"
   },
   "source": [
    "## Logistic Regression & KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "c03bf10d54caac55aa511ab556570c4c0f8469c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of logistic regression is  0.7911371422256603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression()\n",
    "\n",
    "model_lg = lg.fit(x_train,y_train)\n",
    "pred_lg = lg.predict(x_test)\n",
    "a3 = accuracy_score(y_test, pred_lg)\n",
    "print(\"The Accuracy of logistic regression is \", a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "2afe0c31705ea99b3df342921835417e91a969db"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "9b20127aa6de02eaaa5428d43e1f65f3a0b43b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of KNN is  0.7596419493866725\n"
     ]
    }
   ],
   "source": [
    "model_knn =knn.fit(x_train,y_train) \n",
    "pred_knn = knn.predict(x_test)\n",
    "a4 = accuracy_score(y_test, pred_knn)\n",
    "print(\"The Accuracy of KNN is \", a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "320ee22f6847ce98263b9309c12ed6b17ad1242d"
   },
   "source": [
    "# Build optimized Random forest model with tuned hyperparameters from grid search model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "c13ef3808997e6aa926a7a2ab95720f57dddc568"
   },
   "outputs": [],
   "source": [
    "rf_param = {\n",
    "    \"n_estimators\": [25,50,100],\n",
    "    \"criterion\" : [\"gini\"],\n",
    "    \"max_depth\" : [3,4,5,6],\n",
    "    \"max_features\" : [\"auto\",\"sqrt\",\"log2\"],\n",
    "    \"random_state\" : [123]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "d4a63e0033c293440051ee7f9f812bf48f3e32af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [25, 50, 100], 'criterion': ['gini'], 'max_depth': [3, 4, 5, 6], 'max_features': ['auto', 'sqrt', 'log2'], 'random_state': [123]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV(rf, rf_param, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "10d2067afaf7fb2c7cb4e6740db81238ad57b404",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid =GridSearchCV(rf, rf_param, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "920bc6d90d4aa68336554be02cfc3789d989bebd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 6,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 123}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "e7fa9c64023d976755eac8cb079da333af82c1f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462813570560282"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(criterion = 'gini',\n",
    "    max_depth = 6,\n",
    "    max_features = 'auto',\n",
    "    n_estimators = 100,\n",
    "    random_state = 123)\n",
    "model_rf1 = rf1.fit(x_train,y_train)\n",
    "pred_rf1 = rf1.predict(x_test)\n",
    "accuracy_score(y_test, pred_rf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f6d64d06ab439aaa9bb16ad7bd1c6e62abcafec"
   },
   "source": [
    "# K FOLD Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "2bcb8778b3908d94a6ecb9ece61c07b93abdaba6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79024621, 0.81628788, 0.79498106, 0.79403409, 0.79971591,\n",
       "       0.80776515, 0.80672667, 0.79952607, 0.79905213, 0.80805687])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(tree,x_train,y_train,scoring= \"accuracy\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "68c89d69d71c63fbdf9cad5fd34ff8359c66662b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7639760250539661"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(tree,x,y,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "46fedd8d9cc8583275a1f49f146046728d371b79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446926761990472"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf,x_train,y_train,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "bd39a51661a03879fae3bd0d3316aab80a7c2060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881883610197511"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lg,x_train,y_train,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "7ee7c85805c50b27bf9f2291f46316fb63c95fe5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642686401045582"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn,x_train,y_train,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "515e5f5c9181d45e6f820b518acc5c768fa5c26b"
   },
   "source": [
    "# Voting Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "4f48f73c49a84ab59bb66791099166b2c4fffbca"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "e2a36648b67aa8ccd0a45822b8aa6774b2e4ade3"
   },
   "outputs": [],
   "source": [
    "model_vote = VotingClassifier(estimators=[('logistic Regression', lg), ('random forrest', rf), ('knn neighbors', knn),(' decision tree', tree)], voting='soft')\n",
    "model_vote = model_vote.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "4bdb63d97ba527a4549cd03883b8654802563b13"
   },
   "outputs": [],
   "source": [
    "vote_pred = model_vote.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "fcfde9016c98a7d7d8130109c074b954786fd470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of voting classifier is  0.8371090728257266\n"
     ]
    }
   ],
   "source": [
    "a5 =  accuracy_score(y_test, vote_pred)\n",
    "print(\"The Accuracy of voting classifier is \", a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "dedbd45285846adc5ee7dcc35b148cb3585b6e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90      6788\n",
      "           1       0.77      0.50      0.60      2261\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      9049\n",
      "   macro avg       0.81      0.72      0.75      9049\n",
      "weighted avg       0.83      0.84      0.82      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, vote_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56a0b7324ac27e7c747f89896ba8c50421f46b2e"
   },
   "source": [
    "# Ensemble Technique Bagging \n",
    "\n",
    "## Increase Accuracy by Applying Ensemble technique BAGGING to our tuned random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "15f7a81d545e25f18e5b6e9d38ef53c1601b4439"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "d29bd47d5b793d5bb732954126d65ca31ce14edb"
   },
   "outputs": [],
   "source": [
    "bagg = BaggingClassifier(base_estimator=rf1,n_estimators=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "a7e594bd915aa5ac7ddb06d02045f7ce841b239c"
   },
   "outputs": [],
   "source": [
    "model_bagg =bagg.fit(x_train,y_train) \n",
    "pred_bagg = bagg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "3c87107b23c77823ce9b7591133614ff6274a766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of BAAGING is  0.8460603381589126\n"
     ]
    }
   ],
   "source": [
    "a6 = accuracy_score(y_test, pred_bagg)\n",
    "print(\"The Accuracy of BAAGING is \", a6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "d40d7bffcf4b44472558546c3c4ca93f56d4a96b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6496,  292],\n",
       "       [1101, 1160]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_bagg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "e9aa7155dd006dbd1638cb0603af0600978de85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90      6788\n",
      "           1       0.80      0.51      0.62      2261\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      9049\n",
      "   macro avg       0.83      0.74      0.76      9049\n",
      "weighted avg       0.84      0.85      0.83      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_bagg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7a3462fa71e660beba1c3bcddfa8773cba1f33d"
   },
   "source": [
    "#  Ensemble Technique  ADA Boost \n",
    "\n",
    "## Increase Accuracy by Applying Ensemble technique ADABOOST to our tuned random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "223838ecb17eea19d3e9cfef7e334546b60367ff"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "024d359c921869720b89f046d985b1afc1b7ad09"
   },
   "outputs": [],
   "source": [
    "Adaboost = AdaBoostClassifier(base_estimator=rf1, n_estimators=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "41e5ecc6cd4ba3e6f9a29a3f48afd4868742503f"
   },
   "outputs": [],
   "source": [
    "model_boost =Adaboost.fit(x_train,y_train) \n",
    "pred_boost = Adaboost.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "116a125308885136f39085cdc52631ce2147f0f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of BOOSTING is  0.8660625483478838\n"
     ]
    }
   ],
   "source": [
    "a7 = accuracy_score(y_test, pred_boost)\n",
    "print(\"The Accuracy of BOOSTING is \", a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "c0d15b3e40194d2d3e9589dc4d30bbae36c8c032"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6393,  395],\n",
       "       [ 817, 1444]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "5bea2ff5ef457d44de88fc5f2478251c5647f23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91      6788\n",
      "           1       0.79      0.64      0.70      2261\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      9049\n",
      "   macro avg       0.84      0.79      0.81      9049\n",
      "weighted avg       0.86      0.87      0.86      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_boost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5050773eabbd35cac48c952e1c597707cb8de8af"
   },
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
    "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
    "\n",
    "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
    "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:\n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "    \n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4,0.25]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8,0]\n",
    "    bottom = [-1,-1,-1,0,0]\n",
    "    top = [1,1,1,2,1]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fair_metrics_and_plot(data, model, plot=False, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    # fair_metrics function available in the metrics.py file\n",
    "    fair = fair_metrics(data, pred)\n",
    "\n",
    "    if plot:\n",
    "        # plot_fair_metrics function available in the visualisations.py file\n",
    "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
    "        plot_fair_metrics(fair)\n",
    "        display(fair)\n",
    "    \n",
    "    return fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>132870</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>140359</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>264663</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>216864</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>150601</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>310152</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>40</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30162 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education.num  capital.gain  capital.loss  hours.per.week  \\\n",
       "1       82  132870              9             0          4356              18   \n",
       "3       54  140359              4             0          3900              40   \n",
       "4       41  264663             10             0          3900              40   \n",
       "5       34  216864              9             0          3770              45   \n",
       "6       38  150601              6             0          3770              40   \n",
       "...    ...     ...            ...           ...           ...             ...   \n",
       "32556   22  310152             10             0             0              40   \n",
       "32557   27  257302             12             0             0              38   \n",
       "32558   40  154374              9             0             0              40   \n",
       "32559   58  151910              9             0             0              40   \n",
       "32560   22  201490              9             0             0              20   \n",
       "\n",
       "      income  workclass  education  marital.status  occupation  relationship  \\\n",
       "1          0          2         11               6           3             1   \n",
       "3          0          2          5               0           6             4   \n",
       "4          0          2         15               5           9             3   \n",
       "5          0          2         11               0           7             4   \n",
       "6          0          2          0               5           0             4   \n",
       "...      ...        ...        ...             ...         ...           ...   \n",
       "32556      0          2         15               4          10             1   \n",
       "32557      0          2          7               2          12             5   \n",
       "32558      1          2         11               2           6             0   \n",
       "32559      0          2         11               6           0             4   \n",
       "32560      0          2         11               4           0             3   \n",
       "\n",
       "       race  sex  native.country  \n",
       "1         4    0              38  \n",
       "3         4    0              38  \n",
       "4         4    0              38  \n",
       "5         4    0              38  \n",
       "6         4    1              38  \n",
       "...     ...  ...             ...  \n",
       "32556     4    1              38  \n",
       "32557     4    0              38  \n",
       "32558     4    1              38  \n",
       "32559     4    0              38  \n",
       "32560     4    1              38  \n",
       "\n",
       "[30162 rows x 15 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "dataset_orig = StandardDataset(data,\n",
    "                                  label_name='income',\n",
    "                                  protected_attribute_names=['sex'],\n",
    "                                  favorable_classes=[1],\n",
    "                                  privileged_classes=[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.200159\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train = pd.read_pickle('../../Results/VotingClassifier/4-prediction-ensemble-methods-multiple-ml-s-acc-86_Train.pkl')\n",
    "data_orig_test = pd.read_pickle('../../Results/VotingClassifier/4-prediction-ensemble-methods-multiple-ml-s-acc-86_Test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "nb_path = ipynbname.path()\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "import pickle\n",
    "\n",
    "#data_orig_train, data_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "X_train = data_orig_train.features\n",
    "y_train = data_orig_train.labels.ravel()\n",
    "\n",
    "X_test = data_orig_test.features\n",
    "y_test = data_orig_test.labels.ravel()\n",
    "\n",
    "\n",
    "lg = LogisticRegression()\n",
    "rf = RandomForestClassifier(criterion = 'gini',\n",
    "    max_depth = 6,\n",
    "    max_features = 'auto',\n",
    "    n_estimators = 100,\n",
    "    random_state = 123)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "tree = DecisionTreeClassifier(random_state = 123)\n",
    "\n",
    "\n",
    "estimators = [('logistic Regression', lg), ('random forrest', rf), ('knn neighbors', knn),(' decision tree', tree)]\n",
    "\n",
    "\n",
    "votingC = VotingClassifier(estimators=[('logistic Regression', lg), ('random forrest', rf), ('knn neighbors', knn),(' decision tree', tree)], voting='soft')\n",
    "\n",
    "\n",
    "model = votingC.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "mdl = model.fit(X_train, y_train)\n",
    "#with open('../../Results/VotingClassifier/' + nb_fname + '.pkl', 'wb') as f:\n",
    "#    pickle.dump(mdl, f)\n",
    "\n",
    "#with open('../../Results/VotingClassifier/' + nb_fname + '_Train' + '.pkl', 'wb') as f:\n",
    "#    pickle.dump(data_orig_train, f) \n",
    "    \n",
    "#with open('../../Results/VotingClassifier/' + nb_fname + '_Test' + '.pkl', 'wb') as f:\n",
    "#    pickle.dump(data_orig_test, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "final_metrics = []\n",
    "accuracy = []\n",
    "list_estimators = []\n",
    "f1= []\n",
    "\n",
    "for name, ml_model in estimators:\n",
    "    \n",
    "    list_estimators.append((name,ml_model))\n",
    "    #print(list_estimators)\n",
    "    model = VotingClassifier(estimators=list_estimators, voting='hard', flatten_transform=False)\n",
    "    #list_estimators = []\n",
    "    mdl = model.fit(X_train, y_train)\n",
    "    yy = mdl.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, yy))\n",
    "    f1.append(f1_score(y_test, yy))\n",
    "    fair = get_fair_metrics_and_plot(data_orig_test, mdl)        \n",
    "    fair_list = fair.iloc[1].tolist()\n",
    "    #fair_list.insert(0, i)\n",
    "    final_metrics.append(fair_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy_soft = yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_hard = yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558\n"
     ]
    }
   ],
   "source": [
    "print(len([i for i, (x, y) in enumerate(zip(yy_soft, yy_hard)) if x != y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92 0.95 0.82 0.92 0.77 0.84 0.75 0.97 0.96 0.5  0.76 0.4  0.98 0.82\n",
      " 0.53 0.06 0.97 0.58 0.91 0.9  0.91 0.84 0.56 0.9  0.71 0.91 0.91 0.96\n",
      " 0.98 0.85 0.76 0.83 0.56 0.44 0.59 0.95 0.95 0.16 0.82 0.91 0.91 0.91\n",
      " 0.69 0.82 0.03 0.79 0.96 0.74 0.91 0.88 0.87 0.57 0.82 0.96 0.76 0.52\n",
      " 0.97 0.88 0.8  0.88 0.92 0.84 0.92 0.57 0.02 0.92 0.79 0.92 0.08 0.84\n",
      " 0.88 0.91 0.86 0.71 0.79 0.87 0.91 0.89 0.97 0.85 0.88 0.9  0.62 0.7\n",
      " 0.82 0.85 0.54 0.5  0.97 0.91 0.55 0.84 0.87 0.88 0.85 0.97 0.87 0.89\n",
      " 0.81 0.56 0.92 0.54 0.96 0.89 0.74 0.97 0.81 0.9  0.72 0.78 0.95 0.72\n",
      " 0.7  0.87 0.9  0.45 0.03 0.38 0.66 0.84 0.78 0.83 0.88 0.89 0.65 0.84\n",
      " 0.7  0.87 0.48 0.95 0.43 0.96 0.94 0.34 0.92 0.93 0.85 0.9  0.84 0.75\n",
      " 0.68 0.93 0.85 0.88 0.28 0.78 0.84 0.55 0.46 0.53 0.89 0.8  0.9  0.54\n",
      " 0.94 0.84 0.67 0.91 0.86 0.71 0.53 0.83 0.6  0.8  0.84 0.93 0.96 0.96\n",
      " 0.87 0.93 0.9  0.8  0.57 0.03 0.86 0.93 0.69 0.95 0.62 0.78 0.89 0.95\n",
      " 0.87 0.8  0.86 0.78 0.55 0.91 0.81 0.97 0.02 0.9  0.96 0.61 0.96 0.87\n",
      " 0.98 0.51 0.69 0.88 0.86 0.27 0.91 0.86 0.44 0.75 0.08 0.7  0.58 0.69\n",
      " 0.96 0.35 0.87 0.56 0.82 0.86 0.51 0.15 0.05 0.89 0.93 0.48 0.98 0.93\n",
      " 0.96 0.96 0.84 0.93 0.89 0.81 0.66 0.66 0.56 0.88 0.86 0.07 0.91 0.38\n",
      " 0.96 0.79 0.93 0.72 0.84 0.97 0.82 0.88 0.62 0.95 0.82 0.17 0.92 0.85\n",
      " 0.74 0.72 0.81 0.93 0.86 0.6  0.92 0.26 0.93 0.89 0.79 0.84 0.77 0.52\n",
      " 0.91 0.57 0.93 0.64 0.96 0.56 0.76 0.8  0.35 0.84 0.53 0.85 0.5  0.79\n",
      " 0.87 0.93 0.87 0.43 0.86 0.59 0.1  0.83 0.91 0.88 0.89 0.96 0.94 0.58\n",
      " 0.86 0.6  0.86 0.79 0.74 0.84 0.04 0.92 0.98 0.61 0.78 0.85 0.91 0.88\n",
      " 0.9  0.96 0.2  0.55 0.81 0.92 0.69 0.89 0.82 0.9  0.77 0.41 0.04 0.85\n",
      " 0.9  0.9  0.87 0.21 0.86 0.47 0.71 0.72 0.57 0.05 0.86 0.97 0.96 0.8\n",
      " 0.77 0.87 0.92 0.86 0.95 0.9  0.66 0.87 0.91 0.95 0.47 0.79 0.86 0.88\n",
      " 0.95 0.48 0.89 0.97 0.59 0.05 0.85 0.9  0.93 0.76 0.76 0.89 0.68 0.72\n",
      " 0.86 0.79 0.91 0.85 0.44 0.81 0.91 0.03 0.78 0.27 0.84 0.58 0.91 0.41\n",
      " 0.95 0.76 0.82 0.92 0.04 0.92 0.92 0.84 0.82 0.89 0.42 0.63 0.83 0.95\n",
      " 0.78 0.26 0.92 0.89 0.23 0.84 0.84 0.96 0.54 0.62 0.92 0.78 0.86 0.87\n",
      " 0.69 0.9  0.89 0.55 0.89 0.6  0.9  0.81 0.09 0.83 0.72 0.69 0.76 0.48\n",
      " 0.84 0.96 0.95 0.81 0.65 0.91 0.77 0.57 0.92 0.54 0.93 0.08 0.85 0.83\n",
      " 0.13 0.73 0.62 0.69 0.86 0.92 0.93 0.94 0.96 0.91 0.76 0.05 0.85 0.83\n",
      " 0.91 0.92 0.82 0.77 0.79 0.94 0.84 0.91 0.26 0.72 0.96 0.47 0.9  0.93\n",
      " 0.71 0.44 0.81 0.91 0.85 0.96 0.5  0.87 0.03 0.91 0.91 0.15 0.66 0.92\n",
      " 0.71 0.88 0.48 0.96 0.65 0.85 0.95 0.97 0.28 0.52 0.18 0.67 0.92 0.75\n",
      " 0.89 0.85 0.32 0.92 0.85 0.9  0.32 0.91 0.96 0.97 0.7  0.84 0.82 0.43\n",
      " 0.91 0.72 0.55 0.87 0.47 0.67 0.54 0.95 0.97 0.66 0.93 0.97 0.83 0.54\n",
      " 0.75 0.96 0.92 0.08 0.51 0.86 0.9  0.87 0.65 0.83 0.09 0.81 0.53 0.95\n",
      " 0.72 0.93 0.03 0.74 0.84 0.84 0.94 0.52 0.86 0.75 0.24 0.96 0.81 0.9\n",
      " 0.86 0.52 0.93 0.22 0.06 0.82 0.78 0.95 0.78 0.77 0.91 0.54 0.88 0.7\n",
      " 0.55 0.07 0.49 0.25 0.87 0.42 0.95 0.52 0.95 0.93 0.82 0.29 0.8  0.73\n",
      " 0.68 0.75 0.98 0.88 0.57 0.92 0.96 0.73 0.82 0.93 0.4  0.85 0.89 0.95\n",
      " 0.84 0.78 0.72 0.97 0.86 0.87 0.87 0.83 0.89 0.79 0.97 0.67 0.56 0.96\n",
      " 0.54 0.79 0.9  0.86 0.91 0.88 0.89 0.83 0.88 0.82 0.94 0.83 0.57 0.94\n",
      " 0.86 0.75 0.76 0.88 0.88 0.93 0.88 0.91 0.88 0.68 0.97 0.87 0.78 0.88\n",
      " 0.86 0.94 0.91 0.91 0.82 0.89 0.82 0.94 0.82 0.78 0.75 0.67 0.81 0.95\n",
      " 0.96 0.55 0.97 0.82 0.8  0.72 0.51 0.97 0.74 0.81 0.96 0.85 0.77 0.82\n",
      " 0.97 0.86 0.96 0.75 0.52 0.06 0.9  0.55 0.82 0.71 0.5  0.87 0.5  0.67\n",
      " 0.78 0.82 0.97 0.75 0.82 0.12 0.54 0.96 0.69 0.67 0.29 0.91 0.48 0.83\n",
      " 0.83 0.63 0.88 0.68 0.8  0.96 0.75 0.85 0.61 0.98 0.54 0.95 0.63 0.74\n",
      " 0.8  0.61 0.54 0.81 0.85 0.97 0.03 0.9  0.93 0.75 0.84 0.9  0.58 0.73\n",
      " 0.08 0.87 0.88 0.67 0.86 0.38 0.35 0.55 0.53 0.9  0.97 0.97 0.96 0.81\n",
      " 0.87 0.9  0.49 0.96 0.85 0.64 0.48 0.64 0.85 0.47 0.83 0.63 0.85 0.47\n",
      " 0.54 0.6  0.94 0.88 0.89 0.83 0.41 0.81 0.53 0.85 0.79 0.6  0.81 0.74\n",
      " 0.83 0.97 0.68 0.86 0.49 0.92 0.77 0.85 0.85 0.96 0.83 0.79 0.83 0.88\n",
      " 0.48 0.82 0.9  0.6  0.52 0.85 0.62 0.93 0.76 0.46 0.8  0.89 0.87 0.8\n",
      " 0.81 0.67 0.92 0.92 0.62 0.05 0.96 0.89 0.76 0.81 0.47 0.86 0.33 0.88\n",
      " 0.12 0.6  0.82 0.81 0.9  0.48 0.85 0.54 0.85 0.89 0.93 0.39 0.59 0.83\n",
      " 0.84 0.92 0.87 0.81 0.5  0.82 0.86 0.48 0.82 0.91 0.91 0.92 0.77 0.94\n",
      " 0.52 0.88 0.91 0.36 0.61 0.03 0.87 0.53 0.92 0.83 0.61 0.79 0.86 0.79\n",
      " 0.81 0.83 0.88 0.77 0.57 0.85 0.43 0.84 0.93 0.53 0.92 0.97 0.81 0.4\n",
      " 0.87 0.76 0.89 0.85 0.04 0.93 0.92 0.89 0.91 0.56 0.95 0.05 0.85 0.84\n",
      " 0.88 0.86 0.97 0.58 0.95 0.92 0.97 0.14 0.77 0.89 0.5  0.69 0.88 0.9\n",
      " 0.8  0.84 0.93 0.3  0.86 0.96 0.05 0.77 0.71 0.86 0.91 0.92 0.93 0.96\n",
      " 0.92 0.55 0.86 0.92 0.96 0.97 0.91 0.55 0.94 0.47 0.92 0.95 0.74 0.91\n",
      " 0.9  0.86 0.74 0.56 0.89 0.98 0.91 0.52 0.97 0.8  0.87 0.68 0.91 0.89\n",
      " 0.31 0.63 0.58 0.11 0.88 0.83 0.51 0.9  0.56 0.76 0.32 0.9  0.9  0.93\n",
      " 0.66 0.88 0.1  0.82 0.81 0.64 0.64 0.89 0.51 0.77 0.82 0.98 0.94 0.91\n",
      " 0.08 0.88 0.78 0.55 0.92 0.9  0.86 0.89 0.8  0.49 0.61 0.54 0.02 0.83\n",
      " 0.58 0.35 0.91 0.35 0.6  0.78 0.72 0.58 0.98 0.73 0.93 0.85 0.87 0.86\n",
      " 0.53 0.43 0.46 0.91 0.91 0.43 0.81 0.87 0.96 0.91 0.88 0.96 0.94 0.88\n",
      " 0.44 0.32 0.81 0.86 0.8  0.84 0.82 0.76 0.81 0.85 0.71 0.8  0.96 0.94\n",
      " 0.65 0.97 0.06 0.98 0.88 0.9  0.89 0.07 0.72 0.91 0.87 0.9  0.85 0.96\n",
      " 0.44 0.86 0.52 0.96 0.97 0.95 0.72 0.03 0.44 0.97 0.78 0.93 0.13 0.82\n",
      " 0.07 0.53 0.05 0.54 0.87 0.85 0.96 0.7  0.89 0.87 0.86 0.8  0.64 0.67\n",
      " 0.43 0.96 0.59 0.58 0.83 0.83 0.95 0.72 0.89 0.29 0.97 0.8  0.8  0.96\n",
      " 0.58 0.83 0.62 0.89 0.78 0.55 0.92 0.58 0.84 0.42 0.59 0.41 0.48 0.52\n",
      " 0.85 0.92 0.87 0.83 0.6  0.92 0.52 0.88 0.33 0.84 0.58 0.82 0.94 0.58\n",
      " 0.21 0.92 0.63 0.85 0.93 0.73 0.8  0.8  0.95 0.84 0.83 0.8  0.1  0.91\n",
      " 0.84 0.77 0.78 0.86 0.74 0.86 0.82 0.89 0.83 0.8  0.79 0.83 0.05 0.87\n",
      " 0.92 0.84 0.93 0.8  0.9  0.93 0.48 0.96 0.61 0.92 0.5  0.74 0.87 0.69\n",
      " 0.86 0.85 0.87 0.93 0.51 0.66 0.68 0.87 0.88 0.67 0.85 0.8  0.93 0.74\n",
      " 0.8  0.14 0.54 0.87 0.61 0.76 0.87 0.87 0.9  0.75 0.94 0.48 0.88 0.89\n",
      " 0.51 0.77 0.88 0.79 0.95 0.79 0.86 0.68 0.96 0.92 0.7  0.86 0.97 0.84\n",
      " 0.95 0.96 0.89 0.88 0.83 0.81 0.89 0.29 0.38 0.95 0.8  0.84 0.63 0.01\n",
      " 0.91 0.15 0.79 0.97 0.6  0.61 0.47 0.65 0.45 0.53 0.52 0.62 0.05 0.8\n",
      " 0.76 0.92 0.8  0.87 0.6  0.87 0.64 0.79 0.85 0.9  0.47 0.89 0.85 0.91\n",
      " 0.88 0.84 0.78 0.74 0.49 0.91 0.09 0.86 0.85 0.5  0.76 0.81 0.82 0.76\n",
      " 0.01 0.97 0.04 0.92 0.81 0.78 0.89 0.41 0.94 0.79 0.2  0.83 0.91 0.89\n",
      " 0.45 0.82 0.84 0.94 0.91 0.53 0.96 0.79 0.49 0.91 0.89 0.41 0.92 0.78\n",
      " 0.9  0.89 0.84 0.84 0.44 0.97 0.98 0.92 0.85 0.85 0.97 0.83 0.92 0.9\n",
      " 0.57 0.89 0.79 0.88 0.85 0.66 0.81 0.84 0.94 0.72 0.87 0.96 0.48 0.96\n",
      " 0.11 0.12 0.09 0.8  0.63 0.9  0.95 0.88 0.82 0.72 0.47 0.62 0.67 0.84\n",
      " 0.83 0.92 0.79 0.52 0.89 0.45 0.05 0.02 0.98 0.87 0.82 0.71 0.42 0.88\n",
      " 0.93 0.97 0.56 0.87 0.93 0.97 0.77 0.46 0.88 0.89 0.97 0.91 0.85 0.58\n",
      " 0.59 0.6  0.93 0.58 0.84 0.2  0.97 0.78 0.89 0.42 0.97 0.32 0.04 0.81\n",
      " 0.94 0.86 0.24 0.91 0.66 0.53 0.09 0.85 0.9  0.92 0.91 0.86 0.89 0.96\n",
      " 0.91 0.68 0.24 0.7  0.82 0.78 0.78 0.54 0.74 0.95 0.53 0.81 0.79 0.79\n",
      " 0.91 0.84 0.61 0.78 0.08 0.92 0.42 0.87 0.63 0.88 0.54 0.04 0.91 0.98\n",
      " 0.74 0.75 0.89 0.53 0.92 0.45 0.94 0.73 0.87 0.53 0.79 0.9  0.24 0.96\n",
      " 0.82 0.76 0.11 0.81 0.51 0.88 0.45 0.91 0.74 0.86 0.6  0.06 0.81 0.78\n",
      " 0.88 0.96 0.75 0.94 0.54 0.93 0.84 0.87 0.91 0.5  0.93 0.58 0.5  0.55\n",
      " 0.55 0.85 0.82 0.89 0.93 0.6  0.86 0.95 0.88 0.55 0.86 0.48 0.47 0.9\n",
      " 0.79 0.68 0.81 0.88 0.49 0.6  0.88 0.86 0.08 0.94 0.85 0.28 0.95 0.87\n",
      " 0.92 0.54 0.4  0.9  0.82 0.73 0.78 0.83 0.96 0.52 0.95 0.75 0.94 0.39\n",
      " 0.45 0.78 0.59 0.43 0.78 0.86 0.91 0.49 0.9  0.94 0.86 0.78 0.9  0.78\n",
      " 0.9  0.95 0.96 0.55 0.87 0.9  0.82 0.87 0.63 0.67 0.89 0.95 0.91 0.58\n",
      " 0.78 0.84 0.77 0.85 0.81 0.85 0.84 0.87 0.9  0.75 0.51 0.45 0.87 0.87\n",
      " 0.87 0.68 0.72 0.93 0.88 0.48 0.88 0.91 0.57 0.81 0.87 0.7  0.81 0.91\n",
      " 0.92 0.89 0.87 0.67 0.93 0.93 0.91 0.2  0.73 0.85 0.08 0.88 0.46 0.87\n",
      " 0.55 0.55 0.91 0.88 0.96 0.96 0.93 0.82 0.84 0.82 0.63 0.87 0.96 0.86\n",
      " 0.86 0.6  0.85 0.94 0.45 0.62 0.98 0.61 0.07 0.92 0.97 0.88 0.87 0.61\n",
      " 0.79 0.78 0.9  0.49 0.82 0.91 0.95 0.79 0.87 0.92 0.68 0.89 0.8  0.97\n",
      " 0.9  0.8  0.92 0.57 0.87 0.88 0.81 0.83 0.94 0.79 0.87 0.79 0.56 0.6\n",
      " 0.84 0.88 0.92 0.03 0.74 0.81 0.9  0.93 0.79 0.93 0.9  0.54 0.56 0.87\n",
      " 0.95 0.41 0.83 0.96 0.88 0.04 0.81 0.9  0.8  0.92 0.92 0.5  0.82 0.58\n",
      " 0.85 0.56 0.13 0.44 0.75 0.88 0.94 0.83 0.87 0.89 0.17 0.86 0.55 0.86\n",
      " 0.87 0.93 0.88 0.83 0.93 0.88 0.83 0.84 0.83 0.88 0.94 0.83 0.76 0.87\n",
      " 0.98 0.28 0.88 0.73 0.84 0.91 0.93 0.85 0.32 0.2  0.9  0.15 0.33 0.91\n",
      " 0.77 0.74 0.87 0.95 0.51 0.86 0.92 0.55 0.73 0.82 0.89 0.42 0.8  0.85\n",
      " 0.77 0.27 0.92 0.91 0.86 0.94 0.95 0.87 0.63 0.81 0.88 0.97 0.54 0.18\n",
      " 0.89 0.96 0.92 0.91 0.07 0.96 0.43 0.31 0.95 0.47 0.97 0.84 0.97 0.74\n",
      " 0.83 0.78 0.87 0.91 0.86 0.68 0.62 0.9  0.88 0.73 0.65 0.96 0.82 0.73\n",
      " 0.76 0.95 0.88 0.79 0.96 0.73 0.83 0.17 0.47 0.9  0.44 0.97 0.56 0.4\n",
      " 0.86 0.86 0.9  0.91 0.95 0.3  0.87 0.94 0.84 0.55 0.45 0.47 0.65 0.65\n",
      " 0.61 0.96 0.7  0.18 0.48 0.82 0.2  0.88 0.69 0.97 0.57 0.09 0.55 0.74\n",
      " 0.8  0.8  0.92 0.83 0.61 0.9  0.88 0.62 0.28 0.91 0.78 0.85 0.84 0.87\n",
      " 0.34 0.95 0.97 0.97 0.92 0.89 0.96 0.88 0.49 0.88 0.13 0.93 0.94 0.9\n",
      " 0.87 0.86 0.89 0.9  0.82 0.84 0.84 0.43 0.25 0.71 0.95 0.45 0.93 0.53\n",
      " 0.74 0.11 0.52 0.75 0.78 0.88 0.79 0.47 0.97 0.86 0.9  0.9  0.86 0.9\n",
      " 0.49 0.88 0.82 0.71 0.88 0.97 0.17 0.87 0.84 0.94 0.97 0.73 0.72 0.83\n",
      " 0.84 0.59 0.98 0.87 0.87 0.9  0.92 0.52 0.97 0.57 0.87 0.85 0.08 0.88\n",
      " 0.62 0.65 0.89 0.57 0.85 0.87 0.93 0.07 0.47 0.98 0.58 0.8  0.87 0.91\n",
      " 0.95 0.88 0.88 0.92 0.91 0.6  0.95 0.83 0.51 0.91 0.37 0.85 0.97 0.75\n",
      " 0.92 0.66 0.87 0.48 0.95 0.96 0.55 0.95 0.76 0.77 0.87 0.95 0.86 0.53\n",
      " 0.84 0.39 0.9  0.87 0.5  0.87 0.93 0.04 0.87 0.91 0.81 0.81 0.8  0.81\n",
      " 0.81 0.87 0.81 0.51 0.45 0.98 0.97 0.53 0.93 0.39 0.53 0.84 0.49 0.9\n",
      " 0.27 0.86 0.93 0.92 0.87 0.93 0.78 0.8  0.89 0.96 0.46 0.8  0.72 0.89\n",
      " 0.87 0.75 0.86 0.95 0.77 0.51 0.94 0.85 0.82 0.77 0.42 0.95 0.76 0.96\n",
      " 0.95 0.85 0.76 0.85 0.7  0.86 0.84 0.79 0.86 0.82 0.69 0.89 0.81 0.9\n",
      " 0.69 0.95 0.85 0.03 0.83 0.8  0.89 0.47 0.83 0.3  0.49 0.92 0.83 0.92\n",
      " 0.91 0.8  0.89 0.87 0.95 0.6  0.91 0.64 0.93 0.91 0.86 0.53 0.92 0.02\n",
      " 0.87 0.8  0.81 0.77 0.56 0.18 0.61 0.92 0.64 0.93 0.53 0.64 0.6  0.9\n",
      " 0.95 0.88 0.93 0.89 0.78 0.68 0.11 0.88 0.82 0.96 0.38 0.97 0.66 0.85\n",
      " 0.05 0.52 0.61 0.63 0.78 0.88 0.71 0.9  0.86 0.82 0.05 0.77 0.16 0.73\n",
      " 0.76 0.55 0.49 0.84 0.93 0.52 0.97 0.64 0.96 0.75 0.88 0.18 0.91 0.83\n",
      " 0.76 0.93 0.86 0.92 0.75 0.89 0.43 0.08 0.87 0.92 0.81 0.33 0.65 0.54\n",
      " 0.9  0.97 0.45 0.95 0.95 0.66 0.92 0.9  0.81 0.6  0.93 0.79 0.51 0.86\n",
      " 0.81 0.86 0.85 0.56 0.82 0.55 0.97 0.55 0.59 0.79 0.97 0.88 0.94 0.96\n",
      " 0.86 0.79 0.94 0.79 0.4  0.96 0.55 0.77 0.63 0.88 0.95 0.87 0.94 0.92\n",
      " 0.83 0.91 0.61 0.06 0.76 0.71 0.58 0.76 0.48 0.73 0.4  0.5  0.86 0.89\n",
      " 0.56 0.78 0.63 0.82 0.96 0.92 0.87 0.97 0.85 0.53 0.89 0.8  0.84 0.49\n",
      " 0.49 0.97 0.92 0.88 0.43 0.92 0.56 0.96 0.91 0.52 0.86 0.9  0.84 0.97\n",
      " 0.7  0.92 0.93 0.87 0.86 0.92 0.85 0.92 0.93 0.91 0.84 0.58 0.07 0.89\n",
      " 0.07 0.96 0.89 0.87 0.7  0.42 0.79 0.96 0.89 0.09 0.78 0.93 0.9  0.84\n",
      " 0.46 0.52 0.96 0.55 0.83 0.48 0.93 0.98 0.62 0.89 0.39 0.7  0.88 0.56\n",
      " 0.77 0.47 0.35 0.6  0.87 0.41 0.56 0.67 0.51 0.46 0.86 0.6  0.53 0.86\n",
      " 0.92 0.84 0.09 0.83 0.59 0.83 0.61 0.51 0.87 0.79 0.41 0.76 0.5  0.81\n",
      " 0.81 0.93 0.79 0.6  0.86 0.5  0.85 0.98 0.63 0.95 0.52 0.89 0.87 0.61\n",
      " 0.49 0.55 0.22 0.89 0.81 0.56 0.55 0.89 0.31 0.96 0.95 0.46 0.89 0.55\n",
      " 0.9  0.61 0.91 0.83 0.92 0.58 0.89 0.57 0.82 0.85 0.92 0.5  0.79 0.97\n",
      " 0.52 0.6  0.9  0.96 0.88 0.5  0.87 0.41 0.89 0.76 0.92 0.89 0.8  0.87\n",
      " 0.43 0.53 0.91 0.78 0.69 0.02 0.97 0.23 0.9  0.93 0.77 0.89 0.81 0.91\n",
      " 0.86 0.87 0.75 0.9  0.96 0.92 0.38 0.91 0.89 0.02 0.58 0.79 0.95 0.63\n",
      " 0.33 0.94 0.67 0.81 0.94 0.41 0.91 0.81 0.89 0.9  0.73 0.62 0.83 0.7\n",
      " 0.8  0.84 0.55 0.94 0.96 0.9  0.88 0.88 0.89 0.78 0.04 0.83 0.96 0.94\n",
      " 0.93 0.26 0.94 0.45 0.78 0.82 0.5  0.17 0.9  0.92 0.47 0.82 0.14 0.86\n",
      " 0.92 0.83 0.49 0.89 0.89 0.39 0.87 0.84 0.92 0.77 0.92 0.56 0.92 0.61\n",
      " 0.85 0.48 0.96 0.85 0.91 0.71 0.92 0.8  0.56 0.54 0.55 0.87 0.38 0.89\n",
      " 0.89 0.49 0.83 0.84 0.95 0.87 0.97 0.5  0.95 0.41 0.89 0.96 0.83 0.87\n",
      " 0.94 0.15 0.79 0.86 0.73 0.92 0.93 0.85 0.56 0.86 0.92 0.89 0.67 0.56\n",
      " 0.86 0.76 0.43 0.82 0.77 0.59 0.59 0.36 0.59 0.91 0.42 0.76 0.81 0.03\n",
      " 0.71 0.6  0.48 0.92 0.8  0.92 0.87 0.94 0.57 0.82 0.92 0.91 0.86 0.77\n",
      " 0.9  0.68 0.87 0.79 0.73 0.81 0.92 0.9  0.51 0.43 0.86 0.8  0.56 0.66\n",
      " 0.78 0.07 0.2  0.97 0.91 0.88 0.83 0.87 0.77 0.9  0.87 0.77 0.86 0.06\n",
      " 0.44 0.87 0.69 0.89 0.6  0.68 0.98 0.83 0.91 0.97 0.87 0.94 0.77 0.88\n",
      " 0.85 0.54 0.55 0.94 0.84 0.57 0.81 0.48 0.92 0.92 0.85 0.97 0.85 0.85\n",
      " 0.68 0.91 0.76 0.76 0.96 0.59 0.59 0.45 0.24 0.92 0.88 0.57 0.08 0.96\n",
      " 0.91 0.84 0.97 0.86 0.91 0.63 0.56 0.05 0.89 0.31 0.87 0.71 0.74 0.76\n",
      " 0.98 0.88 0.96 0.97 0.82 0.47 0.87 0.85 0.98 0.9  0.83 0.79 0.7  0.81\n",
      " 0.92 0.95 0.88 0.71 0.82 0.44 0.58 0.83 0.96 0.37 0.6  0.54 0.57 0.54\n",
      " 0.82 0.86 0.8  0.55 0.74 0.57 0.59 0.8  0.03 0.52 0.03 0.91 0.71 0.87\n",
      " 0.92 0.96 0.11 0.59 0.74 0.09 0.95 0.49 0.41 0.82 0.43 0.98 0.92 0.85\n",
      " 0.59 0.81 0.84 0.43 0.91 0.96 0.78 0.81 0.89 0.68 0.98 0.3  0.86 0.62\n",
      " 0.95 0.72 0.78 0.79 0.44 0.67 0.83 0.82 0.75 0.87 0.55 0.9  0.77 0.75\n",
      " 0.91 0.87 0.53 0.39 0.85 0.82 0.83 0.91 0.6  0.71 0.82 0.89 0.93 0.25\n",
      " 0.91 0.74 0.19 0.87 0.86 0.83 0.89 0.83 0.87 0.9  0.56 0.62 0.51 0.92\n",
      " 0.83 0.96 0.86 0.84 0.62 0.96 0.85 0.66 0.15 0.91 0.52 0.96 0.72 0.67\n",
      " 0.82 0.92 0.53 0.79 0.76 0.95 0.79 0.76 0.8  0.97 0.89 0.91 0.85 0.57\n",
      " 0.96 0.8  0.54 0.48 0.65 0.91 0.93 0.7  0.96 0.93 0.79 0.83 0.73 0.49\n",
      " 0.93 0.88 0.92 0.06 0.8  0.92 0.79 0.1  0.93 0.95 0.83 0.89 0.63 0.48\n",
      " 0.86 0.87 0.92 0.78 0.65 0.61 0.89 0.78 0.05 0.84 0.78 0.61 0.95 0.85\n",
      " 0.03 0.85 0.91 0.9  0.81 0.84 0.81 0.96 0.91 0.8  0.87 0.87 0.92 0.75\n",
      " 0.9  0.8  0.83 0.87 0.48 0.47 0.88 0.97 0.87 0.78 0.42 0.81 0.55 0.95\n",
      " 0.64 0.86 0.8  0.8  0.44 0.84 0.93 0.75 0.67 0.83 0.89 0.91 0.08 0.69\n",
      " 0.76 0.64 0.55 0.95 0.87 0.81 0.93 0.9  0.82 0.97 0.53 0.88 0.57 0.29\n",
      " 0.86 0.93 0.55 0.08 0.83 0.96 0.87 0.94 0.98 0.89 0.87 0.92 0.82 0.96\n",
      " 0.65 0.95 0.77 0.87 0.37 0.57 0.92 0.56 0.67 0.92 0.83 0.91 0.93 0.7\n",
      " 0.84 0.47 0.87 0.85 0.06 0.72 0.92 0.33 0.84 0.6  0.86 0.93 0.76 0.44\n",
      " 0.58 0.94 0.96 0.91 0.92 0.53 0.62 0.58 0.53 0.86 0.79 0.93 0.98 0.44\n",
      " 0.33 0.91 0.92 0.56 0.75 0.92 0.92 0.87 0.66 0.85 0.44 0.23 0.85 0.85\n",
      " 0.86 0.59 0.74 0.84 0.89 0.93 0.51 0.96 0.15 0.42 0.82 0.91 0.84 0.82\n",
      " 0.31 0.03 0.88 0.92 0.82 0.96 0.36 0.93 0.92 0.44 0.81 0.49 0.87 0.89\n",
      " 0.83 0.72 0.54 0.02 0.92 0.83 0.84 0.75 0.92 0.85 0.98 0.49 0.83 0.82\n",
      " 0.92 0.74 0.93 0.45 0.89 0.65 0.87 0.81 0.47 0.73 0.79 0.68 0.15 0.61\n",
      " 0.2  0.12 0.86 0.82 0.74 0.89 0.97 0.84 0.88 0.92 0.57 0.45 0.11 0.82\n",
      " 0.82 0.67 0.22 0.88 0.93 0.89 0.91 0.8  0.41 0.7  0.42 0.4  0.95 0.79\n",
      " 0.74 0.97 0.51 0.89 0.87 0.91 0.77 0.86 0.73 0.9  0.87 0.58 0.81 0.6\n",
      " 0.89 0.45 0.84 0.83 0.52 0.38 0.45 0.87 0.88 0.87 0.85 0.5  0.82 0.48\n",
      " 0.43 0.51 0.75 0.96 0.97 0.14 0.81 0.42 0.91 0.5  0.8  0.97 0.74 0.86\n",
      " 0.88 0.84 0.9  0.96 0.58 0.85 0.09 0.64 0.84 0.93 0.9  0.85 0.09 0.82\n",
      " 0.9  0.88 0.82 0.57 0.88 0.97 0.09 0.58 0.72 0.97 0.83 0.88 0.82 0.9\n",
      " 0.97 0.89 0.47 0.91 0.93 0.93 0.97 0.98 0.83 0.57 0.92 0.45 0.85 0.55\n",
      " 0.79 0.92 0.89 0.62 0.49 0.86 0.84 0.92 0.84 0.8  0.92 0.92 0.86 0.52\n",
      " 0.75 0.77 0.43 0.57 0.62 0.8  0.98 0.66 0.73 0.87 0.95 0.88 0.72 0.91\n",
      " 0.92 0.88 0.81 0.86 0.91 0.91 0.62 0.51 0.96 0.16 0.68 0.9  0.59 0.72\n",
      " 0.86 0.7  0.17 0.91 0.92 0.42 0.55 0.66 0.55 0.93 0.95 0.9  0.63 0.83\n",
      " 0.97 0.98 0.55 0.85 0.92 0.87 0.89 0.52 0.67 0.87 0.91 0.56 0.88 0.39\n",
      " 0.92 0.42 0.49 0.89 0.54 0.92 0.92 0.76 0.88 0.94 0.48 0.72 0.71 0.73\n",
      " 0.53 0.94 0.83 0.55 0.11 0.86 0.52 0.63 0.3  0.81 0.9  0.02 0.93 0.61\n",
      " 0.86 0.46 0.9  0.73 0.94 0.74 0.87 0.95 0.82 0.47 0.9  0.09 0.83 0.93\n",
      " 0.98 0.71 0.86 0.77 0.88 0.78 0.5  0.7  0.9  0.94 0.54 0.8  0.96 0.75\n",
      " 0.51 0.59 0.05 0.86 0.55 0.43 0.03 0.98 0.87 0.58 0.54 0.61 0.56 0.4\n",
      " 0.5  0.93 0.86 0.88 0.89 0.3  0.03 0.79 0.98 0.85 0.5  0.94 0.75 0.94\n",
      " 0.85 0.94 0.81 0.84 0.81 0.89 0.86 0.55 0.45 0.53 0.82 0.94 0.47 0.91\n",
      " 0.89 0.88 0.8  0.94 0.61 0.83 0.9  0.79 0.97 0.97 0.95 0.9  0.85 0.88\n",
      " 0.66 0.91 0.9  0.07 0.55 0.79 0.89 0.85 0.59 0.82 0.94 0.63 0.05 0.86\n",
      " 0.89 0.7  0.91 0.93 0.8  0.92 0.92 0.8  0.84 0.92 0.5  0.81 0.83 0.92\n",
      " 0.8  0.61 0.04 0.49 0.97 0.96 0.55 0.9  0.93 0.46 0.93 0.8  0.67 0.72\n",
      " 0.48 0.95 0.81 0.59 0.86 0.28 0.89 0.86 0.93 0.77 0.64 0.68 0.91 0.91\n",
      " 0.86 0.88 0.84 0.28 0.55 0.93 0.49 0.78 0.86 0.77 0.79 0.81 0.87 0.89\n",
      " 0.49 0.88 0.54 0.84 0.87 0.59 0.59 0.9  0.72 0.82 0.64 0.8  0.76 0.85\n",
      " 0.89 0.15 0.62 0.95 0.89 0.92 0.87 0.96 0.13 0.71 0.98 0.9  0.91 0.84\n",
      " 0.87 0.71 0.78 0.87 0.89 0.96 0.75 0.55 0.79 0.88 0.64 0.36 0.91 0.91\n",
      " 0.81 0.91 0.83 0.79 0.72 0.83 0.9  0.83 0.86 0.82 0.81 0.52 0.81 0.79\n",
      " 0.89 0.93 0.86 0.95 0.85 0.84 0.94 0.85 0.56 0.68 0.87 0.93 0.97 0.9\n",
      " 0.89 0.91 0.17 0.95 0.5  0.97 0.96 0.86 0.84 0.57 0.46 0.87 0.8  0.79\n",
      " 0.85 0.92 0.87 0.58 0.87 0.73 0.75 0.97 0.93 0.77 0.19 0.96 0.94 0.91\n",
      " 0.05 0.79 0.89 0.85 0.53 0.7  0.97 0.76 0.82 0.83 0.97 0.97 0.88 0.87\n",
      " 0.7  0.81 0.83 0.91 0.73 0.84 0.97 0.94 0.85 0.9  0.88 0.97 0.82 0.98\n",
      " 0.81 0.65 0.59 0.86 0.03 0.83 0.46 0.88 0.82 0.96 0.92 0.5  0.88 0.84\n",
      " 0.64 0.83 0.55 0.73 0.85 0.96 0.87 0.35 0.8  0.74 0.92 0.59 0.41 0.65\n",
      " 0.13 0.9  0.96 0.92 0.48 0.49 0.79 0.8  0.79 0.8  0.62 0.65 0.92 0.97\n",
      " 0.75 0.2  0.59 0.88 0.91 0.96 0.71 0.92 0.84 0.81 0.97 0.57 0.93 0.55\n",
      " 0.48 0.93 0.92 0.54 0.95 0.75 0.89 0.73 0.55 0.95 0.9  0.87 0.53 0.88\n",
      " 0.88 0.8  0.97 0.38 0.87 0.84 0.87 0.81 0.94 0.91 0.91 0.86 0.83 0.54\n",
      " 0.92 0.89 0.76 0.49 0.97 0.07 0.81 0.91 0.63 0.91 0.95 0.73 0.84 0.67\n",
      " 0.85 0.14 0.92 0.96 0.94 0.96 0.83 0.89 0.92 0.79 0.97 0.79 0.9  0.84\n",
      " 0.97 0.62 0.93 0.06 0.51 0.91 0.48 0.67 0.86 0.43 0.86 0.86 0.77 0.53\n",
      " 0.92 0.78 0.58 0.35 0.96 0.97 0.79 0.79 0.92 0.46 0.37 0.6  0.82 0.77\n",
      " 0.9  0.62 0.88 0.97 0.77 0.71 0.87 0.81 0.93 0.96 0.91 0.8  0.73 0.87\n",
      " 0.75 0.82 0.92 0.74 0.95 0.91 0.75 0.79 0.85 0.7  0.49 0.87 0.87 0.86\n",
      " 0.02 0.98 0.87 0.59 0.96 0.7  0.85 0.76 0.39 0.82 0.93 0.91 0.97 0.92\n",
      " 0.88 0.77 0.85 0.84 0.65 0.54 0.97 0.61 0.88 0.87 0.68 0.8  0.53 0.79\n",
      " 0.53 0.41 0.96 0.82 0.8  0.5  0.76 0.27 0.83 0.14 0.83 0.96 0.81 0.56\n",
      " 0.88 0.97 0.83 0.58 0.03 0.95 0.88 0.83 0.78 0.61 0.76 0.82 0.5  0.8\n",
      " 0.96 0.96 0.73 0.74 0.96 0.87 0.91 0.83 0.76 0.54 0.73 0.45 0.86 0.87\n",
      " 0.46 0.88 0.64 0.71 0.84 0.91 0.76 0.87 0.48 0.78 0.9  0.76 0.78 0.34\n",
      " 0.14 0.61 0.9  0.92 0.76 0.87 0.91 0.88 0.96 0.83 0.53 0.91 0.77 0.89\n",
      " 0.92 0.96 0.88 0.59 0.84 0.92 0.35 0.89 0.82 0.84 0.74 0.88 0.85 0.86\n",
      " 0.87 0.84 0.92 0.76 0.84 0.78 0.44 0.79 0.91 0.22 0.84 0.92 0.85 0.87\n",
      " 0.93 0.92 0.72 0.83 0.87 0.9  0.95 0.38 0.68 0.83 0.9  0.71 0.9  0.62\n",
      " 0.87 0.82 0.96 0.29 0.67 0.75 0.19 0.58 0.93 0.83 0.56 0.98 0.92 0.81\n",
      " 0.57 0.79 0.84 0.97 0.97 0.91 0.79 0.97 0.89 0.53 0.97 0.04 0.93 0.58\n",
      " 0.82 0.86 0.49 0.76 0.84 0.91 0.85 0.52 0.91 0.8  0.91 0.75 0.45 0.9\n",
      " 0.02 0.55 0.69 0.92 0.74 0.93 0.75 0.94 0.7  0.87 0.88 0.03 0.89 0.89\n",
      " 0.72 0.9  0.98 0.61 0.7  0.89 0.82 0.88 0.13 0.48 0.91 0.8  0.45 0.87\n",
      " 0.86 0.54 0.64 0.76 0.9  0.84 0.96 0.91 0.52 0.5  0.91 0.96 0.97 0.91\n",
      " 0.8  0.61 0.87 0.86 0.51 0.65 0.77 0.6  0.83 0.88 0.91 0.92 0.92 0.9\n",
      " 0.77 0.71 0.87 0.92 0.02 0.68 0.76 0.73 0.55 0.75 0.71 0.65 0.75 0.47\n",
      " 0.69 0.89 0.88 0.52 0.52 0.87 0.91 0.92 0.78 0.88 0.83 0.96 0.7  0.86\n",
      " 0.83 0.89 0.9  0.91 0.48 0.74 0.83 0.82 0.53 0.85 0.86 0.51 0.92 0.54\n",
      " 0.07 0.23 0.63 0.93 0.86 0.59 0.69 0.54 0.45 0.79 0.6  0.91 0.51 0.97\n",
      " 0.6  0.6  0.84 0.84 0.56 0.96 0.62 0.5  0.86 0.63 0.95 0.15 0.84 0.82\n",
      " 0.48 0.88 0.84 0.85 0.48 0.69 0.44 0.97 0.75 0.9  0.89 0.58 0.51 0.5\n",
      " 0.91 0.68 0.54 0.48 0.95 0.83 0.74 0.9  0.82 0.85 0.78 0.82 0.85 0.44\n",
      " 0.72 0.88 0.89 0.91 0.87 0.9  0.94 0.77 0.93 0.77 0.96 0.89 0.84 0.92\n",
      " 0.84 0.94 0.64 0.97 0.71 0.03 0.91 0.76 0.39 0.35 0.78 0.8  0.97 0.85\n",
      " 0.81 0.06 0.87 0.37 0.59 0.55 0.94 0.97 0.88 0.43 0.93 0.76 0.8  0.9\n",
      " 0.87 0.48 0.92 0.79 0.93 0.96 0.86 0.88 0.97 0.11 0.45 0.86 0.86 0.88\n",
      " 0.94 0.84 0.95 0.87 0.9  0.89 0.89 0.77 0.74 0.38 0.86 0.68 0.88 0.44\n",
      " 0.79 0.41 0.9  0.73 0.88 0.25 0.76 0.71 0.89 0.84 0.77 0.74 0.82 0.05\n",
      " 0.92 0.55 0.55 0.91 0.34 0.67 0.85 0.71 0.92 0.83 0.82 0.83 0.97 0.5\n",
      " 0.91 0.57 0.88 0.97 0.6  0.83 0.8  0.97 0.66 0.1  0.92 0.45 0.85 0.93\n",
      " 0.83 0.86 0.98 0.95 0.84 0.78 0.87 0.24 0.9  0.87 0.87 0.87 0.78 0.77\n",
      " 0.81 0.38 0.67 0.83 0.79 0.63 0.87 0.92 0.51 0.84 0.84 0.93 0.96 0.87\n",
      " 0.02 0.92 0.53 0.86 0.47 0.75 0.89 0.78 0.8  0.8  0.97 0.92 0.91 0.91\n",
      " 0.81 0.76 0.97 0.88 0.95 0.6  0.82 0.83 0.93 0.92 0.84 0.91 0.94 0.53\n",
      " 0.83 0.64 0.62 0.87 0.88 0.41 0.79 0.87 0.58 0.84 0.81 0.64 0.77 0.95\n",
      " 0.9  0.66 0.83 0.86 0.69 0.85 0.81 0.65 0.96 0.79 0.97 0.87 0.45 0.89\n",
      " 0.97 0.44 0.81 0.91 0.77 0.87 0.42 0.97 0.97 0.04 0.92 0.38 0.44 0.73\n",
      " 0.76 0.51 0.94 0.67 0.96 0.54 0.97 0.79 0.59 0.82 0.9  0.81 0.63 0.71\n",
      " 0.94 0.24 0.81 0.83 0.79 0.91 0.74 0.95 0.83 0.89 0.59 0.71 0.79 0.43\n",
      " 0.91 0.81 0.79 0.85 0.85 0.96 0.83 0.5  0.87 0.95 0.74 0.82 0.85 0.89\n",
      " 0.5  0.51 0.91 0.05 0.6  0.49 0.77 0.93 0.73 0.82 0.54 0.49 0.09 0.82\n",
      " 0.75 0.97 0.49 0.84 0.76 0.96 0.83 0.55 0.9  0.75 0.16 0.9  0.87 0.65\n",
      " 0.97 0.76 0.92 0.07 0.98 0.95 0.93 0.59 0.84 0.93 0.5  0.51 0.84 0.85\n",
      " 0.81 0.94 0.85 0.48 0.81 0.52 0.77 0.93 0.75 0.93 0.76 0.8  0.88 0.95\n",
      " 0.91 0.75 0.87 0.58 0.88 0.94 0.81 0.88 0.91 0.92 0.8  0.54 0.88 0.49\n",
      " 0.84 0.34 0.88 0.8  0.72 0.81 0.64 0.78 0.03 0.37 0.6  0.62 0.55 0.97\n",
      " 0.48 0.98 0.89 0.54 0.76 0.9  0.79 0.9  0.83 0.02 0.83 0.88 0.92 0.95\n",
      " 0.95 0.54 0.89 0.85 0.95 0.29 0.79 0.82 0.82 0.86 0.82 0.49 0.92 0.59\n",
      " 0.68 0.91 0.91 0.87 0.77 0.55 0.09 0.89 0.48 0.79 0.96 0.92 0.98 0.85\n",
      " 0.83 0.87 0.97 0.93 0.92 0.96 0.74 0.88 0.82 0.69 0.86 0.76 0.51 0.68\n",
      " 0.93 0.24 0.98 0.8  0.86 0.46 0.81 0.19 0.85 0.93 0.75 0.92 0.97 0.79\n",
      " 0.81 0.89 0.87 0.97 0.86 0.88 0.93 0.23 0.28 0.67 0.85 0.49 0.83 0.53\n",
      " 0.95 0.88 0.47 0.86 0.8  0.87 0.5  0.84 0.54 0.42 0.85 0.59 0.75 0.19\n",
      " 0.85 0.48 0.92 0.94 0.76 0.96 0.83 0.96 0.48 0.85 0.89 0.33 0.86 0.49\n",
      " 0.5  0.49 0.82 0.02 0.82 0.97 0.77 0.31 0.84 0.82 0.78 0.84 0.59 0.03\n",
      " 0.6  0.55 0.97 0.88 0.87 0.68 0.49 0.83 0.88 0.95 0.93 0.86 0.92 0.83\n",
      " 0.91 0.96 0.53 0.78 0.86 0.97 0.07 0.25 0.59 0.66 0.96 0.79 0.48 0.87\n",
      " 0.72 0.45 0.88 0.49 0.95 0.42 0.79 0.84 0.44 0.34 0.89 0.79 0.36 0.58\n",
      " 0.55 0.82 0.85 0.52 0.42 0.07 0.46 0.82 0.86 0.84 0.91 0.07 0.46 0.93\n",
      " 0.82 0.81 0.87 0.34 0.57 0.91 0.77 0.58 0.55 0.52 0.56 0.85 0.96 0.81\n",
      " 0.53 0.88 0.86 0.85 0.87 0.87 0.51 0.56 0.71 0.88 0.84 0.98 0.86 0.91\n",
      " 0.93 0.89 0.75 0.48 0.92 0.77 0.86 0.47 0.7  0.05 0.81 0.82 0.89 0.73\n",
      " 0.95 0.62 0.4  0.92 0.64 0.97 0.92 0.76 0.57 0.81 0.9  0.6  0.37 0.46\n",
      " 0.77 0.97 0.94 0.81 0.97 0.59 0.91 0.92 0.96 0.89 0.84 0.89 0.9  0.63\n",
      " 0.75 0.96 0.89 0.86 0.87 0.8  0.92 0.98 0.98 0.88 0.98 0.82 0.75 0.83\n",
      " 0.92 0.88 0.76 0.59 0.58 0.93 0.23 0.8  0.61 0.81 0.89 0.92 0.84 0.62\n",
      " 0.93 0.92 0.86 0.6  0.91 0.56 0.78 0.97 0.53 0.52 0.91 0.84 0.51 0.86\n",
      " 0.83 0.95 0.74 0.93 0.63 0.91 0.86 0.76 0.76 0.12 0.82 0.85 0.91 0.28\n",
      " 0.86 0.68 0.82 0.97 0.97 0.92 0.89 0.79 0.23 0.86 0.95 0.91 0.88 0.8\n",
      " 0.92 0.88 0.81 0.27 0.83 0.8  0.97 0.13 0.59 0.93 0.76 0.74 0.37 0.07\n",
      " 0.79 0.64 0.97 0.41 0.52 0.5  0.92 0.88 0.54 0.73 0.5  0.53 0.8  0.91\n",
      " 0.83 0.85 0.75 0.81 0.87 0.44 0.79 0.96 0.82 0.43 0.9  0.91 0.78 0.85\n",
      " 0.84 0.88 0.8  0.81 0.95 0.88 0.95 0.87 0.44 0.96 0.6  0.86 0.8  0.59\n",
      " 0.81 0.89 0.49 0.91 0.74 0.97 0.86 0.44 0.9  0.87 0.55 0.91 0.53 0.7\n",
      " 0.82 0.88 0.62 0.56 0.83 0.03 0.92 0.83 0.03 0.91 0.97 0.44 0.85 0.88\n",
      " 0.65 0.96 0.8  0.95 0.86 0.93 0.65 0.12 0.85 0.96 0.78 0.77 0.85 0.92\n",
      " 0.97 0.71 0.88 0.87 0.72 0.91 0.87 0.78 0.87 0.85 0.65 0.91 0.87 0.97\n",
      " 0.92 0.86 0.73 0.85 0.58 0.82 0.89 0.53 0.88 0.84 0.82 0.76 0.61 0.8\n",
      " 0.91 0.53 0.29 0.74 0.9  0.7  0.55 0.88 0.9  0.92 0.91 0.26 0.61 0.84\n",
      " 0.82 0.83 0.39 0.96 0.89 0.86 0.92 0.58 0.77 0.56 0.85 0.92 0.96 0.16\n",
      " 0.82 0.93 0.88 0.54 0.84 0.92 0.91 0.83 0.8  0.82 0.59 0.94 0.8  0.41\n",
      " 0.59 0.55 0.97 0.48 0.86 0.87 0.29 0.6  0.85 0.84 0.95 0.97 0.84 0.45\n",
      " 0.96 0.86 0.07 0.9  0.73 0.86 0.9  0.82 0.89 0.85 0.02 0.02 0.83 0.86\n",
      " 0.44 0.76 0.97 0.91 0.82 0.73 0.58 0.57 0.98 0.81 0.72 0.77 0.54 0.71\n",
      " 0.73 0.65 0.72 0.86 0.91 0.22 0.73 0.88 0.28 0.86 0.52 0.57 0.4  0.85\n",
      " 0.83 0.29 0.76 0.49 0.81 0.08 0.82 0.5  0.93 0.79 0.85 0.91 0.87 0.89\n",
      " 0.87 0.5  0.7  0.85 0.45 0.93 0.94 0.81 0.74 0.59 0.81 0.08 0.81 0.65\n",
      " 0.54 0.94 0.89 0.92 0.7  0.72 0.5  0.8  0.59 0.9  0.88 0.78 0.88 0.49\n",
      " 0.74 0.91 0.97 0.19 0.48 0.96 0.74 0.76 0.61 0.84 0.82 0.62 0.39 0.88\n",
      " 0.08 0.77 0.06 0.52 0.84 0.87 0.82 0.83 0.74 0.96 0.93 0.55 0.8  0.92\n",
      " 0.91 0.92 0.88 0.84 0.84 0.88 0.57 0.54 0.9  0.88 0.73 0.45 0.73 0.53\n",
      " 0.9  0.96 0.86 0.85 0.9  0.91 0.94 0.88 0.7  0.9  0.27 0.91 0.87 0.75\n",
      " 0.92 0.03 0.97 0.47 0.7  0.56 0.94 0.45 0.72 0.84 0.83 0.95 0.92 0.75\n",
      " 0.49 0.07 0.59 0.72 0.93 0.41 0.3  0.86 0.9  0.72 0.94 0.6  0.94 0.74\n",
      " 0.05 0.86 0.89 0.85 0.85 0.85 0.72 0.88 0.67 0.48 0.83 0.93 0.35 0.58\n",
      " 0.91 0.44 0.66 0.78 0.95 0.97 0.6  0.85 0.97 0.93 0.66 0.59 0.11 0.05\n",
      " 0.9  0.73 0.91 0.95 0.05 0.79 0.88 0.82 0.9  0.97 0.89 0.91 0.81 0.85\n",
      " 0.03 0.79 0.77 0.48 0.85 0.8  0.04 0.43 0.16 0.78 0.92 0.44 0.76 0.74\n",
      " 0.83 0.46 0.89 0.86 0.14 0.88 0.93 0.88 0.93 0.63 0.88 0.89 0.85 0.17\n",
      " 0.93 0.96 0.57 0.66 0.91 0.53 0.91 0.72 0.45 0.51 0.44 0.55 0.87 0.85\n",
      " 0.98 0.79 0.8  0.83 0.97 0.9  0.62 0.91 0.76 0.97 0.66 0.91 0.53 0.89\n",
      " 0.6  0.27 0.49 0.5  0.74 0.86 0.91 0.87 0.39 0.94 0.92 0.44 0.83 0.57\n",
      " 0.83 0.93 0.87 0.12 0.09 0.8  0.45 0.84 0.52 0.81 0.78 0.53 0.94 0.9\n",
      " 0.87 0.89 0.87 0.91 0.85 0.79 0.86 0.91 0.82 0.9  0.57 0.91 0.68 0.94\n",
      " 0.81 0.88 0.94 0.86 0.95 0.5  0.96 0.92 0.6  0.82 0.87 0.92 0.44 0.96\n",
      " 0.87 0.82 0.56 0.93 0.92 0.88 0.49 0.91 0.66 0.88 0.58 0.55 0.17 0.81\n",
      " 0.78 0.77 0.59 0.58 0.93 0.39 0.91 0.76 0.65 0.94 0.04 0.81 0.89 0.56\n",
      " 0.91 0.91 0.35 0.51 0.21 0.79 0.86 0.52 0.38 0.05 0.96 0.88 0.81 0.86\n",
      " 0.49 0.72 0.53 0.51 0.84 0.89 0.82 0.94 0.87 0.65 0.67 0.71 0.89 0.82\n",
      " 0.86 0.44 0.91 0.84 0.87 0.97 0.19 0.73 0.82 0.85 0.79 0.59 0.85 0.87\n",
      " 0.94 0.72 0.9  0.91 0.85 0.59 0.75 0.83 0.79 0.27 0.78 0.9  0.88 0.82\n",
      " 0.07 0.89 0.82 0.57 0.97 0.79 0.97 0.85 0.86 0.47 0.93 0.62 0.95 0.81\n",
      " 0.56 0.54 0.83 0.89 0.88 0.41 0.82 0.85 0.86 0.86 0.91 0.78 0.04 0.88\n",
      " 0.96 0.92 0.91 0.38 0.8  0.79 0.88 0.73 0.58 0.96 0.87 0.78 0.53 0.45\n",
      " 0.91 0.83 0.31 0.92 0.82 0.77 0.91 0.58 0.77 0.05 0.83 0.83 0.11 0.93\n",
      " 0.56 0.65 0.89 0.78 0.84 0.85 0.52 0.88 0.76 0.91 0.83 0.55 0.22 0.7\n",
      " 0.53 0.88 0.94 0.82 0.98 0.88 0.8  0.94 0.93 0.82 0.86 0.5  0.85 0.88\n",
      " 0.85 0.95 0.56 0.94 0.55 0.77 0.56 0.96 0.89 0.85 0.97 0.78 0.57 0.6\n",
      " 0.87 0.81 0.86 0.97 0.86 0.89 0.81 0.73 0.9  0.82 0.94 0.05 0.43 0.51\n",
      " 0.54 0.67 0.98 0.52 0.47 0.91 0.91 0.77 0.48 0.39 0.97 0.97 0.76 0.87\n",
      " 0.04 0.5  0.04 0.88 0.84 0.83 0.86 0.88 0.86 0.83 0.94 0.33 0.89 0.45\n",
      " 0.91 0.48 0.94 0.92 0.95 0.84 0.87 0.89 0.6  0.48 0.7  0.84 0.82 0.91\n",
      " 0.82 0.81 0.86 0.82 0.49 0.28 0.63 0.49 0.86 0.9  0.48 0.77 0.92 0.79\n",
      " 0.44 0.88 0.92 0.61 0.81 0.89 0.23 0.74 0.92 0.54 0.65 0.91 0.86 0.88\n",
      " 0.86 0.78 0.29 0.9  0.42 0.92 0.91 0.51 0.6  0.9  0.92 0.89 0.49 0.97\n",
      " 0.87 0.8  0.34 0.45 0.93 0.96 0.92 0.77 0.79 0.73 0.35 0.57 0.86 0.79\n",
      " 0.04 0.85 0.55 0.73 0.87 0.25 0.93 0.93 0.94 0.76 0.86 0.98 0.46 0.91\n",
      " 0.91 0.91 0.82 0.93 0.85 0.88 0.73 0.76 0.89 0.91 0.54 0.96 0.75 0.6\n",
      " 0.98 0.82 0.89 0.63 0.81 0.84 0.81 0.34 0.96 0.85 0.81 0.79 0.01 0.8\n",
      " 0.89 0.51 0.85 0.94 0.93 0.46 0.5  0.76 0.59 0.86 0.92 0.87 0.37 0.81\n",
      " 0.35 0.83 0.92 0.38 0.98 0.92 0.78 0.89 0.87 0.03 0.53 0.81 0.62 0.44\n",
      " 0.96 0.32 0.91 0.47 0.62 0.94 0.97 0.97 0.64 0.9  0.48 0.74 0.84 0.95\n",
      " 0.25 0.69 0.72 0.78 0.85 0.88 0.59 0.72 0.68 0.47 0.77 0.89 0.87 0.85\n",
      " 0.83 0.92 0.85 0.03 0.95 0.82 0.76 0.26 0.86 0.91 0.92 0.9  0.94 0.67\n",
      " 0.54 0.79 0.86 0.83 0.61 0.96 0.71 0.78 0.91 0.73 0.48 0.91 0.66 0.56\n",
      " 0.49 0.96 0.9  0.95 0.93 0.84 0.79 0.79 0.82 0.85 0.96 0.17 0.56 0.87\n",
      " 0.86 0.75 0.15 0.86 0.56 0.88 0.44 0.5  0.55 0.73 0.12 0.96 0.88 0.84\n",
      " 0.92 0.14 0.86 0.92 0.51 0.9  0.93 0.75 0.87 0.74 0.87 0.44 0.79 0.8\n",
      " 0.37 0.94 0.82 0.46 0.83 0.54 0.77 0.84 0.53 0.62 0.79 0.78 0.87 0.91\n",
      " 0.89 0.81 0.86 0.92 0.92 0.55 0.83 0.92 0.68 0.62 0.96 0.85 0.6  0.95\n",
      " 0.83 0.58 0.63 0.47 0.04 0.94 0.83 0.96 0.11 0.96 0.63 0.93 0.89 0.85\n",
      " 0.96 0.84 0.88 0.91 0.89 0.83 0.91 0.56 0.94 0.88 0.76 0.65 0.92 0.82\n",
      " 0.83 0.88 0.83 0.74 0.93 0.86 0.83 0.51 0.92 0.91 0.97 0.5  0.87 0.89\n",
      " 0.74 0.86 0.89 0.8  0.87 0.97 0.85 0.42 0.91 0.81 0.87 0.51 0.71 0.7\n",
      " 0.93 0.48 0.89 0.86 0.74 0.9  0.85 0.95 0.43 0.84 0.93 0.87 0.89 0.86\n",
      " 0.55 0.82 0.53 0.89 0.8  0.92 0.52 0.83 0.1  0.55 0.92 0.91 0.94 0.51\n",
      " 0.46 0.33 0.67 0.92 0.71 0.84 0.81 0.91 0.1  0.85 0.9  0.87 0.96 0.78\n",
      " 0.44 0.75 0.87 0.07 0.8  0.87 0.03 0.85 0.87 0.76 0.92 0.89 0.92 0.84\n",
      " 0.96 0.93 0.52 0.83 0.81 0.55 0.68 0.74 0.88 0.74 0.86 0.91 0.98 0.44\n",
      " 0.49 0.95 0.75 0.88 0.03 0.72 0.3  0.92 0.23 0.84 0.61 0.03 0.86 0.86\n",
      " 0.41 0.56 0.53 0.78 0.96 0.76 0.97 0.85 0.61 0.9  0.83 0.9  0.92 0.8\n",
      " 0.67 0.54 0.81 0.82 0.03 0.48 0.96 0.54 0.98 0.58 0.93 0.85 0.91 0.67\n",
      " 0.78 0.95 0.44 0.53 0.16 0.92 0.89 0.9  0.8  0.01 0.85 0.8  0.33 0.11\n",
      " 0.52 0.44 0.82 0.68 0.85 0.92 0.85 0.88 0.71 0.9  0.83 0.81 0.79 0.36\n",
      " 0.09 0.89 0.85 0.58 0.88 0.04 0.97 0.83 0.76 0.86 0.8  0.12 0.41 0.65\n",
      " 0.34 0.66 0.92 0.72 0.78 0.83 0.78 0.84 0.79 0.82 0.5  0.91 0.82 0.83\n",
      " 0.84 0.92 0.74 0.9  0.92 0.39 0.57 0.88 0.82 0.8  0.98 0.92 0.85 0.85\n",
      " 0.87 0.94 0.87 0.84 0.83 0.81 0.59 0.54 0.41 0.28 0.56 0.77 0.79 0.91\n",
      " 0.39 0.49 0.94 0.59 0.58 0.88 0.87 0.87 0.56 0.95 0.85 0.84 0.79 0.97\n",
      " 0.7  0.92 0.36 0.97 0.89 0.82 0.87 0.97 0.89 0.43 0.78 0.92 0.82 0.83\n",
      " 0.9  0.64 0.9  0.58 0.5  0.46 0.82 0.76 0.73 0.27 0.8  0.77 0.75 0.9\n",
      " 0.49 0.8  0.47 0.53 0.88 0.82 0.8  0.87 0.95 0.6  0.62 0.6  0.7  0.5\n",
      " 0.85 0.42 0.75 0.74 0.59 0.48 0.84 0.74 0.57 0.96 0.85 0.69 0.86 0.83\n",
      " 0.92 0.84 0.85 0.84 0.85 0.96 0.93 0.86 0.75 0.83 0.95 0.95 0.88 0.85\n",
      " 0.95 0.87 0.8  0.93 0.83 0.88 0.94 0.91 0.8  0.85 0.95 0.21 0.77 0.75\n",
      " 0.91 0.83 0.34 0.84 0.84 0.75 0.9  0.63 0.86 0.81 0.77 0.89 0.92 0.9\n",
      " 0.93 0.69 0.93 0.9  0.95 0.91 0.95 0.82 0.76 0.49 0.9  0.81 0.84 0.96\n",
      " 0.92 0.84 0.48 0.56 0.46 0.4  0.83 0.49 0.95 0.8  0.86 0.52 0.53 0.89\n",
      " 0.87 0.9  0.98 0.91 0.91 0.64 0.19 0.86 0.58 0.58 0.57 0.85 0.85 0.83\n",
      " 0.58 0.27 0.74 0.69 0.91 0.92 0.82 0.81 0.49 0.94 0.42 0.86 0.83 0.92\n",
      " 0.89 0.55 0.87 0.87 0.82 0.82 0.9  0.79 0.1  0.96 0.83 0.95 0.06 0.51\n",
      " 0.89 0.76 0.91 0.48 0.84 0.04 0.44 0.65 0.96 0.72 0.84 0.85 0.08 0.5\n",
      " 0.67 0.69 0.73 0.59 0.91 0.86 0.97 0.89 0.91 0.97 0.82 0.97 0.86 0.09\n",
      " 0.82 0.91 0.4  0.8  0.94 0.25 0.97 0.83 0.96 0.81 0.84 0.86 0.92 0.88\n",
      " 0.84 0.82 0.97 0.83 0.56 0.26 0.79 0.93 0.13 0.93 0.88 0.5  0.84 0.92\n",
      " 0.86 0.8  0.92 0.93 0.88 0.91 0.3  0.07 0.92 0.62 0.84 0.7  0.91 0.83\n",
      " 0.96 0.85 0.21 0.49 0.8  0.54 0.86 0.92 0.98 0.39 0.54 0.97 0.16 0.87\n",
      " 0.91 0.92 0.5  0.57 0.51 0.8  0.81 0.55 0.82 0.79 0.51 0.83 0.92 0.89\n",
      " 0.69 0.87 0.97 0.83 0.85 0.93 0.19 0.88 0.11 0.71 0.86 0.89 0.87 0.85\n",
      " 0.59 0.73 0.73 0.89 0.8  0.87 0.43 0.87 0.77 0.87 0.49 0.76 0.85 0.9\n",
      " 0.85 0.74 0.83 0.05 0.26 0.54 0.96 0.92 0.91 0.92 0.92 0.87 0.09 0.92\n",
      " 0.5  0.08 0.51 0.87 0.78 0.73 0.88 0.91 0.76 0.38 0.87 0.92 0.86 0.51\n",
      " 0.87 0.63 0.24 0.78 0.9  0.9  0.85 0.82 0.39 0.87 0.49 0.97 0.93 0.95\n",
      " 0.56 0.88 0.88 0.89 0.85 0.75 0.72 0.89 0.82 0.86 0.82 0.87 0.97 0.97\n",
      " 0.87 0.1  0.73 0.88 0.88 0.9  0.54 0.82 0.58 0.91 0.8  0.82 0.74 0.86\n",
      " 0.15 0.92 0.86 0.77 0.23 0.8  0.95 0.92 0.9  0.93 0.76 0.89 0.77 0.87\n",
      " 0.9  0.89 0.97 0.94 0.45 0.9  0.89 0.82 0.87 0.89 0.24 0.8  0.6  0.84\n",
      " 0.58 0.81 0.78 0.87 0.11 0.85 0.91 0.44 0.9  0.87 0.68 0.87 0.78 0.78\n",
      " 0.67 0.95 0.51 0.86 0.94 0.76 0.81 0.64 0.9  0.86 0.97 0.94 0.98 0.82\n",
      " 0.87 0.43 0.73 0.82 0.63 0.51 0.86 0.27 0.72 0.34 0.82 0.76 0.94 0.78\n",
      " 0.33 0.78 0.18 0.23 0.9  0.55 0.5  0.33 0.48 0.74 0.97 0.9  0.53 0.86\n",
      " 0.86 0.43 0.55 0.77 0.85 0.88 0.73 0.13 0.79 0.92 0.62 0.49 0.84 0.69\n",
      " 0.8  0.84 0.52 0.52 0.94 0.78 0.58 0.79 0.81 0.78 0.92 0.88 0.6  0.15\n",
      " 0.58 0.55 0.49 0.82 0.91 0.06 0.62 0.89 0.83 0.8  0.5  0.81 0.77 0.15\n",
      " 0.62 0.55 0.87 0.74 0.96 0.97 0.94 0.71 0.92 0.83 0.96 0.92 0.86 0.92\n",
      " 0.55 0.88 0.44 0.79 0.89 0.93 0.64 0.86 0.69 0.95 0.72 0.82 0.92 0.48\n",
      " 0.72 0.86 0.68 0.49 0.55 0.97 0.83 0.65 0.63 0.88 0.81 0.64 0.84 0.7\n",
      " 0.84 0.02 0.8  0.78 0.87 0.04 0.91 0.46 0.79 0.95 0.76 0.88 0.55 0.7\n",
      " 0.82 0.88 0.83 0.77 0.86 0.8  0.74 0.97 0.83 0.79 0.92 0.91 0.97 0.88\n",
      " 0.91 0.89 0.42 0.83 0.08 0.93 0.56 0.1  0.93 0.87 0.92 0.54 0.8  0.96\n",
      " 0.79 0.76 0.96 0.93 0.52 0.89 0.94 0.63 0.95 0.97 0.87 0.91 0.9  0.94\n",
      " 0.87 0.48 0.67 0.73 0.86 0.49 0.95 0.86 0.64 0.65 0.11 0.49 0.85 0.87\n",
      " 0.86 0.71 0.89 0.87 0.61 0.71 0.07 0.87 0.86 0.85 0.65 0.9  0.93 0.7\n",
      " 0.96 0.91 0.05 0.89 0.83 0.95 0.92 0.98 0.76 0.7  0.59 0.49 0.85 0.59\n",
      " 0.94 0.1  0.82 0.72 0.6  0.85 0.13 0.9  0.51 0.85 0.97 0.7  0.03 0.9\n",
      " 0.89 0.97 0.93 0.76 0.97 0.81 0.62 0.81 0.75 0.77 0.96 0.72 0.96 0.98\n",
      " 0.74 0.42 0.86 0.96 0.49 0.16 0.89 0.96 0.59 0.9  0.93 0.95 0.34 0.94\n",
      " 0.94 0.88 0.89 0.92 0.87 0.35 0.9  0.51 0.77 0.91 0.43 0.89 0.88 0.87\n",
      " 0.93 0.56 0.79 0.13 0.74 0.1  0.6  0.88 0.96 0.62 0.93 0.91 0.92 0.51\n",
      " 0.83 0.54 0.84 0.88 0.91 0.67 0.85 0.88 0.93 0.93 0.48 0.91 0.91 0.15\n",
      " 0.91 0.1  0.61 0.16 0.9  0.82 0.37 0.06 0.82 0.9  0.43 0.17 0.03 0.9\n",
      " 0.71 0.62 0.83 0.25 0.85 0.93 0.08 0.9  0.03 0.86 0.97 0.92 0.92 0.8\n",
      " 0.86 0.51 0.85 0.89 0.88 0.52 0.97 0.84 0.86 0.81 0.75 0.83 0.61 0.82\n",
      " 0.62 0.86 0.46 0.97 0.91 0.96 0.7  0.78 0.84 0.79 0.44 0.57 0.06 0.82\n",
      " 0.91 0.03 0.83 0.88 0.9  0.93 0.79 0.38 0.87 0.77 0.85 0.95 0.87 0.86\n",
      " 0.97 0.94 0.78 0.95 0.91 0.84 0.67 0.79 0.47 0.94 0.05 0.82 0.8  0.86\n",
      " 0.97 0.73 0.75 0.8  0.88 0.8  0.17 0.77 0.84 0.93 0.75 0.87 0.88 0.97\n",
      " 0.86 0.75 0.8  0.84 0.88 0.83 0.07 0.86 0.74 0.67 0.92 0.81 0.88 0.87\n",
      " 0.9  0.97 0.94 0.97 0.96 0.91 0.64 0.89 0.55 0.8  0.39 0.73 0.85 0.85\n",
      " 0.83 0.6  0.56 0.86 0.55 0.88 0.54 0.93 0.89 0.81 0.48 0.93 0.95 0.49\n",
      " 0.09 0.52 0.5  0.57 0.9  0.93 0.91 0.89 0.87 0.92 0.89 0.79 0.86 0.33\n",
      " 0.94 0.98 0.83 0.91 0.43 0.75 0.87 0.72 0.92 0.6  0.54 0.2  0.92 0.88\n",
      " 0.66 0.53 0.91 0.73 0.87 0.46 0.78 0.91 0.2  0.61 0.91 0.57 0.48 0.87\n",
      " 0.84 0.37 0.89 0.81 0.96 0.95 0.52 0.33 0.82 0.86 0.59 0.66 0.98 0.57\n",
      " 0.92 0.84 0.8  0.06 0.93 0.49 0.57 0.97 0.03 0.55 0.86 0.85 0.58 0.59\n",
      " 0.59 0.86 0.72 0.78 0.6  0.9  0.8  0.75 0.74 0.8  0.77 0.88 0.76 0.84\n",
      " 0.39 0.82 0.64 0.88 0.92 0.6  0.89 0.88 0.97 0.55 0.87 0.44 0.84 0.58\n",
      " 0.97 0.88 0.93 0.95 0.64 0.84 0.87 0.94 0.56 0.83 0.61 0.93 0.86 0.86\n",
      " 0.51 0.51 0.89 0.74 0.91 0.83 0.6  0.88 0.93 0.86 0.61 0.52 0.6  0.93\n",
      " 0.44 0.85 0.58 0.84 0.92 0.92 0.71 0.66 0.87 0.04 0.94 0.87 0.9  0.88\n",
      " 0.57 0.7  0.96 0.09 0.83 0.81 0.91 0.9  0.62 0.85 0.12 0.5  0.86 0.96\n",
      " 0.62 0.89 0.55 0.85 0.8  0.92 0.81 0.73 0.89 0.63 0.92 0.92 0.89 0.7\n",
      " 0.46 0.84 0.59 0.96 0.21 0.67 0.86 0.84 0.97 0.78 0.67 0.91 0.67 0.92\n",
      " 0.89 0.8  0.86 0.94 0.72 0.96 0.13 0.89 0.92 0.29 0.85 0.51 0.96 0.91\n",
      " 0.56 0.47 0.83 0.87 0.9  0.56 0.87 0.92 0.93 0.55 0.06 0.79 0.79 0.97\n",
      " 0.86 0.52 0.87 0.96 0.86 0.78 0.5  0.55 0.9  0.53 0.78 0.04 0.97 0.91\n",
      " 0.92 0.84 0.42 0.87 0.97 0.53 0.54 0.97 0.95 0.08 0.29 0.83 0.92 0.66\n",
      " 0.61 0.6  0.94 0.88 0.89 0.93 0.96 0.51 0.85 0.95 0.91 0.93 0.88 0.84\n",
      " 0.75 0.6  0.59 0.78 0.38 0.98 0.52 0.54 0.92 0.44 0.62 0.56 0.6  0.83\n",
      " 0.79 0.64 0.97 0.79 0.98 0.81 0.71 0.94 0.85 0.49 0.91 0.97 0.91 0.85\n",
      " 0.04 0.8  0.91 0.89 0.97 0.85 0.68 0.8  0.09 0.78 0.87 0.84 0.78 0.82\n",
      " 0.69 0.86 0.78 0.93 0.96 0.87 0.84 0.42 0.82 0.67 0.81 0.74 0.51 0.56\n",
      " 0.59 0.09 0.78 0.8  0.87 0.9  0.87 0.92 0.92 0.95 0.91 0.62 0.35 0.53\n",
      " 0.88 0.97 0.95 0.77 0.88 0.56 0.8  0.07 0.96 0.89 0.97 0.81 0.87 0.89\n",
      " 0.92 0.68 0.1  0.89 0.9  0.85 0.88 0.92 0.97 0.8  0.86 0.96 0.73 0.85\n",
      " 0.74 0.85 0.54 0.84 0.56 0.79 0.86 0.02 0.86 0.91 0.54 0.79 0.87 0.81\n",
      " 0.83 0.76 0.91 0.83 0.84 0.91 0.84 0.52 0.94 0.96 0.9  0.81 0.88 0.67\n",
      " 0.91 0.84 0.93 0.58 0.36 0.72 0.58 0.8  0.59 0.71 0.88 0.98 0.83 0.89\n",
      " 0.86 0.86 0.98 0.94 0.55 0.83 0.86 0.58 0.87 0.97 0.94 0.47 0.75 0.68\n",
      " 0.72 0.95 0.93 0.57 0.57 0.86 0.85 0.88 0.82 0.83 0.83 0.85 0.95 0.76\n",
      " 0.81 0.96 0.69 0.82 0.95 0.59 0.87 0.97 0.97 0.6  0.74 0.82 0.93 0.96\n",
      " 0.5  0.54 0.96 0.93 0.73 0.6  0.96 0.85 0.97 0.93 0.87 0.88 0.87 0.86\n",
      " 0.88 0.93 0.96 0.81 0.67 0.78 0.3  0.76 0.86 0.77 0.61 0.79 0.96 0.43\n",
      " 0.96 0.84 0.46 0.79 0.96 0.86 0.78 0.9  0.93 0.58 0.5  0.55 0.79 0.79\n",
      " 0.41 0.56 0.55 0.96 0.76 0.82 0.94 0.88 0.82 0.88 0.85 0.97 0.69 0.73\n",
      " 0.83 0.68 0.97 0.81 0.83 0.82 0.8  0.31 0.74 0.9  0.61 0.89 0.16 0.87\n",
      " 0.96 0.78 0.81 0.53 0.81 0.91 0.59 0.59 0.85 0.47 0.34 0.97 0.91 0.76\n",
      " 0.77 0.97 0.82 0.83 0.51 0.95 0.71 0.86 0.89 0.53 0.95 0.86 0.59 0.78\n",
      " 0.9  0.4  0.97 0.95 0.61 0.52 0.92 0.59 0.73 0.9  0.82 0.87 0.84 0.31\n",
      " 0.9  0.97 0.82 0.86 0.86 0.19 0.49 0.67 0.81 0.95 0.86 0.52 0.88 0.87\n",
      " 0.82 0.58 0.71 0.57 0.87 0.94 0.85 0.89 0.31 0.87 0.53 0.87 0.57 0.97\n",
      " 0.83 0.13 0.77 0.67 0.9  0.87 0.91 0.85 0.89 0.92 0.19 0.87 0.58 0.55\n",
      " 0.83 0.72 0.84 0.67 0.81 0.81 0.91 0.96 0.83 0.78 0.41 0.83 0.93 0.89\n",
      " 0.79 0.71 0.92 0.94 0.86 0.6  0.57 0.76 0.82 0.86 0.93 0.46 0.8  0.91\n",
      " 0.82 0.96 0.51 0.45 0.95 0.84 0.69 0.95 0.64 0.9  0.8  0.57 0.31 0.94\n",
      " 0.92 0.62 0.97 0.94 0.85 0.92 0.59 0.87 0.77 0.54 0.72 0.84 0.21 0.91\n",
      " 0.97 0.68 0.48 0.94 0.88 0.97 0.8  0.89 0.55 0.78 0.9  0.62 0.52 0.08\n",
      " 0.96 0.88 0.84 0.82 0.81 0.84 0.96 0.02 0.92 0.92 0.45 0.94 0.84 0.93\n",
      " 0.6  0.53 0.81 0.9  0.87 0.92 0.58 0.8  0.65 0.95 0.6  0.51 0.86 0.62\n",
      " 0.79 0.53 0.91 0.55 0.97 0.81 0.69 0.26 0.91 0.46 0.89 0.64 0.9  0.78\n",
      " 0.76 0.94 0.92 0.8  0.93 0.92 0.89 0.86 0.7  0.4  0.86 0.02 0.78 0.9\n",
      " 0.7  0.03 0.43 0.81 0.93 0.72 0.81 0.81 0.42 0.81 0.92 0.03 0.96 0.51\n",
      " 0.77 0.22 0.8  0.5  0.89 0.88 0.86 0.94 0.63 0.31 0.54 0.69 0.96 0.83\n",
      " 0.71 0.81 0.9  0.91 0.83 0.75 0.97 0.12 0.86 0.87 0.89 0.93 0.82 0.93\n",
      " 0.74 0.46 0.81 0.92 0.79 0.88 0.53 0.84 0.08 0.77 0.84 0.5  0.81 0.9\n",
      " 0.79 0.89 0.18 0.8  0.94 0.97 0.54 0.85 0.88 0.92 0.94 0.79 0.91 0.52\n",
      " 0.49 0.78 0.82 0.88 0.87 0.9  0.48 0.56 0.77 0.57 0.1  0.97 0.76 0.62\n",
      " 0.88 0.16 0.08 0.82 0.87 0.97 0.96 0.9  0.65 0.08 0.83 0.66 0.73 0.93\n",
      " 0.98 0.92 0.93 0.92 0.82 0.97 0.6  0.65 0.29 0.03 0.82 0.87 0.08 0.08\n",
      " 0.77 0.98 0.89 0.52 0.78 0.91 0.89 0.94 0.94 0.83 0.53 0.55 0.86 0.84\n",
      " 0.96 0.93 0.95 0.95 0.53 0.58 0.81 0.82 0.76 0.83 0.84 0.75 0.96 0.93\n",
      " 0.94 0.95 0.91 0.83 0.19 0.8  0.91 0.67 0.78 0.68 0.9  0.78 0.02 0.76\n",
      " 0.95 0.7  0.23 0.75 0.45 0.96 0.88 0.83 0.51 0.93 0.54 0.97 0.54 0.55\n",
      " 0.78 0.87 0.91 0.89 0.86 0.09 0.44 0.86 0.85 0.54 0.81 0.89 0.71 0.92\n",
      " 0.55 0.41 0.97 0.92 0.49 0.91 0.85 0.94 0.91 0.82 0.31 0.5  0.88 0.58\n",
      " 0.92 0.42 0.95 0.62 0.87 0.78 0.66 0.55 0.87 0.89 0.45 0.9  0.87 0.9\n",
      " 0.79 0.81 0.96 0.78 0.4  0.42 0.92 0.88 0.85 0.87 0.6  0.79 0.81 0.97\n",
      " 0.92 0.89 0.36 0.82 0.75 0.61 0.15 0.82 0.98 0.81 0.18 0.97 0.49 0.13\n",
      " 0.84 0.96 0.63 0.25 0.91 0.89 0.85 0.92 0.88 0.83 0.91 0.92 0.87 0.91\n",
      " 0.86 0.81 0.96 0.76 0.85 0.84 0.97 0.66 0.92 0.69 0.82 0.84 0.6  0.26\n",
      " 0.9  0.91 0.78 0.76 0.54 0.81 0.81 0.61 0.72 0.86 0.92 0.57 0.82 0.88\n",
      " 0.4  0.71 0.93 0.41 0.78 0.86 0.92 0.9  0.96 0.81 0.51 0.63 0.95 0.97\n",
      " 0.89 0.36 0.9  0.85 0.74 0.9  0.88 0.84 0.81 0.41 0.84 0.5  0.87 0.91\n",
      " 0.41 0.89 0.77 0.89 0.97 0.48 0.88 0.91 0.52 0.92 0.86 0.88 0.96 0.48\n",
      " 0.86 0.52 0.92 0.88 0.59 0.94 0.55 0.79 0.61 0.07 0.87 0.86 0.78 0.71\n",
      " 0.22 0.93 0.87 0.45 0.83 0.42 0.84 0.53 0.98 0.89 0.65 0.85 0.92 0.49\n",
      " 0.39 0.9  0.88 0.51 0.94 0.79 0.97 0.65 0.42 0.73 0.89 0.77 0.84 0.82\n",
      " 0.49 0.94 0.82 0.6  0.9  0.49 0.91 0.91 0.85 0.35 0.51 0.79 0.8  0.83\n",
      " 0.19 0.94 0.62 0.02 0.62 0.87 0.42 0.93 0.85 0.45 0.02 0.73 0.54 0.48\n",
      " 0.66 0.92 0.79 0.86 0.37 0.81 0.78 0.9  0.95 0.98 0.91 0.71 0.48 0.48\n",
      " 0.81 0.73 0.95 0.86 0.91 0.73 0.9  0.88 0.03 0.91 0.36 0.92 0.8  0.95\n",
      " 0.91 0.84 0.78 0.91 0.72 0.55 0.58 0.73 0.44 0.94 0.8  0.05 0.56 0.03\n",
      " 0.91 0.91 0.98 0.88 0.03 0.97 0.85 0.86 0.79 0.88 0.81 0.92 0.66 0.82\n",
      " 0.81 0.9  0.81 0.8  0.9  0.85 0.89 0.59 0.62 0.9  0.84 0.85 0.48 0.56\n",
      " 0.71 0.79 0.48 0.95 0.97 0.48 0.25 0.58 0.87 0.75 0.96 0.86 0.77 0.78\n",
      " 0.92 0.57 0.93 0.31 0.87 0.91 0.2  0.59 0.55 0.44 0.78 0.48 0.55 0.61\n",
      " 0.88 0.73 0.63 0.93 0.19 0.8  0.85 0.44 0.47 0.14 0.81 0.71 0.69 0.78\n",
      " 0.9  0.96 0.74 0.96 0.92 0.85 0.97 0.92 0.91 0.91 0.94 0.64 0.38 0.96\n",
      " 0.86 0.85 0.91 0.77 0.94 0.86 0.82 0.96 0.51 0.87 0.85 0.84 0.87 0.11\n",
      " 0.52 0.75 0.83 0.83 0.81 0.87 0.14 0.93 0.97 0.91 0.93 0.81 0.44 0.93\n",
      " 0.76 0.93 0.62 0.79 0.96 0.55 0.63 0.86 0.79 0.82 0.92 0.95 0.87 0.45\n",
      " 0.83 0.78 0.83 0.94 0.57 0.73 0.78 0.94 0.87 0.05 0.55 0.71 0.89 0.58\n",
      " 0.87 0.78 0.72 0.89 0.42 0.89 0.94 0.15 0.71 0.83 0.48 0.49 0.84 0.88\n",
      " 0.56 0.91 0.87 0.54 0.83 0.79 0.46 0.82 0.65 0.82 0.03 0.84 0.96 0.85\n",
      " 0.92 0.9  0.82 0.62 0.79 0.9  0.83 0.93 0.35 0.98 0.87 0.96 0.87 0.04\n",
      " 0.86 0.29 0.59 0.58 0.85 0.92 0.87 0.8  0.93 0.55 0.91 0.92 0.65 0.88\n",
      " 0.55 0.83 0.82 0.94 0.9  0.88 0.96 0.51 0.87 0.86 0.81 0.81 0.87 0.54\n",
      " 0.82 0.9  0.93 0.68 0.43 0.31 0.86 0.77 0.75 0.94 0.45 0.66 0.93 0.91\n",
      " 0.02 0.78 0.75 0.86 0.92 0.95 0.84 0.13 0.88 0.85 0.96 0.88 0.73 0.89\n",
      " 0.45 0.88 0.46 0.92 0.05 0.54 0.51 0.06 0.83 0.73 0.68 0.82 0.6  0.91\n",
      " 0.83 0.93 0.53 0.94 0.53 0.48 0.86 0.9  0.82 0.82 0.45 0.7  0.84 0.11\n",
      " 0.81 0.69 0.07 0.92 0.87 0.26 0.5  0.9  0.88 0.67 0.84 0.45 0.97 0.95\n",
      " 0.82 0.79 0.91 0.93 0.97 0.57 0.55 0.55 0.84 0.62 0.96 0.77 0.05 0.46\n",
      " 0.57 0.77 0.92 0.96 0.89 0.87 0.83 0.79 0.94 0.94 0.97 0.85 0.89 0.46\n",
      " 0.72 0.49 0.96 0.89 0.43 0.97 0.86 0.89 0.81 0.9  0.91 0.71 0.96 0.66\n",
      " 0.48 0.91 0.87 0.93 0.93 0.74 0.93 0.6  0.54 0.86 0.86 0.78 0.58 0.98\n",
      " 0.87 0.81 0.52 0.45 0.74 0.81 0.9  0.49 0.8  0.87 0.03 0.97 0.84 0.37\n",
      " 0.92 0.93 0.71 0.52 0.48 0.97 0.93 0.54 0.89 0.92 0.92 0.61 0.78 0.92\n",
      " 0.48 0.81 0.51 0.91 0.65 0.92 0.86 0.3  0.87 0.89 0.85 0.85 0.58 0.92\n",
      " 0.53 0.88 0.71 0.51 0.56 0.64 0.85 0.92 0.8  0.89 0.94 0.49 0.07 0.89\n",
      " 0.77 0.93 0.95 0.9  0.65 0.81 0.85 0.63 0.58 0.88 0.03 0.88 0.79 0.78\n",
      " 0.11 0.6  0.6  0.88 0.74 0.33 0.57 0.33 0.81 0.79 0.67 0.81 0.89 0.83\n",
      " 0.96 0.76 0.9  0.48 0.86 0.67 0.92 0.91 0.81 0.62 0.92 0.74 0.01 0.8\n",
      " 0.83 0.86 0.23 0.89 0.84 0.33 0.82 0.89 0.7  0.47 0.95 0.82 0.95 0.5\n",
      " 0.87 0.84 0.87 0.85 0.9  0.92 0.9  0.74 0.51 0.77 0.83 0.92 0.49 0.81\n",
      " 0.96 0.27 0.65 0.93 0.47 0.82 0.7  0.79 0.86 0.16 0.8  0.16 0.61 0.06\n",
      " 0.05 0.9  0.86 0.9  0.54 0.92 0.8  0.79 0.81 0.05 0.89 0.85 0.93 0.95\n",
      " 0.9  0.7  0.28 0.91 0.98 0.58 0.96 0.92 0.88 0.43 0.86 0.88 0.93 0.93\n",
      " 0.85 0.92 0.73 0.81 0.88 0.85 0.64 0.56 0.9  0.52 0.94 0.94 0.97 0.91\n",
      " 0.67 0.91 0.68 0.96 0.82 0.79 0.6  0.49 0.63 0.63 0.95 0.87 0.53 0.89\n",
      " 0.92 0.82 0.91 0.02 0.6  0.85 0.82 0.88 0.85 0.72 0.78 0.82 0.84 0.77\n",
      " 0.55 0.88 0.96 0.86 0.42 0.92 0.97 0.22 0.8  0.54 0.95 0.84 0.82 0.95\n",
      " 0.35 0.86 0.89 0.94 0.92 0.77 0.94 0.53 0.62 0.98 0.91 0.92 0.94 0.94\n",
      " 0.55 0.35 0.81 0.79 0.27 0.37 0.49 0.92 0.91 0.91 0.79 0.8  0.81 0.94\n",
      " 0.84 0.94 0.81 0.37 0.72 0.83 0.81 0.62 0.83 0.83 0.75 0.9  0.73 0.91\n",
      " 0.89 0.97 0.95 0.6  0.95 0.77 0.15 0.94 0.81 0.82 0.92 0.65 0.88 0.97\n",
      " 0.6  0.83 0.96 0.56 0.56 0.96 0.43 0.04 0.35 0.9  0.96 0.79 0.84 0.77\n",
      " 0.03 0.98 0.51 0.93 0.47 0.86 0.85 0.76 0.51 0.96 0.5  0.83 0.87 0.93\n",
      " 0.83 0.88 0.44 0.5  0.97 0.89 0.96 0.6  0.94 0.86 0.65 0.08 0.66 0.78\n",
      " 0.27 0.97 0.97 0.53 0.84 0.75 0.86 0.46 0.69 0.46 0.68 0.38 0.9  0.81\n",
      " 0.81 0.87 0.86 0.42 0.71 0.02 0.72 0.85 0.96 0.87 0.89 0.03 0.89 0.96\n",
      " 0.98 0.8  0.59 0.87 0.82 0.9  0.5  0.75 0.04 0.54 0.48 0.85 0.62 0.91\n",
      " 0.84 0.77 0.09 0.83 0.93 0.94 0.91 0.88 0.47 0.77 0.97 0.93 0.84 0.96\n",
      " 0.89 0.86 0.81 0.74 0.98 0.1  0.79 0.84 0.81 0.82 0.15 0.91 0.64 0.86\n",
      " 0.87 0.84 0.86 0.92 0.76 0.87 0.55 0.88 0.77 0.93 0.63 0.82 0.09 0.59\n",
      " 0.1  0.84 0.93 0.8  0.87 0.91 0.9  0.93 0.6  0.37 0.78 0.79 0.86 0.28\n",
      " 0.88 0.83 0.97 0.55 0.82 0.81 0.9  0.76 0.86 0.62 0.48 0.82 0.08 0.79\n",
      " 0.82 0.85 0.73 0.58 0.97 0.55 0.77 0.12 0.55 0.94 0.44 0.71 0.02 0.97\n",
      " 0.84 0.97 0.81 0.8  0.98 0.39 0.86 0.52 0.87 0.92 0.55 0.43 0.89 0.82\n",
      " 0.56 0.81 0.85 0.89 0.73 0.45 0.85 0.89 0.81 0.83 0.58 0.57 0.91 0.92\n",
      " 0.63 0.76 0.62 0.94 0.37 0.96 0.54 0.75 0.71 0.96 0.91 0.48 0.95 0.36\n",
      " 0.72 0.34 0.88 0.86 0.93 0.3  0.86 0.54 0.69 0.87 0.83 0.79 0.45 0.93\n",
      " 0.84 0.72 0.82 0.77 0.13 0.35 0.83 0.49 0.51 0.94 0.97 0.62 0.77 0.78\n",
      " 0.8  0.02 0.81 0.57 0.82 0.51 0.49 0.87 0.87 0.91 0.91 0.88 0.92 0.79\n",
      " 0.93 0.82 0.91 0.81 0.84 0.82 0.58 0.93 0.95 0.82 0.94 0.9  0.75 0.82\n",
      " 0.33 0.34 0.55 0.54 0.84 0.89 0.68 0.81 0.68 0.1  0.24 0.81 0.93 0.96\n",
      " 0.85 0.44 0.92 0.72 0.84 0.85 0.53 0.41 0.77 0.85 0.85 0.96 0.87 0.92\n",
      " 0.23 0.03 0.88 0.8  0.57 0.95 0.91 0.32 0.8  0.88 0.63 0.39 0.43 0.82\n",
      " 0.84 0.53 0.43 0.95 0.91 0.21 0.79 0.73 0.92 0.49 0.91 0.91 0.77 0.64\n",
      " 0.92 0.88 0.86 0.87 0.81 0.92 0.07 0.92 0.52 0.71 0.77 0.82 0.86 0.94\n",
      " 0.76 0.94 0.78 0.89 0.57 0.9  0.72 0.96 0.97 0.87 0.32 0.97 0.88 0.83\n",
      " 0.91 0.79 0.79 0.32 0.05 0.88 0.54 0.16 0.77 0.7  0.36 0.9  0.77 0.07\n",
      " 0.11 0.83 0.92 0.46 0.93 0.83 0.91 0.84 0.87 0.82 0.9  0.66 0.86 0.44\n",
      " 0.91 0.84 0.62 0.78 0.72 0.74 0.95 0.64 0.97 0.93 0.83 0.86 0.92 0.85\n",
      " 0.64 0.95 0.56 0.94 0.83 0.92 0.59 0.81 0.86 0.82 0.47 0.86 0.85 0.96\n",
      " 0.59 0.69 0.89 0.96 0.82 0.85 0.77 0.52 0.8  0.83 0.79 0.13 0.6  0.55\n",
      " 0.82 0.94 0.92 0.81 0.84 0.39 0.96 0.93 0.79 0.55 0.92 0.96 0.88 0.97\n",
      " 0.96 0.95 0.87 0.63 0.63 0.89 0.68 0.75 0.88 0.07 0.97 0.31 0.96 0.56\n",
      " 0.08 0.13 0.56 0.8  0.68 0.53 0.39 0.6  0.43 0.85 0.41 0.08 0.55 0.81\n",
      " 0.88 0.78 0.03 0.71 0.96 0.97 0.55 0.79 0.96 0.97 0.88 0.87 0.64 0.74\n",
      " 0.56 0.66 0.79 0.54 0.55 0.78 0.87 0.87 0.83 0.86 0.74 0.87 0.85 0.89\n",
      " 0.75 0.81 0.93 0.97 0.86 0.78 0.97 0.89 0.97 0.91 0.49 0.61 0.78 0.91\n",
      " 0.94 0.32 0.84 0.78 0.83 0.83 0.83 0.82 0.83 0.53 0.95 0.38 0.91 0.96\n",
      " 0.84 0.88 0.88 0.19 0.91 0.55 0.92 0.64 0.89 0.91 0.9  0.79 0.78 0.62\n",
      " 0.96 0.84 0.08 0.97 0.87 0.8  0.73 0.78 0.89 0.8  0.51 0.93 0.73 0.96\n",
      " 0.77 0.67 0.79 0.03 0.56 0.14 0.93 0.84 0.82 0.54 0.97 0.97 0.97 0.9\n",
      " 0.71 0.77 0.7  0.71 0.91 0.6  0.74 0.84 0.88 0.85 0.97 0.86 0.82 0.82\n",
      " 0.81 0.97 0.91 0.82 0.96 0.48 0.83 0.91 0.84 0.87 0.96 0.91 0.84 0.94\n",
      " 0.86 0.76 0.6  0.91 0.84 0.91 0.77 0.8  0.81 0.85 0.85 0.87 0.97 0.81\n",
      " 0.84 0.87 0.77 0.95 0.64 0.82 0.6  0.9  0.6  0.77 0.86 0.86 0.4  0.57\n",
      " 0.79 0.96 0.79 0.82 0.51 0.79 0.58 0.89 0.79 0.82 0.98 0.79 0.88 0.97\n",
      " 0.9  0.95 0.42 0.82 0.93 0.49 0.95 0.61 0.86 0.75 0.76 0.89 0.5  0.74\n",
      " 0.46 0.86 0.9  0.97 0.85]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = model.predict_proba(X_test)[:,0].round(2)\n",
    "print(f)\n",
    "np.median(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9049"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = model.transform(X_test)\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fff = model.transform(X_test)\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "for i in range(0,len(fff)):\n",
    "    for j in range(0,len(fff[i])):\n",
    "        if fff[i][j][0] >= 0.5:\n",
    "            fff[i][j][0] = 1\n",
    "        else:\n",
    "            fff[i][j][0] = 0\n",
    "        if(fff[i][j][1] < 0.5):\n",
    "            fff[i][j][1] = 0\n",
    "        else:\n",
    "            fff[i][j][1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elements(seq) -> dict:\n",
    "    \"\"\"Tally elements from `seq`.\"\"\"\n",
    "    hist = {}\n",
    "    for i in seq:\n",
    "        hist[i] = hist.get(i, 0) + 1\n",
    "    return hist\n",
    "\n",
    "final2 = []\n",
    "final_final = []\n",
    "for i in range(0, len(ff)):\n",
    "    for j in range(0, len(ff[i])):\n",
    "        final_value = (max(ff[i][j]))\n",
    "        if np.argmax(ff[i][j]) == 0:\n",
    "            final2.append(final_value)\n",
    "        \n",
    "    final_final.append(final2) \n",
    "    final2 = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=final2[9050:18098], bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "\n",
    "print(n)\n",
    "#plt.grid(axis='y', alpha=0.75)\n",
    "#plt.xlabel('Value')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.title('My Very Own Histogram')\n",
    "#plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "#maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "#plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f5da3d4fd0>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])\n",
    "n,b1in, patches= plt.hist(final_final, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f5da5a4280>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])\n",
    "n,b1in, patches= plt.hist(final_final, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x201.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(final_final, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "ax1.set_title('Class 0')\n",
    "ax2.hist(final_final1, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "ax2.set_title('Class 1')\n",
    "f.tight_layout()\n",
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])\n",
    "f.set_size_inches(6,2.8)\n",
    "plt.savefig('voting.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>T0</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.812576</td>\n",
       "      <td>0.781302</td>\n",
       "      <td>0.806498</td>\n",
       "      <td>0.808156</td>\n",
       "      <td>0.812576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.440264</td>\n",
       "      <td>0.407662</td>\n",
       "      <td>0.405029</td>\n",
       "      <td>0.448539</td>\n",
       "      <td>0.440264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <td>-0.066572</td>\n",
       "      <td>-0.063687</td>\n",
       "      <td>-0.059564</td>\n",
       "      <td>-0.074546</td>\n",
       "      <td>-0.066572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <td>-0.032983</td>\n",
       "      <td>-0.002994</td>\n",
       "      <td>-0.027154</td>\n",
       "      <td>-0.039369</td>\n",
       "      <td>-0.032983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_abs_odds_difference</th>\n",
       "      <td>0.020277</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>0.027194</td>\n",
       "      <td>0.020277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disparate_impact</th>\n",
       "      <td>-1.079553</td>\n",
       "      <td>-0.643027</td>\n",
       "      <td>-1.106966</td>\n",
       "      <td>-1.022627</td>\n",
       "      <td>-1.079553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theil_index</th>\n",
       "      <td>0.203083</td>\n",
       "      <td>0.214830</td>\n",
       "      <td>0.211923</td>\n",
       "      <td>0.201688</td>\n",
       "      <td>0.203083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               classifier        T0        T1        T2  \\\n",
       "accuracy                         0.812576  0.781302  0.806498  0.808156   \n",
       "f1                               0.440264  0.407662  0.405029  0.448539   \n",
       "statistical_parity_difference   -0.066572 -0.063687 -0.059564 -0.074546   \n",
       "equal_opportunity_difference    -0.032983 -0.002994 -0.027154 -0.039369   \n",
       "average_abs_odds_difference      0.020277  0.010964  0.017036  0.027194   \n",
       "disparate_impact                -1.079553 -0.643027 -1.106966 -1.022627   \n",
       "theil_index                      0.203083  0.214830  0.211923  0.201688   \n",
       "\n",
       "                                     T3  \n",
       "accuracy                       0.812576  \n",
       "f1                             0.440264  \n",
       "statistical_parity_difference -0.066572  \n",
       "equal_opportunity_difference  -0.032983  \n",
       "average_abs_odds_difference    0.020277  \n",
       "disparate_impact              -1.079553  \n",
       "theil_index                    0.203083  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "final_result = pd.DataFrame(final_metrics)\n",
    "final_result[3] = np.log(final_result[3])\n",
    "final_result = final_result.transpose()\n",
    "acc_f1 = pd.DataFrame(accuracy)\n",
    "acc_f1['f1'] = f1\n",
    "acc_f1 = pd.DataFrame(acc_f1).transpose()\n",
    "acc = acc_f1.rename(index={0: 'accuracy', 1: 'f1'})\n",
    "final_result = final_result.rename(index={0: 'statistical_parity_difference', 1: 'equal_opportunity_difference', 2: 'average_abs_odds_difference', 3: 'disparate_impact', 4: 'theil_index'})\n",
    "final_result = pd.concat([acc,final_result])\n",
    "final_result.columns = ['T' + str(col) for col in final_result.columns]\n",
    "final_result.insert(0, \"classifier\", final_result['T' + str(len(list_estimators) - 1)])   ##Add final metrics add the beginning of the df\n",
    "#final_result.to_csv('../../Results/VotingClassifier/' + nb_fname + '.csv')\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
