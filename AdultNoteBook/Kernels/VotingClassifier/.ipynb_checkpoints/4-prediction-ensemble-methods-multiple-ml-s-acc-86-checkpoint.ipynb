{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32b2ef04814b284c19f71e623e98022efd322732"
   },
   "source": [
    "# Adult Census Income Analysis - Decision TREE, Random Forest, CV, Tuning the model with Ensemble Techniques(Baaging , ADAboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7462e6028fbfac80717afb34961109db9f1d26b4"
   },
   "source": [
    "### A stable and optimized model to predict the income of a given population, which is labelled as <= 50K and >50K. The attributes (predictors) are age, working class type, marital status, gender, race etc.\n",
    "#### Following are the steps, \n",
    "#### 1.clean and prepare the data,\n",
    "#### 2.Analyze Data,\n",
    "#### 3.Label Encoding,\n",
    "#### 4.Build a decision tree and Random forest with default hyperparameters,\n",
    "#### 5.Build several classifier models to compare, cross validate and for voting classifier model\n",
    "#### 6.choose the optimal hyperparameters using grid search cross-validation.\n",
    "#### 7.Build optimized Random forest model with tuned hyperparameters from grid search model\n",
    "#### 8.Increase Accuracy by Applying Ensemble technique BAGGING to our tuned random forest model\n",
    "#### 9.Increase Accuracy by Applying Ensemble technique ADABOOST to our tuned random forest model\n",
    "####  I hope you enjoy this notebook and find it useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce6da2377aff6dd18a2a76b7d0b67e732e00bfcb"
   },
   "source": [
    "## Clean & Analyze Data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7361e3b94dfc782419e36e24e47650f6bee7acbf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "import matplotlib.patches as patches\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "#from packages import *\n",
    "#from ml_fairness import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "39f4cd7d56fe0b06e47bfc6cf2c950feaad7e8e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ffa2817a5eb614c43310643022ddbd8a7afa8d42"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "04d567bee2cf3de195c43c3a9ecae60f33b93f38"
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv(\"../../Data/adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0ef0457d40b0333ae9687a364eb00ea6f386bf67"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90         ?   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "2   66         ?  186061  Some-college             10        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "0                  ?  Not-in-family  White  Female             0   \n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                  ?      Unmarried  Black  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "0          4356              40  United-States  <=50K  \n",
       "1          4356              18  United-States  <=50K  \n",
       "2          4356              40  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "59ecb2faafed022b16c91590939620ae684e868f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education.num   32561 non-null  int64 \n",
      " 5   marital.status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital.gain    32561 non-null  int64 \n",
      " 11  capital.loss    32561 non-null  int64 \n",
      " 12  hours.per.week  32561 non-null  int64 \n",
      " 13  native.country  32561 non-null  object\n",
      " 14  income          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a62594b2ac936da01d921ac9ee380394803c7e60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education.num     0\n",
       "marital.status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital.gain      0\n",
       "capital.loss      0\n",
       "hours.per.week    0\n",
       "native.country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "de0c8f4d41821ac89947f1ce11c69bf410424321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workclass         1836\n",
       "education            0\n",
       "marital.status       0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "native.country     583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select all categorical variables\n",
    "df_categorical = data.select_dtypes(include=['object'])\n",
    "\n",
    "# checking whether any other columns contain a \"?\"\n",
    "df_categorical.apply(lambda x: x==\"?\", axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "801d05ea01a07ece2c0bbe4b7f95160e9fbefbc3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               1836\n",
       "workclass         1836\n",
       "fnlwgt            1836\n",
       "education         1836\n",
       "education.num     1836\n",
       "marital.status    1836\n",
       "occupation        1836\n",
       "relationship      1836\n",
       "race              1836\n",
       "sex               1836\n",
       "capital.gain      1836\n",
       "capital.loss      1836\n",
       "hours.per.week    1836\n",
       "native.country    1836\n",
       "income            1836\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['workclass'] == '?' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4a990ca11cc0710e10062a60b16781df33255cb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               1843\n",
       "workclass         1843\n",
       "fnlwgt            1843\n",
       "education         1843\n",
       "education.num     1843\n",
       "marital.status    1843\n",
       "occupation        1843\n",
       "relationship      1843\n",
       "race              1843\n",
       "sex               1843\n",
       "capital.gain      1843\n",
       "capital.loss      1843\n",
       "hours.per.week    1843\n",
       "native.country    1843\n",
       "income            1843\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['occupation'] == '?' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "3141dc44e649ffc20f10424b190ac9c8d6f88b7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               583\n",
       "workclass         583\n",
       "fnlwgt            583\n",
       "education         583\n",
       "education.num     583\n",
       "marital.status    583\n",
       "occupation        583\n",
       "relationship      583\n",
       "race              583\n",
       "sex               583\n",
       "capital.gain      583\n",
       "capital.loss      583\n",
       "hours.per.week    583\n",
       "native.country    583\n",
       "income            583\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['native.country'] == '?' ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a6ed9e88a758c17b6aa9e2dd8d472fd09f9dac26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005638647461687295"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1836/32561)/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51a8e4950eb27e6854f8cf9c6d83f212a1d83b2"
   },
   "source": [
    " ### Missing Value % is very insignificant  so we will drop those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "539c79a31706c11cc52d70f431401e52d896dc68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               32561\n",
       "workclass         32561\n",
       "fnlwgt            32561\n",
       "education         32561\n",
       "education.num     32561\n",
       "marital.status    32561\n",
       "occupation        32561\n",
       "relationship      32561\n",
       "race              32561\n",
       "sex               32561\n",
       "capital.gain      32561\n",
       "capital.loss      32561\n",
       "hours.per.week    32561\n",
       "native.country    32561\n",
       "income            32561\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3429d7ada61be066e494c6acd19cffdbfa035e29"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"workclass\"] != \"?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1349aa0ed7099010bfd8e96ffbdb858d2277ea9f"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"occupation\"] != \"?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "8a0b26a32092dd60ce36765d66eb2f5ed68f76c9"
   },
   "outputs": [],
   "source": [
    "data = data[data[\"native.country\"] != \"?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4d67c1faab29f37a890aaa4b8d69a6d1524fe964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               30162\n",
       "workclass         30162\n",
       "fnlwgt            30162\n",
       "education         30162\n",
       "education.num     30162\n",
       "marital.status    30162\n",
       "occupation        30162\n",
       "relationship      30162\n",
       "race              30162\n",
       "sex               30162\n",
       "capital.gain      30162\n",
       "capital.loss      30162\n",
       "hours.per.week    30162\n",
       "native.country    30162\n",
       "income            30162\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "377e5040e9ec46e875f1648a388dda15a282b15e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "5   34   Private  216864       HS-grad              9       Divorced   \n",
       "6   38   Private  150601          10th              6      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "5      Other-service      Unmarried  White  Female             0   \n",
       "6       Adm-clerical      Unmarried  White    Male             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "1          4356              18  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  \n",
       "5          3770              45  United-States  <=50K  \n",
       "6          3770              40  United-States  <=50K  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "8973966358496bc69ded01402417e797e9a8abec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<=50K', '>50K'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"income\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "71ead6f769e3b2412645c5446f9607c176beca4e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "5   34   Private  216864       HS-grad              9       Divorced   \n",
       "6   38   Private  150601          10th              6      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "5      Other-service      Unmarried  White  Female             0   \n",
       "6       Adm-clerical      Unmarried  White    Male             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country  income  \n",
       "1          4356              18  United-States       0  \n",
       "3          3900              40  United-States       0  \n",
       "4          3900              40  United-States       0  \n",
       "5          3770              45  United-States       0  \n",
       "6          3770              40  United-States       0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"income\"] = data[\"income\"].map({'<=50K' : 0, '>50K': 1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "8c3cd40f8cd1c4b1d6663f9029c645e2246718b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"income\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16c4b44756d205c485138c9e988e00099b56567c"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "8930628f17b35db2be301b97ac1e34acaa2688f5"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "catogorical_data = data.select_dtypes(include =['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ea714a2426659b7a2bdb7ee382f7a2843a8390c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Private</td>\n",
       "      <td>10th</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  workclass     education marital.status         occupation   relationship  \\\n",
       "1   Private       HS-grad        Widowed    Exec-managerial  Not-in-family   \n",
       "3   Private       7th-8th       Divorced  Machine-op-inspct      Unmarried   \n",
       "4   Private  Some-college      Separated     Prof-specialty      Own-child   \n",
       "5   Private       HS-grad       Divorced      Other-service      Unmarried   \n",
       "6   Private          10th      Separated       Adm-clerical      Unmarried   \n",
       "\n",
       "    race     sex native.country  \n",
       "1  White  Female  United-States  \n",
       "3  White  Female  United-States  \n",
       "4  White  Female  United-States  \n",
       "5  White  Female  United-States  \n",
       "6  White    Male  United-States  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catogorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "abc005c18d8499593ecbf512f18aabf5ffd8cfe8"
   },
   "outputs": [],
   "source": [
    "catogorical_data = catogorical_data.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "1e64759b9f1ad053ed3c27db18e262a289e41565"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass  education  marital.status  occupation  relationship  race  sex  \\\n",
       "1          2         11               6           3             1     4    0   \n",
       "3          2          5               0           6             4     4    0   \n",
       "4          2         15               5           9             3     4    0   \n",
       "5          2         11               0           7             4     4    0   \n",
       "6          2          0               5           0             4     4    1   \n",
       "\n",
       "   native.country  \n",
       "1              38  \n",
       "3              38  \n",
       "4              38  \n",
       "5              38  \n",
       "6              38  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catogorical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "7a051dcff726e3ad29d40cf202ae20631aef2041"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>132870</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>140359</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>264663</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>216864</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>150601</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education.num  capital.gain  capital.loss  hours.per.week  \\\n",
       "1   82  132870              9             0          4356              18   \n",
       "3   54  140359              4             0          3900              40   \n",
       "4   41  264663             10             0          3900              40   \n",
       "5   34  216864              9             0          3770              45   \n",
       "6   38  150601              6             0          3770              40   \n",
       "\n",
       "   income  workclass  education  marital.status  occupation  relationship  \\\n",
       "1       0          2         11               6           3             1   \n",
       "3       0          2          5               0           6             4   \n",
       "4       0          2         15               5           9             3   \n",
       "5       0          2         11               0           7             4   \n",
       "6       0          2          0               5           0             4   \n",
       "\n",
       "   race  sex  native.country  \n",
       "1     4    0              38  \n",
       "3     4    0              38  \n",
       "4     4    0              38  \n",
       "5     4    0              38  \n",
       "6     4    1              38  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(catogorical_data.columns, axis=1)\n",
    "data = pd.concat([data, catogorical_data], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "0c387fad9e66ca47ac40bde78712ca65bf17f5c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30162 entries, 1 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   age             30162 non-null  int64\n",
      " 1   fnlwgt          30162 non-null  int64\n",
      " 2   education.num   30162 non-null  int64\n",
      " 3   capital.gain    30162 non-null  int64\n",
      " 4   capital.loss    30162 non-null  int64\n",
      " 5   hours.per.week  30162 non-null  int64\n",
      " 6   income          30162 non-null  int64\n",
      " 7   workclass       30162 non-null  int32\n",
      " 8   education       30162 non-null  int32\n",
      " 9   marital.status  30162 non-null  int32\n",
      " 10  occupation      30162 non-null  int32\n",
      " 11  relationship    30162 non-null  int32\n",
      " 12  race            30162 non-null  int32\n",
      " 13  sex             30162 non-null  int32\n",
      " 14  native.country  30162 non-null  int32\n",
      "dtypes: int32(8), int64(7)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "3c8381e875346a9cf9e71fc42a2d94763a73af22"
   },
   "outputs": [],
   "source": [
    "data['income'] = data['income'].astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bedbd44b9962d3c451c83333728b1785e435f046"
   },
   "source": [
    "## Decision Tree Model with Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "b1e9480c1274e73df82082ce231507a88bbf0dc6"
   },
   "outputs": [],
   "source": [
    "x=data.drop('income',axis=1)\n",
    "y=data['income']\n",
    "#Train & Test split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state= 476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "67fbd003d313f5b35823f3bdcca716c1a363579d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "model_tree = tree.fit(x_train,y_train)\n",
    "model_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "d3b0b401d7de935334802a6b40810e106fc9d0b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "4831f4b65f4bf77df61c12d363e2069b747093e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Desicion Tree is  0.8125759752458835\n"
     ]
    }
   ],
   "source": [
    "model_tree = tree.fit(x_train,y_train)\n",
    "pred_tree = tree.predict(x_test)\n",
    "a1 = accuracy_score(y_test,pred_tree)\n",
    "print(\"The Accuracy of Desicion Tree is \", a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "2dca916976e4efa778d2caeb647bc79c303b6942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5931,  857],\n",
       "       [ 839, 1422]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "cb90ece9466f7d446f9921aab016c29d1783e6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      6788\n",
      "           1       0.62      0.63      0.63      2261\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      9049\n",
      "   macro avg       0.75      0.75      0.75      9049\n",
      "weighted avg       0.81      0.81      0.81      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5f89d14d398ea7ea5a4f469525466c98a76cd3"
   },
   "source": [
    "## Random Forest Model with Default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "598c1a77316eb07c6995f05f33208f33bf7cfce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Random Forest is  0.8450657531218919\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "model_rf = rf.fit(x_train,y_train)\n",
    "pred_rf = rf.predict(x_test)\n",
    "a2 = accuracy_score(y_test, pred_rf)\n",
    "print(\"The Accuracy of Random Forest is \", a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50a9d2bb57b7b8803b1bd7c2068192cdda0831e9"
   },
   "source": [
    "## Logistic Regression & KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "c03bf10d54caac55aa511ab556570c4c0f8469c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of logistic regression is  0.7911371422256603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression()\n",
    "\n",
    "model_lg = lg.fit(x_train,y_train)\n",
    "pred_lg = lg.predict(x_test)\n",
    "a3 = accuracy_score(y_test, pred_lg)\n",
    "print(\"The Accuracy of logistic regression is \", a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "2afe0c31705ea99b3df342921835417e91a969db"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "9b20127aa6de02eaaa5428d43e1f65f3a0b43b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of KNN is  0.7596419493866725\n"
     ]
    }
   ],
   "source": [
    "model_knn =knn.fit(x_train,y_train) \n",
    "pred_knn = knn.predict(x_test)\n",
    "a4 = accuracy_score(y_test, pred_knn)\n",
    "print(\"The Accuracy of KNN is \", a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "320ee22f6847ce98263b9309c12ed6b17ad1242d"
   },
   "source": [
    "# Build optimized Random forest model with tuned hyperparameters from grid search model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "c13ef3808997e6aa926a7a2ab95720f57dddc568"
   },
   "outputs": [],
   "source": [
    "rf_param = {\n",
    "    \"n_estimators\": [25,50,100],\n",
    "    \"criterion\" : [\"gini\"],\n",
    "    \"max_depth\" : [3,4,5,6],\n",
    "    \"max_features\" : [\"auto\",\"sqrt\",\"log2\"],\n",
    "    \"random_state\" : [123]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "d4a63e0033c293440051ee7f9f812bf48f3e32af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [25, 50, 100], 'criterion': ['gini'], 'max_depth': [3, 4, 5, 6], 'max_features': ['auto', 'sqrt', 'log2'], 'random_state': [123]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV(rf, rf_param, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "10d2067afaf7fb2c7cb4e6740db81238ad57b404",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid =GridSearchCV(rf, rf_param, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "920bc6d90d4aa68336554be02cfc3789d989bebd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 6,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 123}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train).best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "e7fa9c64023d976755eac8cb079da333af82c1f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462813570560282"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(criterion = 'gini',\n",
    "    max_depth = 6,\n",
    "    max_features = 'auto',\n",
    "    n_estimators = 100,\n",
    "    random_state = 123)\n",
    "model_rf1 = rf1.fit(x_train,y_train)\n",
    "pred_rf1 = rf1.predict(x_test)\n",
    "accuracy_score(y_test, pred_rf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f6d64d06ab439aaa9bb16ad7bd1c6e62abcafec"
   },
   "source": [
    "# K FOLD Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "2bcb8778b3908d94a6ecb9ece61c07b93abdaba6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79024621, 0.81628788, 0.79498106, 0.79403409, 0.79971591,\n",
       "       0.80776515, 0.80672667, 0.79952607, 0.79905213, 0.80805687])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(tree,x_train,y_train,scoring= \"accuracy\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "68c89d69d71c63fbdf9cad5fd34ff8359c66662b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7639760250539661"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(tree,x,y,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "46fedd8d9cc8583275a1f49f146046728d371b79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446926761990472"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf,x_train,y_train,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "bd39a51661a03879fae3bd0d3316aab80a7c2060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881883610197511"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lg,x_train,y_train,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "7ee7c85805c50b27bf9f2291f46316fb63c95fe5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642686401045582"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn,x_train,y_train,scoring= \"accuracy\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "515e5f5c9181d45e6f820b518acc5c768fa5c26b"
   },
   "source": [
    "# Voting Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "4f48f73c49a84ab59bb66791099166b2c4fffbca"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "e2a36648b67aa8ccd0a45822b8aa6774b2e4ade3"
   },
   "outputs": [],
   "source": [
    "model_vote = VotingClassifier(estimators=[('logistic Regression', lg), ('random forrest', rf), ('knn neighbors', knn),(' decision tree', tree)], voting='soft')\n",
    "model_vote = model_vote.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "4bdb63d97ba527a4549cd03883b8654802563b13"
   },
   "outputs": [],
   "source": [
    "vote_pred = model_vote.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "fcfde9016c98a7d7d8130109c074b954786fd470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of voting classifier is  0.8371090728257266\n"
     ]
    }
   ],
   "source": [
    "a5 =  accuracy_score(y_test, vote_pred)\n",
    "print(\"The Accuracy of voting classifier is \", a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "dedbd45285846adc5ee7dcc35b148cb3585b6e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90      6788\n",
      "           1       0.77      0.50      0.60      2261\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      9049\n",
      "   macro avg       0.81      0.72      0.75      9049\n",
      "weighted avg       0.83      0.84      0.82      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, vote_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56a0b7324ac27e7c747f89896ba8c50421f46b2e"
   },
   "source": [
    "# Ensemble Technique Bagging \n",
    "\n",
    "## Increase Accuracy by Applying Ensemble technique BAGGING to our tuned random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "15f7a81d545e25f18e5b6e9d38ef53c1601b4439"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "d29bd47d5b793d5bb732954126d65ca31ce14edb"
   },
   "outputs": [],
   "source": [
    "bagg = BaggingClassifier(base_estimator=rf1,n_estimators=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "a7e594bd915aa5ac7ddb06d02045f7ce841b239c"
   },
   "outputs": [],
   "source": [
    "model_bagg =bagg.fit(x_train,y_train) \n",
    "pred_bagg = bagg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "3c87107b23c77823ce9b7591133614ff6274a766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of BAAGING is  0.8460603381589126\n"
     ]
    }
   ],
   "source": [
    "a6 = accuracy_score(y_test, pred_bagg)\n",
    "print(\"The Accuracy of BAAGING is \", a6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "d40d7bffcf4b44472558546c3c4ca93f56d4a96b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6496,  292],\n",
       "       [1101, 1160]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_bagg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "e9aa7155dd006dbd1638cb0603af0600978de85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90      6788\n",
      "           1       0.80      0.51      0.62      2261\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      9049\n",
      "   macro avg       0.83      0.74      0.76      9049\n",
      "weighted avg       0.84      0.85      0.83      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_bagg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7a3462fa71e660beba1c3bcddfa8773cba1f33d"
   },
   "source": [
    "#  Ensemble Technique  ADA Boost \n",
    "\n",
    "## Increase Accuracy by Applying Ensemble technique ADABOOST to our tuned random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "223838ecb17eea19d3e9cfef7e334546b60367ff"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "024d359c921869720b89f046d985b1afc1b7ad09"
   },
   "outputs": [],
   "source": [
    "Adaboost = AdaBoostClassifier(base_estimator=rf1, n_estimators=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "41e5ecc6cd4ba3e6f9a29a3f48afd4868742503f"
   },
   "outputs": [],
   "source": [
    "model_boost =Adaboost.fit(x_train,y_train) \n",
    "pred_boost = Adaboost.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "116a125308885136f39085cdc52631ce2147f0f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of BOOSTING is  0.8660625483478838\n"
     ]
    }
   ],
   "source": [
    "a7 = accuracy_score(y_test, pred_boost)\n",
    "print(\"The Accuracy of BOOSTING is \", a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "c0d15b3e40194d2d3e9589dc4d30bbae36c8c032"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6393,  395],\n",
       "       [ 817, 1444]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "5bea2ff5ef457d44de88fc5f2478251c5647f23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91      6788\n",
      "           1       0.79      0.64      0.70      2261\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      9049\n",
      "   macro avg       0.84      0.79      0.81      9049\n",
      "weighted avg       0.86      0.87      0.86      9049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_boost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5050773eabbd35cac48c952e1c597707cb8de8af"
   },
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
    "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
    "\n",
    "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
    "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:\n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "    \n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4,0.25]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8,0]\n",
    "    bottom = [-1,-1,-1,0,0]\n",
    "    top = [1,1,1,2,1]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fair_metrics_and_plot(data, model, plot=False, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    # fair_metrics function available in the metrics.py file\n",
    "    fair = fair_metrics(data, pred)\n",
    "\n",
    "    if plot:\n",
    "        # plot_fair_metrics function available in the visualisations.py file\n",
    "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
    "        plot_fair_metrics(fair)\n",
    "        display(fair)\n",
    "    \n",
    "    return fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>132870</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>140359</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>264663</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>216864</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>150601</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>310152</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>40</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30162 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education.num  capital.gain  capital.loss  hours.per.week  \\\n",
       "1       82  132870              9             0          4356              18   \n",
       "3       54  140359              4             0          3900              40   \n",
       "4       41  264663             10             0          3900              40   \n",
       "5       34  216864              9             0          3770              45   \n",
       "6       38  150601              6             0          3770              40   \n",
       "...    ...     ...            ...           ...           ...             ...   \n",
       "32556   22  310152             10             0             0              40   \n",
       "32557   27  257302             12             0             0              38   \n",
       "32558   40  154374              9             0             0              40   \n",
       "32559   58  151910              9             0             0              40   \n",
       "32560   22  201490              9             0             0              20   \n",
       "\n",
       "      income  workclass  education  marital.status  occupation  relationship  \\\n",
       "1          0          2         11               6           3             1   \n",
       "3          0          2          5               0           6             4   \n",
       "4          0          2         15               5           9             3   \n",
       "5          0          2         11               0           7             4   \n",
       "6          0          2          0               5           0             4   \n",
       "...      ...        ...        ...             ...         ...           ...   \n",
       "32556      0          2         15               4          10             1   \n",
       "32557      0          2          7               2          12             5   \n",
       "32558      1          2         11               2           6             0   \n",
       "32559      0          2         11               6           0             4   \n",
       "32560      0          2         11               4           0             3   \n",
       "\n",
       "       race  sex  native.country  \n",
       "1         4    0              38  \n",
       "3         4    0              38  \n",
       "4         4    0              38  \n",
       "5         4    0              38  \n",
       "6         4    1              38  \n",
       "...     ...  ...             ...  \n",
       "32556     4    1              38  \n",
       "32557     4    0              38  \n",
       "32558     4    1              38  \n",
       "32559     4    0              38  \n",
       "32560     4    1              38  \n",
       "\n",
       "[30162 rows x 15 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "dataset_orig = StandardDataset(data,\n",
    "                                  label_name='income',\n",
    "                                  protected_attribute_names=['sex'],\n",
    "                                  favorable_classes=[1],\n",
    "                                  privileged_classes=[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.200159\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train = pd.read_pickle('../../Results/VotingClassifier/4-prediction-ensemble-methods-multiple-ml-s-acc-86_Train.pkl')\n",
    "data_orig_test = pd.read_pickle('../../Results/VotingClassifier/4-prediction-ensemble-methods-multiple-ml-s-acc-86_Test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "nb_path = ipynbname.path()\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "import pickle\n",
    "\n",
    "#data_orig_train, data_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "X_train = data_orig_train.features\n",
    "y_train = data_orig_train.labels.ravel()\n",
    "\n",
    "X_test = data_orig_test.features\n",
    "y_test = data_orig_test.labels.ravel()\n",
    "\n",
    "\n",
    "lg = LogisticRegression()\n",
    "rf = RandomForestClassifier(criterion = 'gini',\n",
    "    max_depth = 6,\n",
    "    max_features = 'auto',\n",
    "    n_estimators = 100,\n",
    "    random_state = 123)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "tree = DecisionTreeClassifier(random_state = 123)\n",
    "\n",
    "\n",
    "estimators = [('logistic Regression', lg), ('random forrest', rf), ('knn neighbors', knn),(' decision tree', tree)]\n",
    "\n",
    "\n",
    "votingC = VotingClassifier(estimators=[('logistic Regression', lg), ('random forrest', rf), ('knn neighbors', knn),(' decision tree', tree)], voting='soft')\n",
    "\n",
    "\n",
    "model = votingC.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "mdl = model.fit(X_train, y_train)\n",
    "#with open('../../Results/VotingClassifier/' + nb_fname + '.pkl', 'wb') as f:\n",
    "#    pickle.dump(mdl, f)\n",
    "\n",
    "#with open('../../Results/VotingClassifier/' + nb_fname + '_Train' + '.pkl', 'wb') as f:\n",
    "#    pickle.dump(data_orig_train, f) \n",
    "    \n",
    "#with open('../../Results/VotingClassifier/' + nb_fname + '_Test' + '.pkl', 'wb') as f:\n",
    "#    pickle.dump(data_orig_test, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "final_metrics = []\n",
    "accuracy = []\n",
    "list_estimators = []\n",
    "f1= []\n",
    "\n",
    "for name, ml_model in estimators:\n",
    "    \n",
    "    list_estimators.append((name,ml_model))\n",
    "    #print(list_estimators)\n",
    "    model = VotingClassifier(estimators=list_estimators, voting='soft', flatten_transform=False)\n",
    "    #list_estimators = []\n",
    "    mdl = model.fit(X_train, y_train)\n",
    "    yy = mdl.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, yy))\n",
    "    f1.append(f1_score(y_test, yy))\n",
    "    fair = get_fair_metrics_and_plot(data_orig_test, mdl)        \n",
    "    fair_list = fair.iloc[1].tolist()\n",
    "    #fair_list.insert(0, i)\n",
    "    final_metrics.append(fair_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92 0.95 0.82 0.92 0.77 0.84 0.75 0.97 0.96 0.5  0.76 0.4  0.98 0.82\n",
      " 0.53 0.06 0.97 0.58 0.91 0.9  0.91 0.84 0.56 0.9  0.71 0.91 0.91 0.96\n",
      " 0.98 0.85 0.76 0.83 0.56 0.44 0.59 0.95 0.95 0.16 0.82 0.91 0.91 0.91\n",
      " 0.69 0.82 0.03 0.79 0.96 0.74 0.91 0.88 0.87 0.57 0.82 0.96 0.76 0.52\n",
      " 0.97 0.88 0.8  0.88 0.92 0.84 0.92 0.57 0.02 0.92 0.79 0.92 0.08 0.84\n",
      " 0.88 0.91 0.86 0.71 0.79 0.87 0.91 0.89 0.97 0.85 0.88 0.9  0.62 0.7\n",
      " 0.82 0.85 0.54 0.5  0.97 0.91 0.55 0.84 0.87 0.88 0.85 0.97 0.87 0.89\n",
      " 0.81 0.56 0.92 0.54 0.96 0.89 0.74 0.97 0.81 0.9  0.72 0.78 0.95 0.72\n",
      " 0.7  0.87 0.9  0.45 0.03 0.38 0.66 0.84 0.78 0.83 0.88 0.89 0.65 0.84\n",
      " 0.7  0.87 0.48 0.95 0.43 0.96 0.94 0.34 0.92 0.93 0.85 0.9  0.84 0.75\n",
      " 0.68 0.93 0.85 0.88 0.28 0.78 0.84 0.55 0.46 0.53 0.89 0.8  0.9  0.54\n",
      " 0.94 0.84 0.67 0.91 0.86 0.71 0.53 0.83 0.6  0.8  0.84 0.93 0.96 0.96\n",
      " 0.87 0.93 0.9  0.8  0.57 0.03 0.86 0.93 0.69 0.95 0.62 0.78 0.89 0.95\n",
      " 0.87 0.8  0.86 0.78 0.55 0.91 0.81 0.97 0.02 0.9  0.96 0.61 0.96 0.87\n",
      " 0.98 0.51 0.69 0.88 0.86 0.27 0.91 0.86 0.44 0.75 0.08 0.7  0.58 0.69\n",
      " 0.96 0.35 0.87 0.56 0.82 0.86 0.51 0.15 0.05 0.89 0.93 0.48 0.98 0.93\n",
      " 0.96 0.96 0.84 0.93 0.89 0.81 0.66 0.66 0.56 0.88 0.86 0.07 0.91 0.38\n",
      " 0.96 0.79 0.93 0.72 0.84 0.97 0.82 0.88 0.62 0.95 0.82 0.17 0.92 0.85\n",
      " 0.74 0.72 0.81 0.93 0.86 0.6  0.92 0.26 0.93 0.89 0.79 0.84 0.77 0.52\n",
      " 0.91 0.57 0.93 0.64 0.96 0.56 0.76 0.8  0.35 0.84 0.53 0.85 0.5  0.79\n",
      " 0.87 0.93 0.87 0.43 0.86 0.59 0.1  0.83 0.91 0.88 0.89 0.96 0.94 0.58\n",
      " 0.86 0.6  0.86 0.79 0.74 0.84 0.04 0.92 0.98 0.61 0.78 0.85 0.91 0.88\n",
      " 0.9  0.96 0.2  0.55 0.81 0.92 0.69 0.89 0.82 0.9  0.77 0.41 0.04 0.85\n",
      " 0.9  0.9  0.87 0.21 0.86 0.47 0.71 0.72 0.57 0.05 0.86 0.97 0.96 0.8\n",
      " 0.77 0.87 0.92 0.86 0.95 0.9  0.66 0.87 0.91 0.95 0.47 0.79 0.86 0.88\n",
      " 0.95 0.48 0.89 0.97 0.59 0.05 0.85 0.9  0.93 0.76 0.76 0.89 0.68 0.72\n",
      " 0.86 0.79 0.91 0.85 0.44 0.81 0.91 0.03 0.78 0.27 0.84 0.58 0.91 0.41\n",
      " 0.95 0.76 0.82 0.92 0.04 0.92 0.92 0.84 0.82 0.89 0.42 0.63 0.83 0.95\n",
      " 0.78 0.26 0.92 0.89 0.23 0.84 0.84 0.96 0.54 0.62 0.92 0.78 0.86 0.87\n",
      " 0.69 0.9  0.89 0.55 0.89 0.6  0.9  0.81 0.09 0.83 0.72 0.69 0.76 0.48\n",
      " 0.84 0.96 0.95 0.81 0.65 0.91 0.77 0.57 0.92 0.54 0.93 0.08 0.85 0.83\n",
      " 0.13 0.73 0.62 0.69 0.86 0.92 0.93 0.94 0.96 0.91 0.76 0.05 0.85 0.83\n",
      " 0.91 0.92 0.82 0.77 0.79 0.94 0.84 0.91 0.26 0.72 0.96 0.47 0.9  0.93\n",
      " 0.71 0.44 0.81 0.91 0.85 0.96 0.5  0.87 0.03 0.91 0.91 0.15 0.66 0.92\n",
      " 0.71 0.88 0.48 0.96 0.65 0.85 0.95 0.97 0.28 0.52 0.18 0.67 0.92 0.75\n",
      " 0.89 0.85 0.32 0.92 0.85 0.9  0.32 0.91 0.96 0.97 0.7  0.84 0.82 0.43\n",
      " 0.91 0.72 0.55 0.87 0.47 0.67 0.54 0.95 0.97 0.66 0.93 0.97 0.83 0.54\n",
      " 0.75 0.96 0.92 0.08 0.51 0.86 0.9  0.87 0.65 0.83 0.09 0.81 0.53 0.95\n",
      " 0.72 0.93 0.03 0.74 0.84 0.84 0.94 0.52 0.86 0.75 0.24 0.96 0.81 0.9\n",
      " 0.86 0.52 0.93 0.22 0.06 0.82 0.78 0.95 0.78 0.77 0.91 0.54 0.88 0.7\n",
      " 0.55 0.07 0.49 0.25 0.87 0.42 0.95 0.52 0.95 0.93 0.82 0.29 0.8  0.73\n",
      " 0.68 0.75 0.98 0.88 0.57 0.92 0.96 0.73 0.82 0.93 0.4  0.85 0.89 0.95\n",
      " 0.84 0.78 0.72 0.97 0.86 0.87 0.87 0.83 0.89 0.79 0.97 0.67 0.56 0.96\n",
      " 0.54 0.79 0.9  0.86 0.91 0.88 0.89 0.83 0.88 0.82 0.94 0.83 0.57 0.94\n",
      " 0.86 0.75 0.76 0.88 0.88 0.93 0.88 0.91 0.88 0.68 0.97 0.87 0.78 0.88\n",
      " 0.86 0.94 0.91 0.91 0.82 0.89 0.82 0.94 0.82 0.78 0.75 0.67 0.81 0.95\n",
      " 0.96 0.55 0.97 0.82 0.8  0.72 0.51 0.97 0.74 0.81 0.96 0.85 0.77 0.82\n",
      " 0.97 0.86 0.96 0.75 0.52 0.06 0.9  0.55 0.82 0.71 0.5  0.87 0.5  0.67\n",
      " 0.78 0.82 0.97 0.75 0.82 0.12 0.54 0.96 0.69 0.67 0.29 0.91 0.48 0.83\n",
      " 0.83 0.63 0.88 0.68 0.8  0.96 0.75 0.85 0.61 0.98 0.54 0.95 0.63 0.74\n",
      " 0.8  0.61 0.54 0.81 0.85 0.97 0.03 0.9  0.93 0.75 0.84 0.9  0.58 0.73\n",
      " 0.08 0.87 0.88 0.67 0.86 0.38 0.35 0.55 0.53 0.9  0.97 0.97 0.96 0.81\n",
      " 0.87 0.9  0.49 0.96 0.85 0.64 0.48 0.64 0.85 0.47 0.83 0.63 0.85 0.47\n",
      " 0.54 0.6  0.94 0.88 0.89 0.83 0.41 0.81 0.53 0.85 0.79 0.6  0.81 0.74\n",
      " 0.83 0.97 0.68 0.86 0.49 0.92 0.77 0.85 0.85 0.96 0.83 0.79 0.83 0.88\n",
      " 0.48 0.82 0.9  0.6  0.52 0.85 0.62 0.93 0.76 0.46 0.8  0.89 0.87 0.8\n",
      " 0.81 0.67 0.92 0.92 0.62 0.05 0.96 0.89 0.76 0.81 0.47 0.86 0.33 0.88\n",
      " 0.12 0.6  0.82 0.81 0.9  0.48 0.85 0.54 0.85 0.89 0.93 0.39 0.59 0.83\n",
      " 0.84 0.92 0.87 0.81 0.5  0.82 0.86 0.48 0.82 0.91 0.91 0.92 0.77 0.94\n",
      " 0.52 0.88 0.91 0.36 0.61 0.03 0.87 0.53 0.92 0.83 0.61 0.79 0.86 0.79\n",
      " 0.81 0.83 0.88 0.77 0.57 0.85 0.43 0.84 0.93 0.53 0.92 0.97 0.81 0.4\n",
      " 0.87 0.76 0.89 0.85 0.04 0.93 0.92 0.89 0.91 0.56 0.95 0.05 0.85 0.84\n",
      " 0.88 0.86 0.97 0.58 0.95 0.92 0.97 0.14 0.77 0.89 0.5  0.69 0.88 0.9\n",
      " 0.8  0.84 0.93 0.3  0.86 0.96 0.05 0.77 0.71 0.86 0.91 0.92 0.93 0.96\n",
      " 0.92 0.55 0.86 0.92 0.96 0.97 0.91 0.55 0.94 0.47 0.92 0.95 0.74 0.91\n",
      " 0.9  0.86 0.74 0.56 0.89 0.98 0.91 0.52 0.97 0.8  0.87 0.68 0.91 0.89\n",
      " 0.31 0.63 0.58 0.11 0.88 0.83 0.51 0.9  0.56 0.76 0.32 0.9  0.9  0.93\n",
      " 0.66 0.88 0.1  0.82 0.81 0.64 0.64 0.89 0.51 0.77 0.82 0.98 0.94 0.91\n",
      " 0.08 0.88 0.78 0.55 0.92 0.9  0.86 0.89 0.8  0.49 0.61 0.54 0.02 0.83\n",
      " 0.58 0.35 0.91 0.35 0.6  0.78 0.72 0.58 0.98 0.73 0.93 0.85 0.87 0.86\n",
      " 0.53 0.43 0.46 0.91 0.91 0.43 0.81 0.87 0.96 0.91 0.88 0.96 0.94 0.88\n",
      " 0.44 0.32 0.81 0.86 0.8  0.84 0.82 0.76 0.81 0.85 0.71 0.8  0.96 0.94\n",
      " 0.65 0.97 0.06 0.98 0.88 0.9  0.89 0.07 0.72 0.91 0.87 0.9  0.85 0.96\n",
      " 0.44 0.86 0.52 0.96 0.97 0.95 0.72 0.03 0.44 0.97 0.78 0.93 0.13 0.82\n",
      " 0.07 0.53 0.05 0.54 0.87 0.85 0.96 0.7  0.89 0.87 0.86 0.8  0.64 0.67\n",
      " 0.43 0.96 0.59 0.58 0.83 0.83 0.95 0.72 0.89 0.29 0.97 0.8  0.8  0.96\n",
      " 0.58 0.83 0.62 0.89 0.78 0.55 0.92 0.58 0.84 0.42 0.59 0.41 0.48 0.52\n",
      " 0.85 0.92 0.87 0.83 0.6  0.92 0.52 0.88 0.33 0.84 0.58 0.82 0.94 0.58\n",
      " 0.21 0.92 0.63 0.85 0.93 0.73 0.8  0.8  0.95 0.84 0.83 0.8  0.1  0.91\n",
      " 0.84 0.77 0.78 0.86 0.74 0.86 0.82 0.89 0.83 0.8  0.79 0.83 0.05 0.87\n",
      " 0.92 0.84 0.93 0.8  0.9  0.93 0.48 0.96 0.61 0.92 0.5  0.74 0.87 0.69\n",
      " 0.86 0.85 0.87 0.93 0.51 0.66 0.68 0.87 0.88 0.67 0.85 0.8  0.93 0.74\n",
      " 0.8  0.14 0.54 0.87 0.61 0.76 0.87 0.87 0.9  0.75 0.94 0.48 0.88 0.89\n",
      " 0.51 0.77 0.88 0.79 0.95 0.79 0.86 0.68 0.96 0.92 0.7  0.86 0.97 0.84\n",
      " 0.95 0.96 0.89 0.88 0.83 0.81 0.89 0.29 0.38 0.95 0.8  0.84 0.63 0.01\n",
      " 0.91 0.15 0.79 0.97 0.6  0.61 0.47 0.65 0.45 0.53 0.52 0.62 0.05 0.8\n",
      " 0.76 0.92 0.8  0.87 0.6  0.87 0.64 0.79 0.85 0.9  0.47 0.89 0.85 0.91\n",
      " 0.88 0.84 0.78 0.74 0.49 0.91 0.09 0.86 0.85 0.5  0.76 0.81 0.82 0.76\n",
      " 0.01 0.97 0.04 0.92 0.81 0.78 0.89 0.41 0.94 0.79 0.2  0.83 0.91 0.89\n",
      " 0.45 0.82 0.84 0.94 0.91 0.53 0.96 0.79 0.49 0.91 0.89 0.41 0.92 0.78\n",
      " 0.9  0.89 0.84 0.84 0.44 0.97 0.98 0.92 0.85 0.85 0.97 0.83 0.92 0.9\n",
      " 0.57 0.89 0.79 0.88 0.85 0.66 0.81 0.84 0.94 0.72 0.87 0.96 0.48 0.96\n",
      " 0.11 0.12 0.09 0.8  0.63 0.9  0.95 0.88 0.82 0.72 0.47 0.62 0.67 0.84\n",
      " 0.83 0.92 0.79 0.52 0.89 0.45 0.05 0.02 0.98 0.87 0.82 0.71 0.42 0.88\n",
      " 0.93 0.97 0.56 0.87 0.93 0.97 0.77 0.46 0.88 0.89 0.97 0.91 0.85 0.58\n",
      " 0.59 0.6  0.93 0.58 0.84 0.2  0.97 0.78 0.89 0.42 0.97 0.32 0.04 0.81\n",
      " 0.94 0.86 0.24 0.91 0.66 0.53 0.09 0.85 0.9  0.92 0.91 0.86 0.89 0.96\n",
      " 0.91 0.68 0.24 0.7  0.82 0.78 0.78 0.54 0.74 0.95 0.53 0.81 0.79 0.79\n",
      " 0.91 0.84 0.61 0.78 0.08 0.92 0.42 0.87 0.63 0.88 0.54 0.04 0.91 0.98\n",
      " 0.74 0.75 0.89 0.53 0.92 0.45 0.94 0.73 0.87 0.53 0.79 0.9  0.24 0.96\n",
      " 0.82 0.76 0.11 0.81 0.51 0.88 0.45 0.91 0.74 0.86 0.6  0.06 0.81 0.78\n",
      " 0.88 0.96 0.75 0.94 0.54 0.93 0.84 0.87 0.91 0.5  0.93 0.58 0.5  0.55\n",
      " 0.55 0.85 0.82 0.89 0.93 0.6  0.86 0.95 0.88 0.55 0.86 0.48 0.47 0.9\n",
      " 0.79 0.68 0.81 0.88 0.49 0.6  0.88 0.86 0.08 0.94 0.85 0.28 0.95 0.87\n",
      " 0.92 0.54 0.4  0.9  0.82 0.73 0.78 0.83 0.96 0.52 0.95 0.75 0.94 0.39\n",
      " 0.45 0.78 0.59 0.43 0.78 0.86 0.91 0.49 0.9  0.94 0.86 0.78 0.9  0.78\n",
      " 0.9  0.95 0.96 0.55 0.87 0.9  0.82 0.87 0.63 0.67 0.89 0.95 0.91 0.58\n",
      " 0.78 0.84 0.77 0.85 0.81 0.85 0.84 0.87 0.9  0.75 0.51 0.45 0.87 0.87\n",
      " 0.87 0.68 0.72 0.93 0.88 0.48 0.88 0.91 0.57 0.81 0.87 0.7  0.81 0.91\n",
      " 0.92 0.89 0.87 0.67 0.93 0.93 0.91 0.2  0.73 0.85 0.08 0.88 0.46 0.87\n",
      " 0.55 0.55 0.91 0.88 0.96 0.96 0.93 0.82 0.84 0.82 0.63 0.87 0.96 0.86\n",
      " 0.86 0.6  0.85 0.94 0.45 0.62 0.98 0.61 0.07 0.92 0.97 0.88 0.87 0.61\n",
      " 0.79 0.78 0.9  0.49 0.82 0.91 0.95 0.79 0.87 0.92 0.68 0.89 0.8  0.97\n",
      " 0.9  0.8  0.92 0.57 0.87 0.88 0.81 0.83 0.94 0.79 0.87 0.79 0.56 0.6\n",
      " 0.84 0.88 0.92 0.03 0.74 0.81 0.9  0.93 0.79 0.93 0.9  0.54 0.56 0.87\n",
      " 0.95 0.41 0.83 0.96 0.88 0.04 0.81 0.9  0.8  0.92 0.92 0.5  0.82 0.58\n",
      " 0.85 0.56 0.13 0.44 0.75 0.88 0.94 0.83 0.87 0.89 0.17 0.86 0.55 0.86\n",
      " 0.87 0.93 0.88 0.83 0.93 0.88 0.83 0.84 0.83 0.88 0.94 0.83 0.76 0.87\n",
      " 0.98 0.28 0.88 0.73 0.84 0.91 0.93 0.85 0.32 0.2  0.9  0.15 0.33 0.91\n",
      " 0.77 0.74 0.87 0.95 0.51 0.86 0.92 0.55 0.73 0.82 0.89 0.42 0.8  0.85\n",
      " 0.77 0.27 0.92 0.91 0.86 0.94 0.95 0.87 0.63 0.81 0.88 0.97 0.54 0.18\n",
      " 0.89 0.96 0.92 0.91 0.07 0.96 0.43 0.31 0.95 0.47 0.97 0.84 0.97 0.74\n",
      " 0.83 0.78 0.87 0.91 0.86 0.68 0.62 0.9  0.88 0.73 0.65 0.96 0.82 0.73\n",
      " 0.76 0.95 0.88 0.79 0.96 0.73 0.83 0.17 0.47 0.9  0.44 0.97 0.56 0.4\n",
      " 0.86 0.86 0.9  0.91 0.95 0.3  0.87 0.94 0.84 0.55 0.45 0.47 0.65 0.65\n",
      " 0.61 0.96 0.7  0.18 0.48 0.82 0.2  0.88 0.69 0.97 0.57 0.09 0.55 0.74\n",
      " 0.8  0.8  0.92 0.83 0.61 0.9  0.88 0.62 0.28 0.91 0.78 0.85 0.84 0.87\n",
      " 0.34 0.95 0.97 0.97 0.92 0.89 0.96 0.88 0.49 0.88 0.13 0.93 0.94 0.9\n",
      " 0.87 0.86 0.89 0.9  0.82 0.84 0.84 0.43 0.25 0.71 0.95 0.45 0.93 0.53\n",
      " 0.74 0.11 0.52 0.75 0.78 0.88 0.79 0.47 0.97 0.86 0.9  0.9  0.86 0.9\n",
      " 0.49 0.88 0.82 0.71 0.88 0.97 0.17 0.87 0.84 0.94 0.97 0.73 0.72 0.83\n",
      " 0.84 0.59 0.98 0.87 0.87 0.9  0.92 0.52 0.97 0.57 0.87 0.85 0.08 0.88\n",
      " 0.62 0.65 0.89 0.57 0.85 0.87 0.93 0.07 0.47 0.98 0.58 0.8  0.87 0.91\n",
      " 0.95 0.88 0.88 0.92 0.91 0.6  0.95 0.83 0.51 0.91 0.37 0.85 0.97 0.75\n",
      " 0.92 0.66 0.87 0.48 0.95 0.96 0.55 0.95 0.76 0.77 0.87 0.95 0.86 0.53\n",
      " 0.84 0.39 0.9  0.87 0.5  0.87 0.93 0.04 0.87 0.91 0.81 0.81 0.8  0.81\n",
      " 0.81 0.87 0.81 0.51 0.45 0.98 0.97 0.53 0.93 0.39 0.53 0.84 0.49 0.9\n",
      " 0.27 0.86 0.93 0.92 0.87 0.93 0.78 0.8  0.89 0.96 0.46 0.8  0.72 0.89\n",
      " 0.87 0.75 0.86 0.95 0.77 0.51 0.94 0.85 0.82 0.77 0.42 0.95 0.76 0.96\n",
      " 0.95 0.85 0.76 0.85 0.7  0.86 0.84 0.79 0.86 0.82 0.69 0.89 0.81 0.9\n",
      " 0.69 0.95 0.85 0.03 0.83 0.8  0.89 0.47 0.83 0.3  0.49 0.92 0.83 0.92\n",
      " 0.91 0.8  0.89 0.87 0.95 0.6  0.91 0.64 0.93 0.91 0.86 0.53 0.92 0.02\n",
      " 0.87 0.8  0.81 0.77 0.56 0.18 0.61 0.92 0.64 0.93 0.53 0.64 0.6  0.9\n",
      " 0.95 0.88 0.93 0.89 0.78 0.68 0.11 0.88 0.82 0.96 0.38 0.97 0.66 0.85\n",
      " 0.05 0.52 0.61 0.63 0.78 0.88 0.71 0.9  0.86 0.82 0.05 0.77 0.16 0.73\n",
      " 0.76 0.55 0.49 0.84 0.93 0.52 0.97 0.64 0.96 0.75 0.88 0.18 0.91 0.83\n",
      " 0.76 0.93 0.86 0.92 0.75 0.89 0.43 0.08 0.87 0.92 0.81 0.33 0.65 0.54\n",
      " 0.9  0.97 0.45 0.95 0.95 0.66 0.92 0.9  0.81 0.6  0.93 0.79 0.51 0.86\n",
      " 0.81 0.86 0.85 0.56 0.82 0.55 0.97 0.55 0.59 0.79 0.97 0.88 0.94 0.96\n",
      " 0.86 0.79 0.94 0.79 0.4  0.96 0.55 0.77 0.63 0.88 0.95 0.87 0.94 0.92\n",
      " 0.83 0.91 0.61 0.06 0.76 0.71 0.58 0.76 0.48 0.73 0.4  0.5  0.86 0.89\n",
      " 0.56 0.78 0.63 0.82 0.96 0.92 0.87 0.97 0.85 0.53 0.89 0.8  0.84 0.49\n",
      " 0.49 0.97 0.92 0.88 0.43 0.92 0.56 0.96 0.91 0.52 0.86 0.9  0.84 0.97\n",
      " 0.7  0.92 0.93 0.87 0.86 0.92 0.85 0.92 0.93 0.91 0.84 0.58 0.07 0.89\n",
      " 0.07 0.96 0.89 0.87 0.7  0.42 0.79 0.96 0.89 0.09 0.78 0.93 0.9  0.84\n",
      " 0.46 0.52 0.96 0.55 0.83 0.48 0.93 0.98 0.62 0.89 0.39 0.7  0.88 0.56\n",
      " 0.77 0.47 0.35 0.6  0.87 0.41 0.56 0.67 0.51 0.46 0.86 0.6  0.53 0.86\n",
      " 0.92 0.84 0.09 0.83 0.59 0.83 0.61 0.51 0.87 0.79 0.41 0.76 0.5  0.81\n",
      " 0.81 0.93 0.79 0.6  0.86 0.5  0.85 0.98 0.63 0.95 0.52 0.89 0.87 0.61\n",
      " 0.49 0.55 0.22 0.89 0.81 0.56 0.55 0.89 0.31 0.96 0.95 0.46 0.89 0.55\n",
      " 0.9  0.61 0.91 0.83 0.92 0.58 0.89 0.57 0.82 0.85 0.92 0.5  0.79 0.97\n",
      " 0.52 0.6  0.9  0.96 0.88 0.5  0.87 0.41 0.89 0.76 0.92 0.89 0.8  0.87\n",
      " 0.43 0.53 0.91 0.78 0.69 0.02 0.97 0.23 0.9  0.93 0.77 0.89 0.81 0.91\n",
      " 0.86 0.87 0.75 0.9  0.96 0.92 0.38 0.91 0.89 0.02 0.58 0.79 0.95 0.63\n",
      " 0.33 0.94 0.67 0.81 0.94 0.41 0.91 0.81 0.89 0.9  0.73 0.62 0.83 0.7\n",
      " 0.8  0.84 0.55 0.94 0.96 0.9  0.88 0.88 0.89 0.78 0.04 0.83 0.96 0.94\n",
      " 0.93 0.26 0.94 0.45 0.78 0.82 0.5  0.17 0.9  0.92 0.47 0.82 0.14 0.86\n",
      " 0.92 0.83 0.49 0.89 0.89 0.39 0.87 0.84 0.92 0.77 0.92 0.56 0.92 0.61\n",
      " 0.85 0.48 0.96 0.85 0.91 0.71 0.92 0.8  0.56 0.54 0.55 0.87 0.38 0.89\n",
      " 0.89 0.49 0.83 0.84 0.95 0.87 0.97 0.5  0.95 0.41 0.89 0.96 0.83 0.87\n",
      " 0.94 0.15 0.79 0.86 0.73 0.92 0.93 0.85 0.56 0.86 0.92 0.89 0.67 0.56\n",
      " 0.86 0.76 0.43 0.82 0.77 0.59 0.59 0.36 0.59 0.91 0.42 0.76 0.81 0.03\n",
      " 0.71 0.6  0.48 0.92 0.8  0.92 0.87 0.94 0.57 0.82 0.92 0.91 0.86 0.77\n",
      " 0.9  0.68 0.87 0.79 0.73 0.81 0.92 0.9  0.51 0.43 0.86 0.8  0.56 0.66\n",
      " 0.78 0.07 0.2  0.97 0.91 0.88 0.83 0.87 0.77 0.9  0.87 0.77 0.86 0.06\n",
      " 0.44 0.87 0.69 0.89 0.6  0.68 0.98 0.83 0.91 0.97 0.87 0.94 0.77 0.88\n",
      " 0.85 0.54 0.55 0.94 0.84 0.57 0.81 0.48 0.92 0.92 0.85 0.97 0.85 0.85\n",
      " 0.68 0.91 0.76 0.76 0.96 0.59 0.59 0.45 0.24 0.92 0.88 0.57 0.08 0.96\n",
      " 0.91 0.84 0.97 0.86 0.91 0.63 0.56 0.05 0.89 0.31 0.87 0.71 0.74 0.76\n",
      " 0.98 0.88 0.96 0.97 0.82 0.47 0.87 0.85 0.98 0.9  0.83 0.79 0.7  0.81\n",
      " 0.92 0.95 0.88 0.71 0.82 0.44 0.58 0.83 0.96 0.37 0.6  0.54 0.57 0.54\n",
      " 0.82 0.86 0.8  0.55 0.74 0.57 0.59 0.8  0.03 0.52 0.03 0.91 0.71 0.87\n",
      " 0.92 0.96 0.11 0.59 0.74 0.09 0.95 0.49 0.41 0.82 0.43 0.98 0.92 0.85\n",
      " 0.59 0.81 0.84 0.43 0.91 0.96 0.78 0.81 0.89 0.68 0.98 0.3  0.86 0.62\n",
      " 0.95 0.72 0.78 0.79 0.44 0.67 0.83 0.82 0.75 0.87 0.55 0.9  0.77 0.75\n",
      " 0.91 0.87 0.53 0.39 0.85 0.82 0.83 0.91 0.6  0.71 0.82 0.89 0.93 0.25\n",
      " 0.91 0.74 0.19 0.87 0.86 0.83 0.89 0.83 0.87 0.9  0.56 0.62 0.51 0.92\n",
      " 0.83 0.96 0.86 0.84 0.62 0.96 0.85 0.66 0.15 0.91 0.52 0.96 0.72 0.67\n",
      " 0.82 0.92 0.53 0.79 0.76 0.95 0.79 0.76 0.8  0.97 0.89 0.91 0.85 0.57\n",
      " 0.96 0.8  0.54 0.48 0.65 0.91 0.93 0.7  0.96 0.93 0.79 0.83 0.73 0.49\n",
      " 0.93 0.88 0.92 0.06 0.8  0.92 0.79 0.1  0.93 0.95 0.83 0.89 0.63 0.48\n",
      " 0.86 0.87 0.92 0.78 0.65 0.61 0.89 0.78 0.05 0.84 0.78 0.61 0.95 0.85\n",
      " 0.03 0.85 0.91 0.9  0.81 0.84 0.81 0.96 0.91 0.8  0.87 0.87 0.92 0.75\n",
      " 0.9  0.8  0.83 0.87 0.48 0.47 0.88 0.97 0.87 0.78 0.42 0.81 0.55 0.95\n",
      " 0.64 0.86 0.8  0.8  0.44 0.84 0.93 0.75 0.67 0.83 0.89 0.91 0.08 0.69\n",
      " 0.76 0.64 0.55 0.95 0.87 0.81 0.93 0.9  0.82 0.97 0.53 0.88 0.57 0.29\n",
      " 0.86 0.93 0.55 0.08 0.83 0.96 0.87 0.94 0.98 0.89 0.87 0.92 0.82 0.96\n",
      " 0.65 0.95 0.77 0.87 0.37 0.57 0.92 0.56 0.67 0.92 0.83 0.91 0.93 0.7\n",
      " 0.84 0.47 0.87 0.85 0.06 0.72 0.92 0.33 0.84 0.6  0.86 0.93 0.76 0.44\n",
      " 0.58 0.94 0.96 0.91 0.92 0.53 0.62 0.58 0.53 0.86 0.79 0.93 0.98 0.44\n",
      " 0.33 0.91 0.92 0.56 0.75 0.92 0.92 0.87 0.66 0.85 0.44 0.23 0.85 0.85\n",
      " 0.86 0.59 0.74 0.84 0.89 0.93 0.51 0.96 0.15 0.42 0.82 0.91 0.84 0.82\n",
      " 0.31 0.03 0.88 0.92 0.82 0.96 0.36 0.93 0.92 0.44 0.81 0.49 0.87 0.89\n",
      " 0.83 0.72 0.54 0.02 0.92 0.83 0.84 0.75 0.92 0.85 0.98 0.49 0.83 0.82\n",
      " 0.92 0.74 0.93 0.45 0.89 0.65 0.87 0.81 0.47 0.73 0.79 0.68 0.15 0.61\n",
      " 0.2  0.12 0.86 0.82 0.74 0.89 0.97 0.84 0.88 0.92 0.57 0.45 0.11 0.82\n",
      " 0.82 0.67 0.22 0.88 0.93 0.89 0.91 0.8  0.41 0.7  0.42 0.4  0.95 0.79\n",
      " 0.74 0.97 0.51 0.89 0.87 0.91 0.77 0.86 0.73 0.9  0.87 0.58 0.81 0.6\n",
      " 0.89 0.45 0.84 0.83 0.52 0.38 0.45 0.87 0.88 0.87 0.85 0.5  0.82 0.48\n",
      " 0.43 0.51 0.75 0.96 0.97 0.14 0.81 0.42 0.91 0.5  0.8  0.97 0.74 0.86\n",
      " 0.88 0.84 0.9  0.96 0.58 0.85 0.09 0.64 0.84 0.93 0.9  0.85 0.09 0.82\n",
      " 0.9  0.88 0.82 0.57 0.88 0.97 0.09 0.58 0.72 0.97 0.83 0.88 0.82 0.9\n",
      " 0.97 0.89 0.47 0.91 0.93 0.93 0.97 0.98 0.83 0.57 0.92 0.45 0.85 0.55\n",
      " 0.79 0.92 0.89 0.62 0.49 0.86 0.84 0.92 0.84 0.8  0.92 0.92 0.86 0.52\n",
      " 0.75 0.77 0.43 0.57 0.62 0.8  0.98 0.66 0.73 0.87 0.95 0.88 0.72 0.91\n",
      " 0.92 0.88 0.81 0.86 0.91 0.91 0.62 0.51 0.96 0.16 0.68 0.9  0.59 0.72\n",
      " 0.86 0.7  0.17 0.91 0.92 0.42 0.55 0.66 0.55 0.93 0.95 0.9  0.63 0.83\n",
      " 0.97 0.98 0.55 0.85 0.92 0.87 0.89 0.52 0.67 0.87 0.91 0.56 0.88 0.39\n",
      " 0.92 0.42 0.49 0.89 0.54 0.92 0.92 0.76 0.88 0.94 0.48 0.72 0.71 0.73\n",
      " 0.53 0.94 0.83 0.55 0.11 0.86 0.52 0.63 0.3  0.81 0.9  0.02 0.93 0.61\n",
      " 0.86 0.46 0.9  0.73 0.94 0.74 0.87 0.95 0.82 0.47 0.9  0.09 0.83 0.93\n",
      " 0.98 0.71 0.86 0.77 0.88 0.78 0.5  0.7  0.9  0.94 0.54 0.8  0.96 0.75\n",
      " 0.51 0.59 0.05 0.86 0.55 0.43 0.03 0.98 0.87 0.58 0.54 0.61 0.56 0.4\n",
      " 0.5  0.93 0.86 0.88 0.89 0.3  0.03 0.79 0.98 0.85 0.5  0.94 0.75 0.94\n",
      " 0.85 0.94 0.81 0.84 0.81 0.89 0.86 0.55 0.45 0.53 0.82 0.94 0.47 0.91\n",
      " 0.89 0.88 0.8  0.94 0.61 0.83 0.9  0.79 0.97 0.97 0.95 0.9  0.85 0.88\n",
      " 0.66 0.91 0.9  0.07 0.55 0.79 0.89 0.85 0.59 0.82 0.94 0.63 0.05 0.86\n",
      " 0.89 0.7  0.91 0.93 0.8  0.92 0.92 0.8  0.84 0.92 0.5  0.81 0.83 0.92\n",
      " 0.8  0.61 0.04 0.49 0.97 0.96 0.55 0.9  0.93 0.46 0.93 0.8  0.67 0.72\n",
      " 0.48 0.95 0.81 0.59 0.86 0.28 0.89 0.86 0.93 0.77 0.64 0.68 0.91 0.91\n",
      " 0.86 0.88 0.84 0.28 0.55 0.93 0.49 0.78 0.86 0.77 0.79 0.81 0.87 0.89\n",
      " 0.49 0.88 0.54 0.84 0.87 0.59 0.59 0.9  0.72 0.82 0.64 0.8  0.76 0.85\n",
      " 0.89 0.15 0.62 0.95 0.89 0.92 0.87 0.96 0.13 0.71 0.98 0.9  0.91 0.84\n",
      " 0.87 0.71 0.78 0.87 0.89 0.96 0.75 0.55 0.79 0.88 0.64 0.36 0.91 0.91\n",
      " 0.81 0.91 0.83 0.79 0.72 0.83 0.9  0.83 0.86 0.82 0.81 0.52 0.81 0.79\n",
      " 0.89 0.93 0.86 0.95 0.85 0.84 0.94 0.85 0.56 0.68 0.87 0.93 0.97 0.9\n",
      " 0.89 0.91 0.17 0.95 0.5  0.97 0.96 0.86 0.84 0.57 0.46 0.87 0.8  0.79\n",
      " 0.85 0.92 0.87 0.58 0.87 0.73 0.75 0.97 0.93 0.77 0.19 0.96 0.94 0.91\n",
      " 0.05 0.79 0.89 0.85 0.53 0.7  0.97 0.76 0.82 0.83 0.97 0.97 0.88 0.87\n",
      " 0.7  0.81 0.83 0.91 0.73 0.84 0.97 0.94 0.85 0.9  0.88 0.97 0.82 0.98\n",
      " 0.81 0.65 0.59 0.86 0.03 0.83 0.46 0.88 0.82 0.96 0.92 0.5  0.88 0.84\n",
      " 0.64 0.83 0.55 0.73 0.85 0.96 0.87 0.35 0.8  0.74 0.92 0.59 0.41 0.65\n",
      " 0.13 0.9  0.96 0.92 0.48 0.49 0.79 0.8  0.79 0.8  0.62 0.65 0.92 0.97\n",
      " 0.75 0.2  0.59 0.88 0.91 0.96 0.71 0.92 0.84 0.81 0.97 0.57 0.93 0.55\n",
      " 0.48 0.93 0.92 0.54 0.95 0.75 0.89 0.73 0.55 0.95 0.9  0.87 0.53 0.88\n",
      " 0.88 0.8  0.97 0.38 0.87 0.84 0.87 0.81 0.94 0.91 0.91 0.86 0.83 0.54\n",
      " 0.92 0.89 0.76 0.49 0.97 0.07 0.81 0.91 0.63 0.91 0.95 0.73 0.84 0.67\n",
      " 0.85 0.14 0.92 0.96 0.94 0.96 0.83 0.89 0.92 0.79 0.97 0.79 0.9  0.84\n",
      " 0.97 0.62 0.93 0.06 0.51 0.91 0.48 0.67 0.86 0.43 0.86 0.86 0.77 0.53\n",
      " 0.92 0.78 0.58 0.35 0.96 0.97 0.79 0.79 0.92 0.46 0.37 0.6  0.82 0.77\n",
      " 0.9  0.62 0.88 0.97 0.77 0.71 0.87 0.81 0.93 0.96 0.91 0.8  0.73 0.87\n",
      " 0.75 0.82 0.92 0.74 0.95 0.91 0.75 0.79 0.85 0.7  0.49 0.87 0.87 0.86\n",
      " 0.02 0.98 0.87 0.59 0.96 0.7  0.85 0.76 0.39 0.82 0.93 0.91 0.97 0.92\n",
      " 0.88 0.77 0.85 0.84 0.65 0.54 0.97 0.61 0.88 0.87 0.68 0.8  0.53 0.79\n",
      " 0.53 0.41 0.96 0.82 0.8  0.5  0.76 0.27 0.83 0.14 0.83 0.96 0.81 0.56\n",
      " 0.88 0.97 0.83 0.58 0.03 0.95 0.88 0.83 0.78 0.61 0.76 0.82 0.5  0.8\n",
      " 0.96 0.96 0.73 0.74 0.96 0.87 0.91 0.83 0.76 0.54 0.73 0.45 0.86 0.87\n",
      " 0.46 0.88 0.64 0.71 0.84 0.91 0.76 0.87 0.48 0.78 0.9  0.76 0.78 0.34\n",
      " 0.14 0.61 0.9  0.92 0.76 0.87 0.91 0.88 0.96 0.83 0.53 0.91 0.77 0.89\n",
      " 0.92 0.96 0.88 0.59 0.84 0.92 0.35 0.89 0.82 0.84 0.74 0.88 0.85 0.86\n",
      " 0.87 0.84 0.92 0.76 0.84 0.78 0.44 0.79 0.91 0.22 0.84 0.92 0.85 0.87\n",
      " 0.93 0.92 0.72 0.83 0.87 0.9  0.95 0.38 0.68 0.83 0.9  0.71 0.9  0.62\n",
      " 0.87 0.82 0.96 0.29 0.67 0.75 0.19 0.58 0.93 0.83 0.56 0.98 0.92 0.81\n",
      " 0.57 0.79 0.84 0.97 0.97 0.91 0.79 0.97 0.89 0.53 0.97 0.04 0.93 0.58\n",
      " 0.82 0.86 0.49 0.76 0.84 0.91 0.85 0.52 0.91 0.8  0.91 0.75 0.45 0.9\n",
      " 0.02 0.55 0.69 0.92 0.74 0.93 0.75 0.94 0.7  0.87 0.88 0.03 0.89 0.89\n",
      " 0.72 0.9  0.98 0.61 0.7  0.89 0.82 0.88 0.13 0.48 0.91 0.8  0.45 0.87\n",
      " 0.86 0.54 0.64 0.76 0.9  0.84 0.96 0.91 0.52 0.5  0.91 0.96 0.97 0.91\n",
      " 0.8  0.61 0.87 0.86 0.51 0.65 0.77 0.6  0.83 0.88 0.91 0.92 0.92 0.9\n",
      " 0.77 0.71 0.87 0.92 0.02 0.68 0.76 0.73 0.55 0.75 0.71 0.65 0.75 0.47\n",
      " 0.69 0.89 0.88 0.52 0.52 0.87 0.91 0.92 0.78 0.88 0.83 0.96 0.7  0.86\n",
      " 0.83 0.89 0.9  0.91 0.48 0.74 0.83 0.82 0.53 0.85 0.86 0.51 0.92 0.54\n",
      " 0.07 0.23 0.63 0.93 0.86 0.59 0.69 0.54 0.45 0.79 0.6  0.91 0.51 0.97\n",
      " 0.6  0.6  0.84 0.84 0.56 0.96 0.62 0.5  0.86 0.63 0.95 0.15 0.84 0.82\n",
      " 0.48 0.88 0.84 0.85 0.48 0.69 0.44 0.97 0.75 0.9  0.89 0.58 0.51 0.5\n",
      " 0.91 0.68 0.54 0.48 0.95 0.83 0.74 0.9  0.82 0.85 0.78 0.82 0.85 0.44\n",
      " 0.72 0.88 0.89 0.91 0.87 0.9  0.94 0.77 0.93 0.77 0.96 0.89 0.84 0.92\n",
      " 0.84 0.94 0.64 0.97 0.71 0.03 0.91 0.76 0.39 0.35 0.78 0.8  0.97 0.85\n",
      " 0.81 0.06 0.87 0.37 0.59 0.55 0.94 0.97 0.88 0.43 0.93 0.76 0.8  0.9\n",
      " 0.87 0.48 0.92 0.79 0.93 0.96 0.86 0.88 0.97 0.11 0.45 0.86 0.86 0.88\n",
      " 0.94 0.84 0.95 0.87 0.9  0.89 0.89 0.77 0.74 0.38 0.86 0.68 0.88 0.44\n",
      " 0.79 0.41 0.9  0.73 0.88 0.25 0.76 0.71 0.89 0.84 0.77 0.74 0.82 0.05\n",
      " 0.92 0.55 0.55 0.91 0.34 0.67 0.85 0.71 0.92 0.83 0.82 0.83 0.97 0.5\n",
      " 0.91 0.57 0.88 0.97 0.6  0.83 0.8  0.97 0.66 0.1  0.92 0.45 0.85 0.93\n",
      " 0.83 0.86 0.98 0.95 0.84 0.78 0.87 0.24 0.9  0.87 0.87 0.87 0.78 0.77\n",
      " 0.81 0.38 0.67 0.83 0.79 0.63 0.87 0.92 0.51 0.84 0.84 0.93 0.96 0.87\n",
      " 0.02 0.92 0.53 0.86 0.47 0.75 0.89 0.78 0.8  0.8  0.97 0.92 0.91 0.91\n",
      " 0.81 0.76 0.97 0.88 0.95 0.6  0.82 0.83 0.93 0.92 0.84 0.91 0.94 0.53\n",
      " 0.83 0.64 0.62 0.87 0.88 0.41 0.79 0.87 0.58 0.84 0.81 0.64 0.77 0.95\n",
      " 0.9  0.66 0.83 0.86 0.69 0.85 0.81 0.65 0.96 0.79 0.97 0.87 0.45 0.89\n",
      " 0.97 0.44 0.81 0.91 0.77 0.87 0.42 0.97 0.97 0.04 0.92 0.38 0.44 0.73\n",
      " 0.76 0.51 0.94 0.67 0.96 0.54 0.97 0.79 0.59 0.82 0.9  0.81 0.63 0.71\n",
      " 0.94 0.24 0.81 0.83 0.79 0.91 0.74 0.95 0.83 0.89 0.59 0.71 0.79 0.43\n",
      " 0.91 0.81 0.79 0.85 0.85 0.96 0.83 0.5  0.87 0.95 0.74 0.82 0.85 0.89\n",
      " 0.5  0.51 0.91 0.05 0.6  0.49 0.77 0.93 0.73 0.82 0.54 0.49 0.09 0.82\n",
      " 0.75 0.97 0.49 0.84 0.76 0.96 0.83 0.55 0.9  0.75 0.16 0.9  0.87 0.65\n",
      " 0.97 0.76 0.92 0.07 0.98 0.95 0.93 0.59 0.84 0.93 0.5  0.51 0.84 0.85\n",
      " 0.81 0.94 0.85 0.48 0.81 0.52 0.77 0.93 0.75 0.93 0.76 0.8  0.88 0.95\n",
      " 0.91 0.75 0.87 0.58 0.88 0.94 0.81 0.88 0.91 0.92 0.8  0.54 0.88 0.49\n",
      " 0.84 0.34 0.88 0.8  0.72 0.81 0.64 0.78 0.03 0.37 0.6  0.62 0.55 0.97\n",
      " 0.48 0.98 0.89 0.54 0.76 0.9  0.79 0.9  0.83 0.02 0.83 0.88 0.92 0.95\n",
      " 0.95 0.54 0.89 0.85 0.95 0.29 0.79 0.82 0.82 0.86 0.82 0.49 0.92 0.59\n",
      " 0.68 0.91 0.91 0.87 0.77 0.55 0.09 0.89 0.48 0.79 0.96 0.92 0.98 0.85\n",
      " 0.83 0.87 0.97 0.93 0.92 0.96 0.74 0.88 0.82 0.69 0.86 0.76 0.51 0.68\n",
      " 0.93 0.24 0.98 0.8  0.86 0.46 0.81 0.19 0.85 0.93 0.75 0.92 0.97 0.79\n",
      " 0.81 0.89 0.87 0.97 0.86 0.88 0.93 0.23 0.28 0.67 0.85 0.49 0.83 0.53\n",
      " 0.95 0.88 0.47 0.86 0.8  0.87 0.5  0.84 0.54 0.42 0.85 0.59 0.75 0.19\n",
      " 0.85 0.48 0.92 0.94 0.76 0.96 0.83 0.96 0.48 0.85 0.89 0.33 0.86 0.49\n",
      " 0.5  0.49 0.82 0.02 0.82 0.97 0.77 0.31 0.84 0.82 0.78 0.84 0.59 0.03\n",
      " 0.6  0.55 0.97 0.88 0.87 0.68 0.49 0.83 0.88 0.95 0.93 0.86 0.92 0.83\n",
      " 0.91 0.96 0.53 0.78 0.86 0.97 0.07 0.25 0.59 0.66 0.96 0.79 0.48 0.87\n",
      " 0.72 0.45 0.88 0.49 0.95 0.42 0.79 0.84 0.44 0.34 0.89 0.79 0.36 0.58\n",
      " 0.55 0.82 0.85 0.52 0.42 0.07 0.46 0.82 0.86 0.84 0.91 0.07 0.46 0.93\n",
      " 0.82 0.81 0.87 0.34 0.57 0.91 0.77 0.58 0.55 0.52 0.56 0.85 0.96 0.81\n",
      " 0.53 0.88 0.86 0.85 0.87 0.87 0.51 0.56 0.71 0.88 0.84 0.98 0.86 0.91\n",
      " 0.93 0.89 0.75 0.48 0.92 0.77 0.86 0.47 0.7  0.05 0.81 0.82 0.89 0.73\n",
      " 0.95 0.62 0.4  0.92 0.64 0.97 0.92 0.76 0.57 0.81 0.9  0.6  0.37 0.46\n",
      " 0.77 0.97 0.94 0.81 0.97 0.59 0.91 0.92 0.96 0.89 0.84 0.89 0.9  0.63\n",
      " 0.75 0.96 0.89 0.86 0.87 0.8  0.92 0.98 0.98 0.88 0.98 0.82 0.75 0.83\n",
      " 0.92 0.88 0.76 0.59 0.58 0.93 0.23 0.8  0.61 0.81 0.89 0.92 0.84 0.62\n",
      " 0.93 0.92 0.86 0.6  0.91 0.56 0.78 0.97 0.53 0.52 0.91 0.84 0.51 0.86\n",
      " 0.83 0.95 0.74 0.93 0.63 0.91 0.86 0.76 0.76 0.12 0.82 0.85 0.91 0.28\n",
      " 0.86 0.68 0.82 0.97 0.97 0.92 0.89 0.79 0.23 0.86 0.95 0.91 0.88 0.8\n",
      " 0.92 0.88 0.81 0.27 0.83 0.8  0.97 0.13 0.59 0.93 0.76 0.74 0.37 0.07\n",
      " 0.79 0.64 0.97 0.41 0.52 0.5  0.92 0.88 0.54 0.73 0.5  0.53 0.8  0.91\n",
      " 0.83 0.85 0.75 0.81 0.87 0.44 0.79 0.96 0.82 0.43 0.9  0.91 0.78 0.85\n",
      " 0.84 0.88 0.8  0.81 0.95 0.88 0.95 0.87 0.44 0.96 0.6  0.86 0.8  0.59\n",
      " 0.81 0.89 0.49 0.91 0.74 0.97 0.86 0.44 0.9  0.87 0.55 0.91 0.53 0.7\n",
      " 0.82 0.88 0.62 0.56 0.83 0.03 0.92 0.83 0.03 0.91 0.97 0.44 0.85 0.88\n",
      " 0.65 0.96 0.8  0.95 0.86 0.93 0.65 0.12 0.85 0.96 0.78 0.77 0.85 0.92\n",
      " 0.97 0.71 0.88 0.87 0.72 0.91 0.87 0.78 0.87 0.85 0.65 0.91 0.87 0.97\n",
      " 0.92 0.86 0.73 0.85 0.58 0.82 0.89 0.53 0.88 0.84 0.82 0.76 0.61 0.8\n",
      " 0.91 0.53 0.29 0.74 0.9  0.7  0.55 0.88 0.9  0.92 0.91 0.26 0.61 0.84\n",
      " 0.82 0.83 0.39 0.96 0.89 0.86 0.92 0.58 0.77 0.56 0.85 0.92 0.96 0.16\n",
      " 0.82 0.93 0.88 0.54 0.84 0.92 0.91 0.83 0.8  0.82 0.59 0.94 0.8  0.41\n",
      " 0.59 0.55 0.97 0.48 0.86 0.87 0.29 0.6  0.85 0.84 0.95 0.97 0.84 0.45\n",
      " 0.96 0.86 0.07 0.9  0.73 0.86 0.9  0.82 0.89 0.85 0.02 0.02 0.83 0.86\n",
      " 0.44 0.76 0.97 0.91 0.82 0.73 0.58 0.57 0.98 0.81 0.72 0.77 0.54 0.71\n",
      " 0.73 0.65 0.72 0.86 0.91 0.22 0.73 0.88 0.28 0.86 0.52 0.57 0.4  0.85\n",
      " 0.83 0.29 0.76 0.49 0.81 0.08 0.82 0.5  0.93 0.79 0.85 0.91 0.87 0.89\n",
      " 0.87 0.5  0.7  0.85 0.45 0.93 0.94 0.81 0.74 0.59 0.81 0.08 0.81 0.65\n",
      " 0.54 0.94 0.89 0.92 0.7  0.72 0.5  0.8  0.59 0.9  0.88 0.78 0.88 0.49\n",
      " 0.74 0.91 0.97 0.19 0.48 0.96 0.74 0.76 0.61 0.84 0.82 0.62 0.39 0.88\n",
      " 0.08 0.77 0.06 0.52 0.84 0.87 0.82 0.83 0.74 0.96 0.93 0.55 0.8  0.92\n",
      " 0.91 0.92 0.88 0.84 0.84 0.88 0.57 0.54 0.9  0.88 0.73 0.45 0.73 0.53\n",
      " 0.9  0.96 0.86 0.85 0.9  0.91 0.94 0.88 0.7  0.9  0.27 0.91 0.87 0.75\n",
      " 0.92 0.03 0.97 0.47 0.7  0.56 0.94 0.45 0.72 0.84 0.83 0.95 0.92 0.75\n",
      " 0.49 0.07 0.59 0.72 0.93 0.41 0.3  0.86 0.9  0.72 0.94 0.6  0.94 0.74\n",
      " 0.05 0.86 0.89 0.85 0.85 0.85 0.72 0.88 0.67 0.48 0.83 0.93 0.35 0.58\n",
      " 0.91 0.44 0.66 0.78 0.95 0.97 0.6  0.85 0.97 0.93 0.66 0.59 0.11 0.05\n",
      " 0.9  0.73 0.91 0.95 0.05 0.79 0.88 0.82 0.9  0.97 0.89 0.91 0.81 0.85\n",
      " 0.03 0.79 0.77 0.48 0.85 0.8  0.04 0.43 0.16 0.78 0.92 0.44 0.76 0.74\n",
      " 0.83 0.46 0.89 0.86 0.14 0.88 0.93 0.88 0.93 0.63 0.88 0.89 0.85 0.17\n",
      " 0.93 0.96 0.57 0.66 0.91 0.53 0.91 0.72 0.45 0.51 0.44 0.55 0.87 0.85\n",
      " 0.98 0.79 0.8  0.83 0.97 0.9  0.62 0.91 0.76 0.97 0.66 0.91 0.53 0.89\n",
      " 0.6  0.27 0.49 0.5  0.74 0.86 0.91 0.87 0.39 0.94 0.92 0.44 0.83 0.57\n",
      " 0.83 0.93 0.87 0.12 0.09 0.8  0.45 0.84 0.52 0.81 0.78 0.53 0.94 0.9\n",
      " 0.87 0.89 0.87 0.91 0.85 0.79 0.86 0.91 0.82 0.9  0.57 0.91 0.68 0.94\n",
      " 0.81 0.88 0.94 0.86 0.95 0.5  0.96 0.92 0.6  0.82 0.87 0.92 0.44 0.96\n",
      " 0.87 0.82 0.56 0.93 0.92 0.88 0.49 0.91 0.66 0.88 0.58 0.55 0.17 0.81\n",
      " 0.78 0.77 0.59 0.58 0.93 0.39 0.91 0.76 0.65 0.94 0.04 0.81 0.89 0.56\n",
      " 0.91 0.91 0.35 0.51 0.21 0.79 0.86 0.52 0.38 0.05 0.96 0.88 0.81 0.86\n",
      " 0.49 0.72 0.53 0.51 0.84 0.89 0.82 0.94 0.87 0.65 0.67 0.71 0.89 0.82\n",
      " 0.86 0.44 0.91 0.84 0.87 0.97 0.19 0.73 0.82 0.85 0.79 0.59 0.85 0.87\n",
      " 0.94 0.72 0.9  0.91 0.85 0.59 0.75 0.83 0.79 0.27 0.78 0.9  0.88 0.82\n",
      " 0.07 0.89 0.82 0.57 0.97 0.79 0.97 0.85 0.86 0.47 0.93 0.62 0.95 0.81\n",
      " 0.56 0.54 0.83 0.89 0.88 0.41 0.82 0.85 0.86 0.86 0.91 0.78 0.04 0.88\n",
      " 0.96 0.92 0.91 0.38 0.8  0.79 0.88 0.73 0.58 0.96 0.87 0.78 0.53 0.45\n",
      " 0.91 0.83 0.31 0.92 0.82 0.77 0.91 0.58 0.77 0.05 0.83 0.83 0.11 0.93\n",
      " 0.56 0.65 0.89 0.78 0.84 0.85 0.52 0.88 0.76 0.91 0.83 0.55 0.22 0.7\n",
      " 0.53 0.88 0.94 0.82 0.98 0.88 0.8  0.94 0.93 0.82 0.86 0.5  0.85 0.88\n",
      " 0.85 0.95 0.56 0.94 0.55 0.77 0.56 0.96 0.89 0.85 0.97 0.78 0.57 0.6\n",
      " 0.87 0.81 0.86 0.97 0.86 0.89 0.81 0.73 0.9  0.82 0.94 0.05 0.43 0.51\n",
      " 0.54 0.67 0.98 0.52 0.47 0.91 0.91 0.77 0.48 0.39 0.97 0.97 0.76 0.87\n",
      " 0.04 0.5  0.04 0.88 0.84 0.83 0.86 0.88 0.86 0.83 0.94 0.33 0.89 0.45\n",
      " 0.91 0.48 0.94 0.92 0.95 0.84 0.87 0.89 0.6  0.48 0.7  0.84 0.82 0.91\n",
      " 0.82 0.81 0.86 0.82 0.49 0.28 0.63 0.49 0.86 0.9  0.48 0.77 0.92 0.79\n",
      " 0.44 0.88 0.92 0.61 0.81 0.89 0.23 0.74 0.92 0.54 0.65 0.91 0.86 0.88\n",
      " 0.86 0.78 0.29 0.9  0.42 0.92 0.91 0.51 0.6  0.9  0.92 0.89 0.49 0.97\n",
      " 0.87 0.8  0.34 0.45 0.93 0.96 0.92 0.77 0.79 0.73 0.35 0.57 0.86 0.79\n",
      " 0.04 0.85 0.55 0.73 0.87 0.25 0.93 0.93 0.94 0.76 0.86 0.98 0.46 0.91\n",
      " 0.91 0.91 0.82 0.93 0.85 0.88 0.73 0.76 0.89 0.91 0.54 0.96 0.75 0.6\n",
      " 0.98 0.82 0.89 0.63 0.81 0.84 0.81 0.34 0.96 0.85 0.81 0.79 0.01 0.8\n",
      " 0.89 0.51 0.85 0.94 0.93 0.46 0.5  0.76 0.59 0.86 0.92 0.87 0.37 0.81\n",
      " 0.35 0.83 0.92 0.38 0.98 0.92 0.78 0.89 0.87 0.03 0.53 0.81 0.62 0.44\n",
      " 0.96 0.32 0.91 0.47 0.62 0.94 0.97 0.97 0.64 0.9  0.48 0.74 0.84 0.95\n",
      " 0.25 0.69 0.72 0.78 0.85 0.88 0.59 0.72 0.68 0.47 0.77 0.89 0.87 0.85\n",
      " 0.83 0.92 0.85 0.03 0.95 0.82 0.76 0.26 0.86 0.91 0.92 0.9  0.94 0.67\n",
      " 0.54 0.79 0.86 0.83 0.61 0.96 0.71 0.78 0.91 0.73 0.48 0.91 0.66 0.56\n",
      " 0.49 0.96 0.9  0.95 0.93 0.84 0.79 0.79 0.82 0.85 0.96 0.17 0.56 0.87\n",
      " 0.86 0.75 0.15 0.86 0.56 0.88 0.44 0.5  0.55 0.73 0.12 0.96 0.88 0.84\n",
      " 0.92 0.14 0.86 0.92 0.51 0.9  0.93 0.75 0.87 0.74 0.87 0.44 0.79 0.8\n",
      " 0.37 0.94 0.82 0.46 0.83 0.54 0.77 0.84 0.53 0.62 0.79 0.78 0.87 0.91\n",
      " 0.89 0.81 0.86 0.92 0.92 0.55 0.83 0.92 0.68 0.62 0.96 0.85 0.6  0.95\n",
      " 0.83 0.58 0.63 0.47 0.04 0.94 0.83 0.96 0.11 0.96 0.63 0.93 0.89 0.85\n",
      " 0.96 0.84 0.88 0.91 0.89 0.83 0.91 0.56 0.94 0.88 0.76 0.65 0.92 0.82\n",
      " 0.83 0.88 0.83 0.74 0.93 0.86 0.83 0.51 0.92 0.91 0.97 0.5  0.87 0.89\n",
      " 0.74 0.86 0.89 0.8  0.87 0.97 0.85 0.42 0.91 0.81 0.87 0.51 0.71 0.7\n",
      " 0.93 0.48 0.89 0.86 0.74 0.9  0.85 0.95 0.43 0.84 0.93 0.87 0.89 0.86\n",
      " 0.55 0.82 0.53 0.89 0.8  0.92 0.52 0.83 0.1  0.55 0.92 0.91 0.94 0.51\n",
      " 0.46 0.33 0.67 0.92 0.71 0.84 0.81 0.91 0.1  0.85 0.9  0.87 0.96 0.78\n",
      " 0.44 0.75 0.87 0.07 0.8  0.87 0.03 0.85 0.87 0.76 0.92 0.89 0.92 0.84\n",
      " 0.96 0.93 0.52 0.83 0.81 0.55 0.68 0.74 0.88 0.74 0.86 0.91 0.98 0.44\n",
      " 0.49 0.95 0.75 0.88 0.03 0.72 0.3  0.92 0.23 0.84 0.61 0.03 0.86 0.86\n",
      " 0.41 0.56 0.53 0.78 0.96 0.76 0.97 0.85 0.61 0.9  0.83 0.9  0.92 0.8\n",
      " 0.67 0.54 0.81 0.82 0.03 0.48 0.96 0.54 0.98 0.58 0.93 0.85 0.91 0.67\n",
      " 0.78 0.95 0.44 0.53 0.16 0.92 0.89 0.9  0.8  0.01 0.85 0.8  0.33 0.11\n",
      " 0.52 0.44 0.82 0.68 0.85 0.92 0.85 0.88 0.71 0.9  0.83 0.81 0.79 0.36\n",
      " 0.09 0.89 0.85 0.58 0.88 0.04 0.97 0.83 0.76 0.86 0.8  0.12 0.41 0.65\n",
      " 0.34 0.66 0.92 0.72 0.78 0.83 0.78 0.84 0.79 0.82 0.5  0.91 0.82 0.83\n",
      " 0.84 0.92 0.74 0.9  0.92 0.39 0.57 0.88 0.82 0.8  0.98 0.92 0.85 0.85\n",
      " 0.87 0.94 0.87 0.84 0.83 0.81 0.59 0.54 0.41 0.28 0.56 0.77 0.79 0.91\n",
      " 0.39 0.49 0.94 0.59 0.58 0.88 0.87 0.87 0.56 0.95 0.85 0.84 0.79 0.97\n",
      " 0.7  0.92 0.36 0.97 0.89 0.82 0.87 0.97 0.89 0.43 0.78 0.92 0.82 0.83\n",
      " 0.9  0.64 0.9  0.58 0.5  0.46 0.82 0.76 0.73 0.27 0.8  0.77 0.75 0.9\n",
      " 0.49 0.8  0.47 0.53 0.88 0.82 0.8  0.87 0.95 0.6  0.62 0.6  0.7  0.5\n",
      " 0.85 0.42 0.75 0.74 0.59 0.48 0.84 0.74 0.57 0.96 0.85 0.69 0.86 0.83\n",
      " 0.92 0.84 0.85 0.84 0.85 0.96 0.93 0.86 0.75 0.83 0.95 0.95 0.88 0.85\n",
      " 0.95 0.87 0.8  0.93 0.83 0.88 0.94 0.91 0.8  0.85 0.95 0.21 0.77 0.75\n",
      " 0.91 0.83 0.34 0.84 0.84 0.75 0.9  0.63 0.86 0.81 0.77 0.89 0.92 0.9\n",
      " 0.93 0.69 0.93 0.9  0.95 0.91 0.95 0.82 0.76 0.49 0.9  0.81 0.84 0.96\n",
      " 0.92 0.84 0.48 0.56 0.46 0.4  0.83 0.49 0.95 0.8  0.86 0.52 0.53 0.89\n",
      " 0.87 0.9  0.98 0.91 0.91 0.64 0.19 0.86 0.58 0.58 0.57 0.85 0.85 0.83\n",
      " 0.58 0.27 0.74 0.69 0.91 0.92 0.82 0.81 0.49 0.94 0.42 0.86 0.83 0.92\n",
      " 0.89 0.55 0.87 0.87 0.82 0.82 0.9  0.79 0.1  0.96 0.83 0.95 0.06 0.51\n",
      " 0.89 0.76 0.91 0.48 0.84 0.04 0.44 0.65 0.96 0.72 0.84 0.85 0.08 0.5\n",
      " 0.67 0.69 0.73 0.59 0.91 0.86 0.97 0.89 0.91 0.97 0.82 0.97 0.86 0.09\n",
      " 0.82 0.91 0.4  0.8  0.94 0.25 0.97 0.83 0.96 0.81 0.84 0.86 0.92 0.88\n",
      " 0.84 0.82 0.97 0.83 0.56 0.26 0.79 0.93 0.13 0.93 0.88 0.5  0.84 0.92\n",
      " 0.86 0.8  0.92 0.93 0.88 0.91 0.3  0.07 0.92 0.62 0.84 0.7  0.91 0.83\n",
      " 0.96 0.85 0.21 0.49 0.8  0.54 0.86 0.92 0.98 0.39 0.54 0.97 0.16 0.87\n",
      " 0.91 0.92 0.5  0.57 0.51 0.8  0.81 0.55 0.82 0.79 0.51 0.83 0.92 0.89\n",
      " 0.69 0.87 0.97 0.83 0.85 0.93 0.19 0.88 0.11 0.71 0.86 0.89 0.87 0.85\n",
      " 0.59 0.73 0.73 0.89 0.8  0.87 0.43 0.87 0.77 0.87 0.49 0.76 0.85 0.9\n",
      " 0.85 0.74 0.83 0.05 0.26 0.54 0.96 0.92 0.91 0.92 0.92 0.87 0.09 0.92\n",
      " 0.5  0.08 0.51 0.87 0.78 0.73 0.88 0.91 0.76 0.38 0.87 0.92 0.86 0.51\n",
      " 0.87 0.63 0.24 0.78 0.9  0.9  0.85 0.82 0.39 0.87 0.49 0.97 0.93 0.95\n",
      " 0.56 0.88 0.88 0.89 0.85 0.75 0.72 0.89 0.82 0.86 0.82 0.87 0.97 0.97\n",
      " 0.87 0.1  0.73 0.88 0.88 0.9  0.54 0.82 0.58 0.91 0.8  0.82 0.74 0.86\n",
      " 0.15 0.92 0.86 0.77 0.23 0.8  0.95 0.92 0.9  0.93 0.76 0.89 0.77 0.87\n",
      " 0.9  0.89 0.97 0.94 0.45 0.9  0.89 0.82 0.87 0.89 0.24 0.8  0.6  0.84\n",
      " 0.58 0.81 0.78 0.87 0.11 0.85 0.91 0.44 0.9  0.87 0.68 0.87 0.78 0.78\n",
      " 0.67 0.95 0.51 0.86 0.94 0.76 0.81 0.64 0.9  0.86 0.97 0.94 0.98 0.82\n",
      " 0.87 0.43 0.73 0.82 0.63 0.51 0.86 0.27 0.72 0.34 0.82 0.76 0.94 0.78\n",
      " 0.33 0.78 0.18 0.23 0.9  0.55 0.5  0.33 0.48 0.74 0.97 0.9  0.53 0.86\n",
      " 0.86 0.43 0.55 0.77 0.85 0.88 0.73 0.13 0.79 0.92 0.62 0.49 0.84 0.69\n",
      " 0.8  0.84 0.52 0.52 0.94 0.78 0.58 0.79 0.81 0.78 0.92 0.88 0.6  0.15\n",
      " 0.58 0.55 0.49 0.82 0.91 0.06 0.62 0.89 0.83 0.8  0.5  0.81 0.77 0.15\n",
      " 0.62 0.55 0.87 0.74 0.96 0.97 0.94 0.71 0.92 0.83 0.96 0.92 0.86 0.92\n",
      " 0.55 0.88 0.44 0.79 0.89 0.93 0.64 0.86 0.69 0.95 0.72 0.82 0.92 0.48\n",
      " 0.72 0.86 0.68 0.49 0.55 0.97 0.83 0.65 0.63 0.88 0.81 0.64 0.84 0.7\n",
      " 0.84 0.02 0.8  0.78 0.87 0.04 0.91 0.46 0.79 0.95 0.76 0.88 0.55 0.7\n",
      " 0.82 0.88 0.83 0.77 0.86 0.8  0.74 0.97 0.83 0.79 0.92 0.91 0.97 0.88\n",
      " 0.91 0.89 0.42 0.83 0.08 0.93 0.56 0.1  0.93 0.87 0.92 0.54 0.8  0.96\n",
      " 0.79 0.76 0.96 0.93 0.52 0.89 0.94 0.63 0.95 0.97 0.87 0.91 0.9  0.94\n",
      " 0.87 0.48 0.67 0.73 0.86 0.49 0.95 0.86 0.64 0.65 0.11 0.49 0.85 0.87\n",
      " 0.86 0.71 0.89 0.87 0.61 0.71 0.07 0.87 0.86 0.85 0.65 0.9  0.93 0.7\n",
      " 0.96 0.91 0.05 0.89 0.83 0.95 0.92 0.98 0.76 0.7  0.59 0.49 0.85 0.59\n",
      " 0.94 0.1  0.82 0.72 0.6  0.85 0.13 0.9  0.51 0.85 0.97 0.7  0.03 0.9\n",
      " 0.89 0.97 0.93 0.76 0.97 0.81 0.62 0.81 0.75 0.77 0.96 0.72 0.96 0.98\n",
      " 0.74 0.42 0.86 0.96 0.49 0.16 0.89 0.96 0.59 0.9  0.93 0.95 0.34 0.94\n",
      " 0.94 0.88 0.89 0.92 0.87 0.35 0.9  0.51 0.77 0.91 0.43 0.89 0.88 0.87\n",
      " 0.93 0.56 0.79 0.13 0.74 0.1  0.6  0.88 0.96 0.62 0.93 0.91 0.92 0.51\n",
      " 0.83 0.54 0.84 0.88 0.91 0.67 0.85 0.88 0.93 0.93 0.48 0.91 0.91 0.15\n",
      " 0.91 0.1  0.61 0.16 0.9  0.82 0.37 0.06 0.82 0.9  0.43 0.17 0.03 0.9\n",
      " 0.71 0.62 0.83 0.25 0.85 0.93 0.08 0.9  0.03 0.86 0.97 0.92 0.92 0.8\n",
      " 0.86 0.51 0.85 0.89 0.88 0.52 0.97 0.84 0.86 0.81 0.75 0.83 0.61 0.82\n",
      " 0.62 0.86 0.46 0.97 0.91 0.96 0.7  0.78 0.84 0.79 0.44 0.57 0.06 0.82\n",
      " 0.91 0.03 0.83 0.88 0.9  0.93 0.79 0.38 0.87 0.77 0.85 0.95 0.87 0.86\n",
      " 0.97 0.94 0.78 0.95 0.91 0.84 0.67 0.79 0.47 0.94 0.05 0.82 0.8  0.86\n",
      " 0.97 0.73 0.75 0.8  0.88 0.8  0.17 0.77 0.84 0.93 0.75 0.87 0.88 0.97\n",
      " 0.86 0.75 0.8  0.84 0.88 0.83 0.07 0.86 0.74 0.67 0.92 0.81 0.88 0.87\n",
      " 0.9  0.97 0.94 0.97 0.96 0.91 0.64 0.89 0.55 0.8  0.39 0.73 0.85 0.85\n",
      " 0.83 0.6  0.56 0.86 0.55 0.88 0.54 0.93 0.89 0.81 0.48 0.93 0.95 0.49\n",
      " 0.09 0.52 0.5  0.57 0.9  0.93 0.91 0.89 0.87 0.92 0.89 0.79 0.86 0.33\n",
      " 0.94 0.98 0.83 0.91 0.43 0.75 0.87 0.72 0.92 0.6  0.54 0.2  0.92 0.88\n",
      " 0.66 0.53 0.91 0.73 0.87 0.46 0.78 0.91 0.2  0.61 0.91 0.57 0.48 0.87\n",
      " 0.84 0.37 0.89 0.81 0.96 0.95 0.52 0.33 0.82 0.86 0.59 0.66 0.98 0.57\n",
      " 0.92 0.84 0.8  0.06 0.93 0.49 0.57 0.97 0.03 0.55 0.86 0.85 0.58 0.59\n",
      " 0.59 0.86 0.72 0.78 0.6  0.9  0.8  0.75 0.74 0.8  0.77 0.88 0.76 0.84\n",
      " 0.39 0.82 0.64 0.88 0.92 0.6  0.89 0.88 0.97 0.55 0.87 0.44 0.84 0.58\n",
      " 0.97 0.88 0.93 0.95 0.64 0.84 0.87 0.94 0.56 0.83 0.61 0.93 0.86 0.86\n",
      " 0.51 0.51 0.89 0.74 0.91 0.83 0.6  0.88 0.93 0.86 0.61 0.52 0.6  0.93\n",
      " 0.44 0.85 0.58 0.84 0.92 0.92 0.71 0.66 0.87 0.04 0.94 0.87 0.9  0.88\n",
      " 0.57 0.7  0.96 0.09 0.83 0.81 0.91 0.9  0.62 0.85 0.12 0.5  0.86 0.96\n",
      " 0.62 0.89 0.55 0.85 0.8  0.92 0.81 0.73 0.89 0.63 0.92 0.92 0.89 0.7\n",
      " 0.46 0.84 0.59 0.96 0.21 0.67 0.86 0.84 0.97 0.78 0.67 0.91 0.67 0.92\n",
      " 0.89 0.8  0.86 0.94 0.72 0.96 0.13 0.89 0.92 0.29 0.85 0.51 0.96 0.91\n",
      " 0.56 0.47 0.83 0.87 0.9  0.56 0.87 0.92 0.93 0.55 0.06 0.79 0.79 0.97\n",
      " 0.86 0.52 0.87 0.96 0.86 0.78 0.5  0.55 0.9  0.53 0.78 0.04 0.97 0.91\n",
      " 0.92 0.84 0.42 0.87 0.97 0.53 0.54 0.97 0.95 0.08 0.29 0.83 0.92 0.66\n",
      " 0.61 0.6  0.94 0.88 0.89 0.93 0.96 0.51 0.85 0.95 0.91 0.93 0.88 0.84\n",
      " 0.75 0.6  0.59 0.78 0.38 0.98 0.52 0.54 0.92 0.44 0.62 0.56 0.6  0.83\n",
      " 0.79 0.64 0.97 0.79 0.98 0.81 0.71 0.94 0.85 0.49 0.91 0.97 0.91 0.85\n",
      " 0.04 0.8  0.91 0.89 0.97 0.85 0.68 0.8  0.09 0.78 0.87 0.84 0.78 0.82\n",
      " 0.69 0.86 0.78 0.93 0.96 0.87 0.84 0.42 0.82 0.67 0.81 0.74 0.51 0.56\n",
      " 0.59 0.09 0.78 0.8  0.87 0.9  0.87 0.92 0.92 0.95 0.91 0.62 0.35 0.53\n",
      " 0.88 0.97 0.95 0.77 0.88 0.56 0.8  0.07 0.96 0.89 0.97 0.81 0.87 0.89\n",
      " 0.92 0.68 0.1  0.89 0.9  0.85 0.88 0.92 0.97 0.8  0.86 0.96 0.73 0.85\n",
      " 0.74 0.85 0.54 0.84 0.56 0.79 0.86 0.02 0.86 0.91 0.54 0.79 0.87 0.81\n",
      " 0.83 0.76 0.91 0.83 0.84 0.91 0.84 0.52 0.94 0.96 0.9  0.81 0.88 0.67\n",
      " 0.91 0.84 0.93 0.58 0.36 0.72 0.58 0.8  0.59 0.71 0.88 0.98 0.83 0.89\n",
      " 0.86 0.86 0.98 0.94 0.55 0.83 0.86 0.58 0.87 0.97 0.94 0.47 0.75 0.68\n",
      " 0.72 0.95 0.93 0.57 0.57 0.86 0.85 0.88 0.82 0.83 0.83 0.85 0.95 0.76\n",
      " 0.81 0.96 0.69 0.82 0.95 0.59 0.87 0.97 0.97 0.6  0.74 0.82 0.93 0.96\n",
      " 0.5  0.54 0.96 0.93 0.73 0.6  0.96 0.85 0.97 0.93 0.87 0.88 0.87 0.86\n",
      " 0.88 0.93 0.96 0.81 0.67 0.78 0.3  0.76 0.86 0.77 0.61 0.79 0.96 0.43\n",
      " 0.96 0.84 0.46 0.79 0.96 0.86 0.78 0.9  0.93 0.58 0.5  0.55 0.79 0.79\n",
      " 0.41 0.56 0.55 0.96 0.76 0.82 0.94 0.88 0.82 0.88 0.85 0.97 0.69 0.73\n",
      " 0.83 0.68 0.97 0.81 0.83 0.82 0.8  0.31 0.74 0.9  0.61 0.89 0.16 0.87\n",
      " 0.96 0.78 0.81 0.53 0.81 0.91 0.59 0.59 0.85 0.47 0.34 0.97 0.91 0.76\n",
      " 0.77 0.97 0.82 0.83 0.51 0.95 0.71 0.86 0.89 0.53 0.95 0.86 0.59 0.78\n",
      " 0.9  0.4  0.97 0.95 0.61 0.52 0.92 0.59 0.73 0.9  0.82 0.87 0.84 0.31\n",
      " 0.9  0.97 0.82 0.86 0.86 0.19 0.49 0.67 0.81 0.95 0.86 0.52 0.88 0.87\n",
      " 0.82 0.58 0.71 0.57 0.87 0.94 0.85 0.89 0.31 0.87 0.53 0.87 0.57 0.97\n",
      " 0.83 0.13 0.77 0.67 0.9  0.87 0.91 0.85 0.89 0.92 0.19 0.87 0.58 0.55\n",
      " 0.83 0.72 0.84 0.67 0.81 0.81 0.91 0.96 0.83 0.78 0.41 0.83 0.93 0.89\n",
      " 0.79 0.71 0.92 0.94 0.86 0.6  0.57 0.76 0.82 0.86 0.93 0.46 0.8  0.91\n",
      " 0.82 0.96 0.51 0.45 0.95 0.84 0.69 0.95 0.64 0.9  0.8  0.57 0.31 0.94\n",
      " 0.92 0.62 0.97 0.94 0.85 0.92 0.59 0.87 0.77 0.54 0.72 0.84 0.21 0.91\n",
      " 0.97 0.68 0.48 0.94 0.88 0.97 0.8  0.89 0.55 0.78 0.9  0.62 0.52 0.08\n",
      " 0.96 0.88 0.84 0.82 0.81 0.84 0.96 0.02 0.92 0.92 0.45 0.94 0.84 0.93\n",
      " 0.6  0.53 0.81 0.9  0.87 0.92 0.58 0.8  0.65 0.95 0.6  0.51 0.86 0.62\n",
      " 0.79 0.53 0.91 0.55 0.97 0.81 0.69 0.26 0.91 0.46 0.89 0.64 0.9  0.78\n",
      " 0.76 0.94 0.92 0.8  0.93 0.92 0.89 0.86 0.7  0.4  0.86 0.02 0.78 0.9\n",
      " 0.7  0.03 0.43 0.81 0.93 0.72 0.81 0.81 0.42 0.81 0.92 0.03 0.96 0.51\n",
      " 0.77 0.22 0.8  0.5  0.89 0.88 0.86 0.94 0.63 0.31 0.54 0.69 0.96 0.83\n",
      " 0.71 0.81 0.9  0.91 0.83 0.75 0.97 0.12 0.86 0.87 0.89 0.93 0.82 0.93\n",
      " 0.74 0.46 0.81 0.92 0.79 0.88 0.53 0.84 0.08 0.77 0.84 0.5  0.81 0.9\n",
      " 0.79 0.89 0.18 0.8  0.94 0.97 0.54 0.85 0.88 0.92 0.94 0.79 0.91 0.52\n",
      " 0.49 0.78 0.82 0.88 0.87 0.9  0.48 0.56 0.77 0.57 0.1  0.97 0.76 0.62\n",
      " 0.88 0.16 0.08 0.82 0.87 0.97 0.96 0.9  0.65 0.08 0.83 0.66 0.73 0.93\n",
      " 0.98 0.92 0.93 0.92 0.82 0.97 0.6  0.65 0.29 0.03 0.82 0.87 0.08 0.08\n",
      " 0.77 0.98 0.89 0.52 0.78 0.91 0.89 0.94 0.94 0.83 0.53 0.55 0.86 0.84\n",
      " 0.96 0.93 0.95 0.95 0.53 0.58 0.81 0.82 0.76 0.83 0.84 0.75 0.96 0.93\n",
      " 0.94 0.95 0.91 0.83 0.19 0.8  0.91 0.67 0.78 0.68 0.9  0.78 0.02 0.76\n",
      " 0.95 0.7  0.23 0.75 0.45 0.96 0.88 0.83 0.51 0.93 0.54 0.97 0.54 0.55\n",
      " 0.78 0.87 0.91 0.89 0.86 0.09 0.44 0.86 0.85 0.54 0.81 0.89 0.71 0.92\n",
      " 0.55 0.41 0.97 0.92 0.49 0.91 0.85 0.94 0.91 0.82 0.31 0.5  0.88 0.58\n",
      " 0.92 0.42 0.95 0.62 0.87 0.78 0.66 0.55 0.87 0.89 0.45 0.9  0.87 0.9\n",
      " 0.79 0.81 0.96 0.78 0.4  0.42 0.92 0.88 0.85 0.87 0.6  0.79 0.81 0.97\n",
      " 0.92 0.89 0.36 0.82 0.75 0.61 0.15 0.82 0.98 0.81 0.18 0.97 0.49 0.13\n",
      " 0.84 0.96 0.63 0.25 0.91 0.89 0.85 0.92 0.88 0.83 0.91 0.92 0.87 0.91\n",
      " 0.86 0.81 0.96 0.76 0.85 0.84 0.97 0.66 0.92 0.69 0.82 0.84 0.6  0.26\n",
      " 0.9  0.91 0.78 0.76 0.54 0.81 0.81 0.61 0.72 0.86 0.92 0.57 0.82 0.88\n",
      " 0.4  0.71 0.93 0.41 0.78 0.86 0.92 0.9  0.96 0.81 0.51 0.63 0.95 0.97\n",
      " 0.89 0.36 0.9  0.85 0.74 0.9  0.88 0.84 0.81 0.41 0.84 0.5  0.87 0.91\n",
      " 0.41 0.89 0.77 0.89 0.97 0.48 0.88 0.91 0.52 0.92 0.86 0.88 0.96 0.48\n",
      " 0.86 0.52 0.92 0.88 0.59 0.94 0.55 0.79 0.61 0.07 0.87 0.86 0.78 0.71\n",
      " 0.22 0.93 0.87 0.45 0.83 0.42 0.84 0.53 0.98 0.89 0.65 0.85 0.92 0.49\n",
      " 0.39 0.9  0.88 0.51 0.94 0.79 0.97 0.65 0.42 0.73 0.89 0.77 0.84 0.82\n",
      " 0.49 0.94 0.82 0.6  0.9  0.49 0.91 0.91 0.85 0.35 0.51 0.79 0.8  0.83\n",
      " 0.19 0.94 0.62 0.02 0.62 0.87 0.42 0.93 0.85 0.45 0.02 0.73 0.54 0.48\n",
      " 0.66 0.92 0.79 0.86 0.37 0.81 0.78 0.9  0.95 0.98 0.91 0.71 0.48 0.48\n",
      " 0.81 0.73 0.95 0.86 0.91 0.73 0.9  0.88 0.03 0.91 0.36 0.92 0.8  0.95\n",
      " 0.91 0.84 0.78 0.91 0.72 0.55 0.58 0.73 0.44 0.94 0.8  0.05 0.56 0.03\n",
      " 0.91 0.91 0.98 0.88 0.03 0.97 0.85 0.86 0.79 0.88 0.81 0.92 0.66 0.82\n",
      " 0.81 0.9  0.81 0.8  0.9  0.85 0.89 0.59 0.62 0.9  0.84 0.85 0.48 0.56\n",
      " 0.71 0.79 0.48 0.95 0.97 0.48 0.25 0.58 0.87 0.75 0.96 0.86 0.77 0.78\n",
      " 0.92 0.57 0.93 0.31 0.87 0.91 0.2  0.59 0.55 0.44 0.78 0.48 0.55 0.61\n",
      " 0.88 0.73 0.63 0.93 0.19 0.8  0.85 0.44 0.47 0.14 0.81 0.71 0.69 0.78\n",
      " 0.9  0.96 0.74 0.96 0.92 0.85 0.97 0.92 0.91 0.91 0.94 0.64 0.38 0.96\n",
      " 0.86 0.85 0.91 0.77 0.94 0.86 0.82 0.96 0.51 0.87 0.85 0.84 0.87 0.11\n",
      " 0.52 0.75 0.83 0.83 0.81 0.87 0.14 0.93 0.97 0.91 0.93 0.81 0.44 0.93\n",
      " 0.76 0.93 0.62 0.79 0.96 0.55 0.63 0.86 0.79 0.82 0.92 0.95 0.87 0.45\n",
      " 0.83 0.78 0.83 0.94 0.57 0.73 0.78 0.94 0.87 0.05 0.55 0.71 0.89 0.58\n",
      " 0.87 0.78 0.72 0.89 0.42 0.89 0.94 0.15 0.71 0.83 0.48 0.49 0.84 0.88\n",
      " 0.56 0.91 0.87 0.54 0.83 0.79 0.46 0.82 0.65 0.82 0.03 0.84 0.96 0.85\n",
      " 0.92 0.9  0.82 0.62 0.79 0.9  0.83 0.93 0.35 0.98 0.87 0.96 0.87 0.04\n",
      " 0.86 0.29 0.59 0.58 0.85 0.92 0.87 0.8  0.93 0.55 0.91 0.92 0.65 0.88\n",
      " 0.55 0.83 0.82 0.94 0.9  0.88 0.96 0.51 0.87 0.86 0.81 0.81 0.87 0.54\n",
      " 0.82 0.9  0.93 0.68 0.43 0.31 0.86 0.77 0.75 0.94 0.45 0.66 0.93 0.91\n",
      " 0.02 0.78 0.75 0.86 0.92 0.95 0.84 0.13 0.88 0.85 0.96 0.88 0.73 0.89\n",
      " 0.45 0.88 0.46 0.92 0.05 0.54 0.51 0.06 0.83 0.73 0.68 0.82 0.6  0.91\n",
      " 0.83 0.93 0.53 0.94 0.53 0.48 0.86 0.9  0.82 0.82 0.45 0.7  0.84 0.11\n",
      " 0.81 0.69 0.07 0.92 0.87 0.26 0.5  0.9  0.88 0.67 0.84 0.45 0.97 0.95\n",
      " 0.82 0.79 0.91 0.93 0.97 0.57 0.55 0.55 0.84 0.62 0.96 0.77 0.05 0.46\n",
      " 0.57 0.77 0.92 0.96 0.89 0.87 0.83 0.79 0.94 0.94 0.97 0.85 0.89 0.46\n",
      " 0.72 0.49 0.96 0.89 0.43 0.97 0.86 0.89 0.81 0.9  0.91 0.71 0.96 0.66\n",
      " 0.48 0.91 0.87 0.93 0.93 0.74 0.93 0.6  0.54 0.86 0.86 0.78 0.58 0.98\n",
      " 0.87 0.81 0.52 0.45 0.74 0.81 0.9  0.49 0.8  0.87 0.03 0.97 0.84 0.37\n",
      " 0.92 0.93 0.71 0.52 0.48 0.97 0.93 0.54 0.89 0.92 0.92 0.61 0.78 0.92\n",
      " 0.48 0.81 0.51 0.91 0.65 0.92 0.86 0.3  0.87 0.89 0.85 0.85 0.58 0.92\n",
      " 0.53 0.88 0.71 0.51 0.56 0.64 0.85 0.92 0.8  0.89 0.94 0.49 0.07 0.89\n",
      " 0.77 0.93 0.95 0.9  0.65 0.81 0.85 0.63 0.58 0.88 0.03 0.88 0.79 0.78\n",
      " 0.11 0.6  0.6  0.88 0.74 0.33 0.57 0.33 0.81 0.79 0.67 0.81 0.89 0.83\n",
      " 0.96 0.76 0.9  0.48 0.86 0.67 0.92 0.91 0.81 0.62 0.92 0.74 0.01 0.8\n",
      " 0.83 0.86 0.23 0.89 0.84 0.33 0.82 0.89 0.7  0.47 0.95 0.82 0.95 0.5\n",
      " 0.87 0.84 0.87 0.85 0.9  0.92 0.9  0.74 0.51 0.77 0.83 0.92 0.49 0.81\n",
      " 0.96 0.27 0.65 0.93 0.47 0.82 0.7  0.79 0.86 0.16 0.8  0.16 0.61 0.06\n",
      " 0.05 0.9  0.86 0.9  0.54 0.92 0.8  0.79 0.81 0.05 0.89 0.85 0.93 0.95\n",
      " 0.9  0.7  0.28 0.91 0.98 0.58 0.96 0.92 0.88 0.43 0.86 0.88 0.93 0.93\n",
      " 0.85 0.92 0.73 0.81 0.88 0.85 0.64 0.56 0.9  0.52 0.94 0.94 0.97 0.91\n",
      " 0.67 0.91 0.68 0.96 0.82 0.79 0.6  0.49 0.63 0.63 0.95 0.87 0.53 0.89\n",
      " 0.92 0.82 0.91 0.02 0.6  0.85 0.82 0.88 0.85 0.72 0.78 0.82 0.84 0.77\n",
      " 0.55 0.88 0.96 0.86 0.42 0.92 0.97 0.22 0.8  0.54 0.95 0.84 0.82 0.95\n",
      " 0.35 0.86 0.89 0.94 0.92 0.77 0.94 0.53 0.62 0.98 0.91 0.92 0.94 0.94\n",
      " 0.55 0.35 0.81 0.79 0.27 0.37 0.49 0.92 0.91 0.91 0.79 0.8  0.81 0.94\n",
      " 0.84 0.94 0.81 0.37 0.72 0.83 0.81 0.62 0.83 0.83 0.75 0.9  0.73 0.91\n",
      " 0.89 0.97 0.95 0.6  0.95 0.77 0.15 0.94 0.81 0.82 0.92 0.65 0.88 0.97\n",
      " 0.6  0.83 0.96 0.56 0.56 0.96 0.43 0.04 0.35 0.9  0.96 0.79 0.84 0.77\n",
      " 0.03 0.98 0.51 0.93 0.47 0.86 0.85 0.76 0.51 0.96 0.5  0.83 0.87 0.93\n",
      " 0.83 0.88 0.44 0.5  0.97 0.89 0.96 0.6  0.94 0.86 0.65 0.08 0.66 0.78\n",
      " 0.27 0.97 0.97 0.53 0.84 0.75 0.86 0.46 0.69 0.46 0.68 0.38 0.9  0.81\n",
      " 0.81 0.87 0.86 0.42 0.71 0.02 0.72 0.85 0.96 0.87 0.89 0.03 0.89 0.96\n",
      " 0.98 0.8  0.59 0.87 0.82 0.9  0.5  0.75 0.04 0.54 0.48 0.85 0.62 0.91\n",
      " 0.84 0.77 0.09 0.83 0.93 0.94 0.91 0.88 0.47 0.77 0.97 0.93 0.84 0.96\n",
      " 0.89 0.86 0.81 0.74 0.98 0.1  0.79 0.84 0.81 0.82 0.15 0.91 0.64 0.86\n",
      " 0.87 0.84 0.86 0.92 0.76 0.87 0.55 0.88 0.77 0.93 0.63 0.82 0.09 0.59\n",
      " 0.1  0.84 0.93 0.8  0.87 0.91 0.9  0.93 0.6  0.37 0.78 0.79 0.86 0.28\n",
      " 0.88 0.83 0.97 0.55 0.82 0.81 0.9  0.76 0.86 0.62 0.48 0.82 0.08 0.79\n",
      " 0.82 0.85 0.73 0.58 0.97 0.55 0.77 0.12 0.55 0.94 0.44 0.71 0.02 0.97\n",
      " 0.84 0.97 0.81 0.8  0.98 0.39 0.86 0.52 0.87 0.92 0.55 0.43 0.89 0.82\n",
      " 0.56 0.81 0.85 0.89 0.73 0.45 0.85 0.89 0.81 0.83 0.58 0.57 0.91 0.92\n",
      " 0.63 0.76 0.62 0.94 0.37 0.96 0.54 0.75 0.71 0.96 0.91 0.48 0.95 0.36\n",
      " 0.72 0.34 0.88 0.86 0.93 0.3  0.86 0.54 0.69 0.87 0.83 0.79 0.45 0.93\n",
      " 0.84 0.72 0.82 0.77 0.13 0.35 0.83 0.49 0.51 0.94 0.97 0.62 0.77 0.78\n",
      " 0.8  0.02 0.81 0.57 0.82 0.51 0.49 0.87 0.87 0.91 0.91 0.88 0.92 0.79\n",
      " 0.93 0.82 0.91 0.81 0.84 0.82 0.58 0.93 0.95 0.82 0.94 0.9  0.75 0.82\n",
      " 0.33 0.34 0.55 0.54 0.84 0.89 0.68 0.81 0.68 0.1  0.24 0.81 0.93 0.96\n",
      " 0.85 0.44 0.92 0.72 0.84 0.85 0.53 0.41 0.77 0.85 0.85 0.96 0.87 0.92\n",
      " 0.23 0.03 0.88 0.8  0.57 0.95 0.91 0.32 0.8  0.88 0.63 0.39 0.43 0.82\n",
      " 0.84 0.53 0.43 0.95 0.91 0.21 0.79 0.73 0.92 0.49 0.91 0.91 0.77 0.64\n",
      " 0.92 0.88 0.86 0.87 0.81 0.92 0.07 0.92 0.52 0.71 0.77 0.82 0.86 0.94\n",
      " 0.76 0.94 0.78 0.89 0.57 0.9  0.72 0.96 0.97 0.87 0.32 0.97 0.88 0.83\n",
      " 0.91 0.79 0.79 0.32 0.05 0.88 0.54 0.16 0.77 0.7  0.36 0.9  0.77 0.07\n",
      " 0.11 0.83 0.92 0.46 0.93 0.83 0.91 0.84 0.87 0.82 0.9  0.66 0.86 0.44\n",
      " 0.91 0.84 0.62 0.78 0.72 0.74 0.95 0.64 0.97 0.93 0.83 0.86 0.92 0.85\n",
      " 0.64 0.95 0.56 0.94 0.83 0.92 0.59 0.81 0.86 0.82 0.47 0.86 0.85 0.96\n",
      " 0.59 0.69 0.89 0.96 0.82 0.85 0.77 0.52 0.8  0.83 0.79 0.13 0.6  0.55\n",
      " 0.82 0.94 0.92 0.81 0.84 0.39 0.96 0.93 0.79 0.55 0.92 0.96 0.88 0.97\n",
      " 0.96 0.95 0.87 0.63 0.63 0.89 0.68 0.75 0.88 0.07 0.97 0.31 0.96 0.56\n",
      " 0.08 0.13 0.56 0.8  0.68 0.53 0.39 0.6  0.43 0.85 0.41 0.08 0.55 0.81\n",
      " 0.88 0.78 0.03 0.71 0.96 0.97 0.55 0.79 0.96 0.97 0.88 0.87 0.64 0.74\n",
      " 0.56 0.66 0.79 0.54 0.55 0.78 0.87 0.87 0.83 0.86 0.74 0.87 0.85 0.89\n",
      " 0.75 0.81 0.93 0.97 0.86 0.78 0.97 0.89 0.97 0.91 0.49 0.61 0.78 0.91\n",
      " 0.94 0.32 0.84 0.78 0.83 0.83 0.83 0.82 0.83 0.53 0.95 0.38 0.91 0.96\n",
      " 0.84 0.88 0.88 0.19 0.91 0.55 0.92 0.64 0.89 0.91 0.9  0.79 0.78 0.62\n",
      " 0.96 0.84 0.08 0.97 0.87 0.8  0.73 0.78 0.89 0.8  0.51 0.93 0.73 0.96\n",
      " 0.77 0.67 0.79 0.03 0.56 0.14 0.93 0.84 0.82 0.54 0.97 0.97 0.97 0.9\n",
      " 0.71 0.77 0.7  0.71 0.91 0.6  0.74 0.84 0.88 0.85 0.97 0.86 0.82 0.82\n",
      " 0.81 0.97 0.91 0.82 0.96 0.48 0.83 0.91 0.84 0.87 0.96 0.91 0.84 0.94\n",
      " 0.86 0.76 0.6  0.91 0.84 0.91 0.77 0.8  0.81 0.85 0.85 0.87 0.97 0.81\n",
      " 0.84 0.87 0.77 0.95 0.64 0.82 0.6  0.9  0.6  0.77 0.86 0.86 0.4  0.57\n",
      " 0.79 0.96 0.79 0.82 0.51 0.79 0.58 0.89 0.79 0.82 0.98 0.79 0.88 0.97\n",
      " 0.9  0.95 0.42 0.82 0.93 0.49 0.95 0.61 0.86 0.75 0.76 0.89 0.5  0.74\n",
      " 0.46 0.86 0.9  0.97 0.85]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = model.predict_proba(X_test)[:,0].round(2)\n",
    "print(f)\n",
    "np.median(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type numpy.ndarray doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-230-eaedbcc3fa60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: type numpy.ndarray doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "ff = model.transform(X_test)\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "round(ff[0],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fff = model.transform(X_test)\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "for i in range(0,len(fff)):\n",
    "    for j in range(0,len(fff[i])):\n",
    "        if fff[i][j][0] >= 0.5:\n",
    "            fff[i][j][0] = 1\n",
    "        else:\n",
    "            fff[i][j][0] = 0\n",
    "        if(fff[i][j][1] < 0.5):\n",
    "            fff[i][j][1] = 0\n",
    "        else:\n",
    "            fff[i][j][1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elements(seq) -> dict:\n",
    "    \"\"\"Tally elements from `seq`.\"\"\"\n",
    "    hist = {}\n",
    "    for i in seq:\n",
    "        hist[i] = hist.get(i, 0) + 1\n",
    "    return hist\n",
    "\n",
    "final2 = []\n",
    "final_final = []\n",
    "for i in range(0, len(ff)):\n",
    "    for j in range(0, len(ff[i])):\n",
    "        final_value = (max(ff[i][j]))\n",
    "        if np.argmax(ff[i][j]) == 0:\n",
    "            final2.append(final_value)\n",
    "        \n",
    "    final_final.append(final2) \n",
    "    final2 = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "n, bins, patches = plt.hist(x=final2[9050:18098], bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "\n",
    "print(n)\n",
    "#plt.grid(axis='y', alpha=0.75)\n",
    "#plt.xlabel('Value')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.title('My Very Own Histogram')\n",
    "#plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "#maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "#plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f5da3d4fd0>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])\n",
    "n,b1in, patches= plt.hist(final_final, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f5da5a4280>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])\n",
    "n,b1in, patches= plt.hist(final_final, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f5db096a00>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(final_final, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "ax1.set_title('Class 0')\n",
    "ax2.hist(final_final1, align='left', bins=[0.5, 0.6, 0.7, 0.8, 0.9, 1], rwidth=0.5, stacked=True, edgecolor='black')\n",
    "ax2.set_title('Class 1')\n",
    "f.tight_layout()\n",
    "plt.legend(['LR', 'RF', 'KNN', 'DT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11926466105322131,\n",
       " 0.1486484826781445,\n",
       " 0.2546424360384043,\n",
       " 0.29394969252820663,\n",
       " 0.11245499550811645,\n",
       " 0.15075278691886357,\n",
       " 0.23633988055007438,\n",
       " 0.09389235083760614,\n",
       " 0.1582903379063719,\n",
       " 0.36046595136702947,\n",
       " 0.23109299629149402,\n",
       " 0.2427596121257543,\n",
       " 0.08516023037314362,\n",
       " 0.21316213055060887,\n",
       " 0.26764903439109766,\n",
       " 0.031168777389661506,\n",
       " 0.09808874490672569,\n",
       " 0.31968006441710534,\n",
       " 0.21254758153157102,\n",
       " 0.2298862446296369,\n",
       " 0.15056491903190639,\n",
       " 0.17543591565013655,\n",
       " 0.2517742382497353,\n",
       " 0.14780944654837203,\n",
       " 0.19676359974309054,\n",
       " 0.13003079927235942,\n",
       " 0.12781203304216568,\n",
       " 0.12771430456267935,\n",
       " 0.07935003083148875,\n",
       " 0.16051736829138424,\n",
       " 0.12448447644621666,\n",
       " 0.1702034536571656,\n",
       " 0.18294383571378306,\n",
       " 0.3321518762622131,\n",
       " 0.23804785626520192,\n",
       " 0.1332366979403468,\n",
       " 0.12658055073484234,\n",
       " 0.23511582613049775,\n",
       " 0.20511474717563638,\n",
       " 0.13420605724646892,\n",
       " 0.12942100975416104,\n",
       " 0.11285920756985564,\n",
       " 0.2508246754630184,\n",
       " 0.20470713014860112,\n",
       " 3.1308289294429414e-14,\n",
       " 0.2392838277989555,\n",
       " 0.11817549399938665,\n",
       " 0.4308884023543972,\n",
       " 0.13109213208546763,\n",
       " 0.28095373531131185,\n",
       " 0.42106866747026633,\n",
       " 0.11278685743676437,\n",
       " 0.21916205796260912,\n",
       " 0.08486850412539157,\n",
       " 0.21858397027864845,\n",
       " 0.297257400744926,\n",
       " 0.09535984072931063,\n",
       " 0.22057713256837191,\n",
       " 0.13628999312637768,\n",
       " 0.18040230219372214,\n",
       " 0.2098346668706659,\n",
       " 0.15209924362916757,\n",
       " 0.09969751604169864,\n",
       " 0.31053946467809956,\n",
       " 0.008966622080694764,\n",
       " 0.08518630566048865,\n",
       " 0.261987359149699,\n",
       " 0.21893895233739102,\n",
       " 0.0032724867725755225,\n",
       " 0.2555542513742223,\n",
       " 0.1858439255946307,\n",
       " 0.11837505462467793,\n",
       " 0.14286867168419218,\n",
       " 0.18131245725272588,\n",
       " 0.16566939287798887,\n",
       " 0.09935335852337755,\n",
       " 0.0908108639609617,\n",
       " 0.21345575024698563,\n",
       " 0.11667115555509612,\n",
       " 0.14733759898924464,\n",
       " 0.06267228384562651,\n",
       " 0.2990848972854618,\n",
       " 0.2669962242789034,\n",
       " 0.43412109324144804,\n",
       " 0.185426672156335,\n",
       " 0.19338844607383152,\n",
       " 0.39776829978481176,\n",
       " 0.43102159911518384,\n",
       " 0.10905204949722538,\n",
       " 0.3130519639785732,\n",
       " 0.23830482163879596,\n",
       " 0.26679127758934373,\n",
       " 0.09388547588149644,\n",
       " 0.24549153900230436,\n",
       " 0.25946393994927647,\n",
       " 0.09805262068540199,\n",
       " 0.12138301620461353,\n",
       " 0.15656235858761644,\n",
       " 0.21658625027545594,\n",
       " 0.1733275459487303,\n",
       " 0.11347814651983748,\n",
       " 0.1848666984356979,\n",
       " 0.14810163574977322,\n",
       " 0.1768912049082735,\n",
       " 0.3082535160542126,\n",
       " 0.10069374388204581,\n",
       " 0.23610957224138476,\n",
       " 0.1485147413486209,\n",
       " 0.31914636328243157,\n",
       " 0.20866265294985395,\n",
       " 0.13340885085311194,\n",
       " 0.49005448277941,\n",
       " 0.2734215954143873,\n",
       " 0.09238787445134766,\n",
       " 0.1464333900150111,\n",
       " 0.212631160299431,\n",
       " 0.03640440815209589,\n",
       " 0.3644282875789988,\n",
       " 0.2913939648329137,\n",
       " 0.19074591714994402,\n",
       " 0.15186631280723828,\n",
       " 0.08563910943520187,\n",
       " 0.22311119549314962,\n",
       " 0.13249377901305598,\n",
       " 0.13738131716779134,\n",
       " 0.1018005843502955,\n",
       " 0.20856557571059106,\n",
       " 0.21454671915795984,\n",
       " 0.28566810903680856,\n",
       " 0.16564830053791144,\n",
       " 0.22553556810413478,\n",
       " 0.12515517327255637,\n",
       " 0.16880383365825838,\n",
       " 0.4452639218272699,\n",
       " 0.08157176678790334,\n",
       " 0.10238665528547196,\n",
       " 0.21429197497786887,\n",
       " 0.1388652544484514,\n",
       " 0.2512072033727417,\n",
       " 0.19208911264763437,\n",
       " 0.43035414537280486,\n",
       " 0.06853669188531417,\n",
       " 0.21484513521359788,\n",
       " 0.21477921056822463,\n",
       " 0.2686789633285174,\n",
       " 0.08006055177009104,\n",
       " 0.22205335889323968,\n",
       " 0.14374583218399464,\n",
       " 0.35561251874182925,\n",
       " 0.16203948310068791,\n",
       " 0.20063501411581813,\n",
       " 0.23636738767327642,\n",
       " 0.15943642136799177,\n",
       " 0.29539772779779655,\n",
       " 0.16017481870164907,\n",
       " 0.18338706082918607,\n",
       " 0.4333313818599246,\n",
       " 0.1280125163134432,\n",
       " 0.22007009003800385,\n",
       " 0.23053702330382542,\n",
       " 0.24918678907879852,\n",
       " 0.20618592924384435,\n",
       " 0.1283516519795677,\n",
       " 0.35792188467018654,\n",
       " 0.15769840515401173,\n",
       " 0.07894911004369598,\n",
       " 0.14350750060773257,\n",
       " 0.1148317700040119,\n",
       " 0.11651047500229648,\n",
       " 0.07010637485261859,\n",
       " 0.3286935591833656,\n",
       " 0.15315274999628434,\n",
       " 0.28042649644043544,\n",
       " 0.03508839287496024,\n",
       " 0.2802488656385359,\n",
       " 0.17145325981888235,\n",
       " 0.4805693845424793,\n",
       " 0.16114509265913618,\n",
       " 0.26581553603806607,\n",
       " 0.2404005018903872,\n",
       " 0.16492549166970688,\n",
       " 0.17292454147000544,\n",
       " 0.22848718334128174,\n",
       " 0.20514450391487535,\n",
       " 0.14033911403127464,\n",
       " 0.1371936137084825,\n",
       " 0.16938811185684924,\n",
       " 0.13705758562429302,\n",
       " 0.3747012767436241,\n",
       " 0.11412854494735321,\n",
       " 2.2426505097428162e-14,\n",
       " 0.15585701448563885,\n",
       " 0.13811294923312853,\n",
       " 0.10607913612601584,\n",
       " 0.1368078722821566,\n",
       " 0.11879511347239394,\n",
       " 0.07685298367322674,\n",
       " 0.2060031305163467,\n",
       " 0.2861369211400312,\n",
       " 0.1707655754130278,\n",
       " 0.27269225562956934,\n",
       " 0.3170793992003398,\n",
       " 0.15591579052876609,\n",
       " 0.14694367142388856,\n",
       " 0.2176255803476454,\n",
       " 0.21415873004804306,\n",
       " 0.005543499181829392,\n",
       " 0.4763121455094662,\n",
       " 0.1456299914675611,\n",
       " 0.21755705274538772,\n",
       " 0.12492101722241447,\n",
       " 0.4312874832265362,\n",
       " 0.2105929802144854,\n",
       " 0.20119588367214508,\n",
       " 0.1468337254084391,\n",
       " 0.34846245574541673,\n",
       " 0.36562931604760607,\n",
       " 0.33301032775791906,\n",
       " 0.035481544751881255,\n",
       " 0.16133544362622948,\n",
       " 0.08470102091723317,\n",
       " 0.3383766398756597,\n",
       " 0.07996635864406354,\n",
       " 0.05772708325370122,\n",
       " 0.15698179659670763,\n",
       " 0.1371602046143175,\n",
       " 0.257066342410394,\n",
       " 0.19323130638016234,\n",
       " 0.29943104006772364,\n",
       " 0.45787801045915405,\n",
       " 0.1808395902448632,\n",
       " 0.3475706946477846,\n",
       " 0.15647232558007979,\n",
       " 0.22092900021920278,\n",
       " 0.1564950605687253,\n",
       " 0.04808832485914638,\n",
       " 0.14700893221018868,\n",
       " 0.2199290920184711,\n",
       " 0.13944879249129738,\n",
       " 0.3401058588540287,\n",
       " 0.060522096380956784,\n",
       " 0.19950941105602926,\n",
       " 0.18604748420418674,\n",
       " 0.12091473506818529,\n",
       " 0.19721773323091435,\n",
       " 0.18102658362332935,\n",
       " 0.4841138841510777,\n",
       " 0.14597246438737732,\n",
       " 0.4922892743166509,\n",
       " 0.3014098382918614,\n",
       " 0.2400353657076879,\n",
       " 0.2549671485284962,\n",
       " 0.21283213147050722,\n",
       " 0.19175772601795846,\n",
       " 0.15573117443141063,\n",
       " 0.07135180933199371,\n",
       " 0.14420140893346553,\n",
       " 0.2328663771260002,\n",
       " 0.12003266099264549,\n",
       " 0.31599783173598034,\n",
       " 0.07786017400347107,\n",
       " 0.15929378072620975,\n",
       " 0.25628431346379205,\n",
       " 0.12573481777587872,\n",
       " 0.3940411719527393,\n",
       " 0.3255621103235016,\n",
       " 0.13278330291755705,\n",
       " 0.15727990561573996,\n",
       " 0.2404112916215755,\n",
       " 0.2634073552058577,\n",
       " 0.1426222271050138,\n",
       " 0.23179930780578123,\n",
       " 0.4552558741377999,\n",
       " 0.3512286164541337,\n",
       " 0.3881917566276119,\n",
       " 0.13661217988833585,\n",
       " 0.16456040709745173,\n",
       " 0.16648892293573206,\n",
       " 0.2744316066907762,\n",
       " 0.23529803368427044,\n",
       " 0.25746143902102553,\n",
       " 0.07012205917094148,\n",
       " 0.3405865739328632,\n",
       " 0.22906867709364645,\n",
       " 0.13001753398096394,\n",
       " 0.40668494080942524,\n",
       " 0.24520014120562117,\n",
       " 0.18658719167195248,\n",
       " 0.13738120717486185,\n",
       " 0.2325890067498373,\n",
       " 0.1775868552185794,\n",
       " 0.13544348657095442,\n",
       " 0.18489870156817698,\n",
       " 0.16534778677232906,\n",
       " 0.2830708073233222,\n",
       " 0.17596576634089112,\n",
       " 0.17078218334831805,\n",
       " 0.23007229637593934,\n",
       " 0.20505292767507285,\n",
       " 0.3224871033644584,\n",
       " 0.01743458370599027,\n",
       " 0.09712655855459787,\n",
       " 0.08685512784987995,\n",
       " 0.21726907100151427,\n",
       " 0.22039664981116514,\n",
       " 0.13508782264334773,\n",
       " 0.2710512218481285,\n",
       " 0.18941769927705165,\n",
       " 0.15519922823293084,\n",
       " 0.12939841715210915,\n",
       " 0.2523083240196886,\n",
       " 0.21943842442053124,\n",
       " 0.2894573995139463,\n",
       " 0.08753417782737231,\n",
       " 0.12242630092152378,\n",
       " 0.23905935634362602,\n",
       " 0.17816754249810643,\n",
       " 0.12642048030637176,\n",
       " 0.2474798666366597,\n",
       " 0.17309780315801465,\n",
       " 0.029337734248337743,\n",
       " 0.22406817893259115,\n",
       " 0.16110698748791313,\n",
       " 0.15019729861512462,\n",
       " 0.21073317726592303,\n",
       " 0.35870710017186724,\n",
       " 0.2650703535606314,\n",
       " 0.16922907078638413,\n",
       " 0.4112561332487733,\n",
       " 0.22358085215603593,\n",
       " 0.2028275455734918,\n",
       " 0.03758006648383905,\n",
       " 0.20171220719816882,\n",
       " 0.09121928392544065,\n",
       " 0.12912781908552012,\n",
       " 0.22168505286473797,\n",
       " 0.19094467341639043,\n",
       " 0.09583472740944185,\n",
       " 0.11180674478283184,\n",
       " 0.12151296978413975,\n",
       " 0.13464690052643655,\n",
       " 0.10654167358830302,\n",
       " 0.3835772865847673,\n",
       " 0.1616321953558687,\n",
       " 0.2575647524689516,\n",
       " 0.14632095255656433,\n",
       " 0.12451274638694104,\n",
       " 0.18399603700585612,\n",
       " 0.17665327723132623,\n",
       " 0.18243221776507232,\n",
       " 0.15968489277453166,\n",
       " 0.18466181866805872,\n",
       " 0.24040666285185114,\n",
       " 0.12085544747236457,\n",
       " 0.4766968636075548,\n",
       " 0.029607130722897224,\n",
       " 0.13506019517471549,\n",
       " 0.25836411641154183,\n",
       " 0.20964759861234777,\n",
       " 0.2577163778652867,\n",
       " 0.4698409250781694,\n",
       " 0.1780556436316919,\n",
       " 0.2165516511040259,\n",
       " 0.40858905474000734,\n",
       " 0.13817934398935183,\n",
       " 0.140253210078036,\n",
       " 0.1454914507898442,\n",
       " 0.17500143980956054,\n",
       " 0.2155444594013504,\n",
       " 0.3201636278724331,\n",
       " 0.20070506100309563,\n",
       " 0.018883909661235765,\n",
       " 0.25605421327227823,\n",
       " 0.48252643699153164,\n",
       " 0.28140538389460784,\n",
       " 0.18573855918894572,\n",
       " 0.1419961328463717,\n",
       " 0.25726780209080613,\n",
       " 0.16425492403138686,\n",
       " 0.12184925901149438,\n",
       " 0.10728212282206878,\n",
       " 0.04827034854920882,\n",
       " 0.00830808471004707,\n",
       " 0.10812338128413872,\n",
       " 0.23141591586541876,\n",
       " 0.1351400776238775,\n",
       " 0.2028178312236205,\n",
       " 0.22757071342310076,\n",
       " 0.28633317855979895,\n",
       " 0.1592920702928651,\n",
       " 0.44498939318491737,\n",
       " 0.13638043231858454,\n",
       " 0.3762659724484264,\n",
       " 0.4955179438649753,\n",
       " 0.23944623466309775,\n",
       " 0.15181265207360778,\n",
       " 0.21161407435176616,\n",
       " 0.14816713458011876,\n",
       " 0.17644357488780593,\n",
       " 0.15326717372212773,\n",
       " 0.2621674020038609,\n",
       " 0.3781047152280571,\n",
       " 0.2744348623685317,\n",
       " 0.3759030171943899,\n",
       " 0.13820451661908115,\n",
       " 0.27798691192754366,\n",
       " 0.2634964487038902,\n",
       " 0.12669182915445515,\n",
       " 0.12369677004048865,\n",
       " 0.19976454277051842,\n",
       " 0.3900985738827375,\n",
       " 0.24392132469352626,\n",
       " 0.1782254859741863,\n",
       " 0.1802264318366566,\n",
       " 0.1243843095671121,\n",
       " 0.22567563518590983,\n",
       " 0.2966838799123028,\n",
       " 0.4837361641049678,\n",
       " 0.36335606111719027,\n",
       " 0.24941294169690895,\n",
       " 0.19622035578226596,\n",
       " 0.11962648605706518,\n",
       " 0.15533464365643904,\n",
       " 0.2162131885600711,\n",
       " 0.23748526578664278,\n",
       " 0.13863323542035286,\n",
       " 0.33065943910495077,\n",
       " 0.13804052864217542,\n",
       " 0.11980550946457917,\n",
       " 0.26487742884650795,\n",
       " 0.20069285252380814,\n",
       " 0.24533433783687064,\n",
       " 0.14781409951013555,\n",
       " 0.3134905230831563,\n",
       " 0.04648596111676262,\n",
       " 0.38283958114787975,\n",
       " 0.1414912775663198,\n",
       " 0.21543397190717284,\n",
       " 0.12559458402493628,\n",
       " 0.11915214456962114,\n",
       " 0.08798675509066214,\n",
       " 0.15188663734445304,\n",
       " 0.14323246527317202,\n",
       " 0.1482330620448394,\n",
       " 0.12334067060805294,\n",
       " 2.731148640577885e-14,\n",
       " 0.11659713324292119,\n",
       " 0.17982336985480393,\n",
       " 0.22800001739041476,\n",
       " 0.10779498423831169,\n",
       " 0.181459451265921,\n",
       " 0.4049998418138723,\n",
       " 0.16771851585913963,\n",
       " 0.12734585190364087,\n",
       " 0.19706833254825154,\n",
       " 0.18198244020795934,\n",
       " 0.40183474338184266,\n",
       " 0.49263250106866385,\n",
       " 0.1533581315607211,\n",
       " 0.1655042724596902,\n",
       " 0.14395274432051297,\n",
       " 0.07712312575989644,\n",
       " 0.193974834048943,\n",
       " 0.20890356379498715,\n",
       " 0.311597615991298,\n",
       " 0.2184861242331297,\n",
       " 0.14951032725151983,\n",
       " 0.13632799508804727,\n",
       " 0.2329426554806426,\n",
       " 0.20421073143534518,\n",
       " 0.0034085698548268484,\n",
       " 0.13086267020286552,\n",
       " 0.2635168812116134,\n",
       " 0.1462384866592944,\n",
       " 0.4405719781579621,\n",
       " 0.23947486432466922,\n",
       " 0.3317749857041801,\n",
       " 0.24581840486935974,\n",
       " 0.2726953048158061,\n",
       " 0.1309306398197486,\n",
       " 0.22549761509371247,\n",
       " 0.23933851765051956,\n",
       " 0.15711666007192363,\n",
       " 0.09069355402825455,\n",
       " 0.4593445449183783,\n",
       " 0.18670732439197416,\n",
       " 0.3701949008197155,\n",
       " 0.29070372250068954,\n",
       " 0.25017651927820717,\n",
       " 0.17103401518380634,\n",
       " 0.15746899879425247,\n",
       " 0.3112004339043745,\n",
       " 0.4858305342980419,\n",
       " 0.11770574267346952,\n",
       " 0.3346306117781945,\n",
       " 0.1745631531587863,\n",
       " 0.4967262853524441,\n",
       " 0.11097777355600227,\n",
       " 0.1216611972156061,\n",
       " 0.09538281315596797,\n",
       " 0.21399308338407005,\n",
       " 0.1755305329515536,\n",
       " 0.18536267252980804,\n",
       " 0.28417522718644045,\n",
       " 0.14546692932558827,\n",
       " 0.4233527993448372,\n",
       " 0.18585478463095673,\n",
       " 0.269289460644126,\n",
       " 0.1837178706781685,\n",
       " 0.1461685650563156,\n",
       " 0.18310430666541894,\n",
       " 0.14229397423243967,\n",
       " 0.1014523847425357,\n",
       " 0.46060468591186643,\n",
       " 0.07539659761049036,\n",
       " 0.1094445478292422,\n",
       " 0.39018571202500796,\n",
       " 0.2686664271316409,\n",
       " 0.38266278016586536,\n",
       " 0.12008887602673421,\n",
       " 0.24793451370763075,\n",
       " 4.8183679268731794e-14,\n",
       " 0.2048375167343327,\n",
       " 0.1074580683771092,\n",
       " 0.14631107922215036,\n",
       " 0.18593419684661508,\n",
       " 0.23563913119474783,\n",
       " 0.19960596314069473,\n",
       " 0.0631803189656166,\n",
       " 0.13148049033575965,\n",
       " 0.1507950969083212,\n",
       " 0.1486635242804943,\n",
       " 0.20685324021391885,\n",
       " 0.06585531022119807,\n",
       " 0.02980103669757428,\n",
       " 0.1447115511675071,\n",
       " 0.288393721432334,\n",
       " 0.2967135919864822,\n",
       " 0.18799748446908257,\n",
       " 0.2805364746788483,\n",
       " 0.13080312132351782,\n",
       " 0.2526777740086036,\n",
       " 0.35430446519126746,\n",
       " 0.13084124786852813,\n",
       " 0.1929553791188822,\n",
       " 0.336762605947331,\n",
       " 0.10878934629655103,\n",
       " 0.27544828809788496,\n",
       " 0.08595403557964201,\n",
       " 0.22007418446239602,\n",
       " 0.0012026707216863386,\n",
       " 0.19457140677074444,\n",
       " 0.13087908470497556,\n",
       " 0.10616780085192386,\n",
       " 0.17361484252770296,\n",
       " 0.48679991154930913,\n",
       " 0.12508568630461733,\n",
       " 0.2836555818902837,\n",
       " 0.18370297040995912,\n",
       " 0.1558431597264671,\n",
       " 0.1734562687960586,\n",
       " 0.00396516542615688,\n",
       " 0.24107414490064344,\n",
       " 0.23636697345188107,\n",
       " 0.10666047108171754,\n",
       " 0.25370471914172,\n",
       " 0.14325448746329553,\n",
       " 0.29295135950158857,\n",
       " 0.14542110459657392,\n",
       " 0.16304166388636407,\n",
       " 0.2628903430954714,\n",
       " 0.37654289758437853,\n",
       " 0.10515089202140479,\n",
       " 0.3002841433200945,\n",
       " 0.1732745566508459,\n",
       " 0.28347375150719756,\n",
       " 0.0794917858504675,\n",
       " 0.20289989865688104,\n",
       " 0.17896402641644185,\n",
       " 0.21757890182626713,\n",
       " 0.14538264507617882,\n",
       " 0.403828936469438,\n",
       " 0.1597738593488787,\n",
       " 0.1811648894449931,\n",
       " 0.3350187871201499,\n",
       " 0.1656587115519092,\n",
       " 0.17835279379714294,\n",
       " 0.1561964143813123,\n",
       " 0.1835527684219336,\n",
       " 0.4287106354314357,\n",
       " 0.22916279342815066,\n",
       " 0.11840487119741054,\n",
       " 0.14463894896434032,\n",
       " 0.24239095569338717,\n",
       " 0.23316125782974276,\n",
       " 0.18417512184509868,\n",
       " 0.1712531114377812,\n",
       " 0.23250859622409908,\n",
       " 0.07839248168072041,\n",
       " 0.20630428084322427,\n",
       " 0.17820704842069596,\n",
       " 0.13086032696754898,\n",
       " 0.30831334069804206,\n",
       " 0.15924529535183482,\n",
       " 0.35630381439837766,\n",
       " 0.13563836907191687,\n",
       " 0.2103064541710357,\n",
       " 0.2502353619915501,\n",
       " 0.1564508555778783,\n",
       " 0.16588755440004196,\n",
       " 0.2606709775308882,\n",
       " 0.28844167210399185,\n",
       " 0.17117485339322316,\n",
       " 0.18005510139493802,\n",
       " 0.34237723148861915,\n",
       " 0.16149294125227492,\n",
       " 0.1107125497734393,\n",
       " 0.23541908907565756,\n",
       " 0.21599141314227382,\n",
       " 0.15762170406947856,\n",
       " 0.11438648151143976,\n",
       " 0.21975418747361192,\n",
       " 0.43816060014241986,\n",
       " 0.10114819785676252,\n",
       " 0.1591712339473298,\n",
       " 0.26204427853111306,\n",
       " 0.09158063839150565,\n",
       " 0.20106366102703482,\n",
       " 0.2612258447867326,\n",
       " 0.22184789240386613,\n",
       " 0.12425556114657592,\n",
       " 0.2286812942706778,\n",
       " 0.12380001040111505,\n",
       " 0.12350192695687306,\n",
       " 0.23512830944655227,\n",
       " 0.18122998225208206,\n",
       " 0.24706722593863445,\n",
       " 0.22220648928125236,\n",
       " 0.20464849746882474,\n",
       " 0.13619088343823516,\n",
       " 0.43000159638803054,\n",
       " 0.2359631146751318,\n",
       " 0.12319630639519413,\n",
       " 0.15479838911050406,\n",
       " 0.1249548303752118,\n",
       " 0.12695594850320874,\n",
       " 0.11862723991640055,\n",
       " 0.18034625620506975,\n",
       " 0.2577384012468859,\n",
       " 0.21017769520105653,\n",
       " 0.468211543558409,\n",
       " 0.09912537866548482,\n",
       " 0.2985194195523584,\n",
       " 0.332424351948303,\n",
       " 0.12884373414225453,\n",
       " 0.19144219347241254,\n",
       " 0.47850483821278156,\n",
       " 0.17078267626146262,\n",
       " 0.10958106010089971,\n",
       " 0.23735253364147285,\n",
       " 0.13498648104255176,\n",
       " 0.3919679228896812,\n",
       " 0.268626763155683,\n",
       " 0.14741081071742013,\n",
       " 0.1495015632659433,\n",
       " 0.24199441583356093,\n",
       " 0.2552188430961189,\n",
       " 0.14993097392087953,\n",
       " 0.1907340312073655,\n",
       " 0.22272078997615896,\n",
       " 0.1763967306421744,\n",
       " 0.3463430788192895,\n",
       " 0.26750072299116173,\n",
       " 0.18310905528389188,\n",
       " 0.0933009310728149,\n",
       " 0.22974489421675215,\n",
       " 0.09593910682856002,\n",
       " 0.18216804435671963,\n",
       " 0.3124022409653208,\n",
       " 0.13347441835295282,\n",
       " 0.383446171463484,\n",
       " 0.39628978427309214,\n",
       " 0.38919804699545313,\n",
       " 0.1337888268247315,\n",
       " 0.23493917879858447,\n",
       " 0.3315406580210928,\n",
       " 0.16122871044374903,\n",
       " 0.18362374412493487,\n",
       " 0.20905955807468155,\n",
       " 0.2517973900208761,\n",
       " 0.1395493239890606,\n",
       " 0.12926122165557324,\n",
       " 0.29218585431812016,\n",
       " 0.1429678213148172,\n",
       " 0.22074871478150565,\n",
       " 0.07611548102607496,\n",
       " 0.21578328505897668,\n",
       " 0.17691538984807498,\n",
       " 0.23727189315831823,\n",
       " 0.34412789279448824,\n",
       " 0.23361864978985958,\n",
       " 0.19253976023884128,\n",
       " 0.4107344646032126,\n",
       " 0.2567192448503382,\n",
       " 0.1417286451003575,\n",
       " 0.12294670402476073,\n",
       " 0.02691334019759628,\n",
       " 0.1184892261946224,\n",
       " 0.08634225882757852,\n",
       " 0.18406398766833532,\n",
       " 0.2529736771151518,\n",
       " 0.14520661967911308,\n",
       " 0.14466209405396319,\n",
       " 0.42986407507676416,\n",
       " 0.020473639760210105,\n",
       " 0.2533039866079607,\n",
       " 0.08316041673944961,\n",
       " 0.39768448245916405,\n",
       " 0.13776604776974533,\n",
       " 0.23746400311110188,\n",
       " 0.44562404810698897,\n",
       " 0.2139654595766258,\n",
       " 0.24786291099461163,\n",
       " 0.16064126449611638,\n",
       " 0.1053966760559151,\n",
       " 0.11500440403093802,\n",
       " 0.12404057642553507,\n",
       " 0.4339392650214786,\n",
       " 0.148872409044497,\n",
       " 0.17002926669165672,\n",
       " 0.23394736948566194,\n",
       " 0.12578019594308618,\n",
       " 0.14483537155858647,\n",
       " 0.19413020767188174,\n",
       " 0.2995161937363657,\n",
       " 0.12958332745944062,\n",
       " 0.23212501770740135,\n",
       " 0.25331778999782195,\n",
       " 0.17061789516644016,\n",
       " 0.256579137562759,\n",
       " 0.17293883370495647,\n",
       " 0.2871445931305837,\n",
       " 0.1791457209918605,\n",
       " 0.2298936217074017,\n",
       " 0.17498429690695577,\n",
       " 0.17554315892974226,\n",
       " 0.17982711394934292,\n",
       " 0.16151514895059285,\n",
       " 0.3886434799901767,\n",
       " 0.3034205789863587,\n",
       " 0.29389481405609513,\n",
       " 0.17423464755353021,\n",
       " 0.23187193243964377,\n",
       " 0.12180369030611025,\n",
       " 0.13600884698037752,\n",
       " 0.24443878293564408,\n",
       " 0.06288878600836319,\n",
       " 0.10033542402841919,\n",
       " 0.13825586967131304,\n",
       " 0.1435191667010434,\n",
       " 0.1932882613242611,\n",
       " 0.10876770291818492,\n",
       " 0.19332042998897028,\n",
       " 0.17301755997518234,\n",
       " 0.24575484911396261,\n",
       " 0.12238564004834482,\n",
       " 0.46267659755693763,\n",
       " 0.47232462594465385,\n",
       " 0.16802732820741256,\n",
       " 0.3349708977132142,\n",
       " 0.4228385920055472,\n",
       " 0.19151580149689712,\n",
       " 0.1112868889038646,\n",
       " 0.43633274474935513,\n",
       " 0.28840989348912605,\n",
       " 0.14289333088531386,\n",
       " 0.14577832405772143,\n",
       " 0.1310099154148715,\n",
       " 0.26742950650992786,\n",
       " 0.17715098356130124,\n",
       " 0.48116383779375915,\n",
       " 0.1728395075371031,\n",
       " 0.09908095607299955,\n",
       " 0.18859270349075236,\n",
       " 0.2358977029536292,\n",
       " 0.2802045768413076,\n",
       " 0.2260929134927883,\n",
       " 0.2761697277629243,\n",
       " 0.20998236488274302,\n",
       " 0.022705370955345416,\n",
       " 0.1430266233836699,\n",
       " 0.35478354907538456,\n",
       " 0.22341843235198403,\n",
       " 0.46307272366187335,\n",
       " 0.1823859869517252,\n",
       " 0.12471523474906165,\n",
       " 0.4597522107127653,\n",
       " 0.2792884912971212,\n",
       " 0.2758704706382701,\n",
       " 0.20405742040313568,\n",
       " 0.49286339470551954,\n",
       " 0.1546588149189906,\n",
       " 0.2626831095308025,\n",
       " 0.16807143541882338,\n",
       " 0.16405854238425557,\n",
       " 0.2934475769106325,\n",
       " 0.1250786524880135,\n",
       " 0.15521863390123644,\n",
       " 0.2791607577975599,\n",
       " 0.20276679225926061,\n",
       " 0.3570852821376821,\n",
       " 0.18881604527149293,\n",
       " 0.25003124630135526,\n",
       " 0.09530896772472655,\n",
       " 0.21853211312408868,\n",
       " 0.30002499674913063,\n",
       " 0.185390670243857,\n",
       " 0.19947001581473395,\n",
       " 0.26426150033199075,\n",
       " 0.22065372731933067,\n",
       " 0.21177841872818404,\n",
       " 0.2021176719890107,\n",
       " 0.1368695396237058,\n",
       " 0.1774133302243777,\n",
       " 0.18957891062385346,\n",
       " 0.1634499660688959,\n",
       " 0.2838255429055323,\n",
       " 0.20277452812674043,\n",
       " 0.23539602138674126,\n",
       " 0.474692912574332,\n",
       " 0.21064832306726672,\n",
       " 0.026559065221227085,\n",
       " 0.08706139444267798,\n",
       " 0.16144385463445862,\n",
       " 0.2571979709564743,\n",
       " 0.2994567824941939,\n",
       " 0.24761914690974177,\n",
       " 0.13122841047877648,\n",
       " 0.27127905970415794,\n",
       " 0.13165731830589397,\n",
       " 0.19632024765402617,\n",
       " 0.1607839520790416,\n",
       " 0.2534531737584023,\n",
       " 0.26988167183519435,\n",
       " 0.32704314162338244,\n",
       " 0.16081174139972218,\n",
       " 0.1878505388929225,\n",
       " 0.20447048577206503,\n",
       " 0.19116660140206382,\n",
       " 0.17510717438145496,\n",
       " 0.12167419796870052,\n",
       " 0.10951356068298346,\n",
       " 0.40339470415148415,\n",
       " 0.4589719374846528,\n",
       " 0.10370243142372705,\n",
       " 0.28100441533701837,\n",
       " 0.2107562198395065,\n",
       " 0.26550103055585433,\n",
       " 0.03363358942124073,\n",
       " 0.08286898147438622,\n",
       " 0.12520091668967104,\n",
       " 0.17534713271252347,\n",
       " 0.10906142733963371,\n",
       " 0.20222819211453452,\n",
       " 0.1443014605740816,\n",
       " 1.84297022087776e-14,\n",
       " 0.21332102722080018,\n",
       " 0.18120847519007405,\n",
       " 0.10983514461474464,\n",
       " 0.288086678231297,\n",
       " 0.10609249126571624,\n",
       " 0.2887276350257974,\n",
       " 0.1523794027889827,\n",
       " 0.0934635478972907,\n",
       " 0.12564724623409493,\n",
       " 0.18164905782821295,\n",
       " 0.2744365907334839,\n",
       " 0.15895135628160192,\n",
       " 0.3015879906949296,\n",
       " 0.2427476991946712,\n",
       " 0.16162374327504375,\n",
       " 0.12415739678708418,\n",
       " 0.27734243081423704,\n",
       " 0.2568683628973043,\n",
       " 0.19640980482320497,\n",
       " 0.42140626708937656,\n",
       " 0.17446286640742353,\n",
       " 0.1386825287939123,\n",
       " 0.0002355801149606096,\n",
       " 0.2935932171840907,\n",
       " 0.28616227181968196,\n",
       " 0.2055306065228542,\n",
       " 0.12124863808798066,\n",
       " 0.11916856248370117,\n",
       " 0.2274305093204447,\n",
       " 0.13424015491749927,\n",
       " 0.19828340846863818,\n",
       " 0.20736047293168527,\n",
       " 0.16308562736755736,\n",
       " 0.241295552633027,\n",
       " 0.1360646920373745,\n",
       " 0.09408622670093046,\n",
       " 0.3366137070321714,\n",
       " 0.15640821872109856,\n",
       " 0.16283409654981423,\n",
       " 0.26825995055324897,\n",
       " 0.11617005724985707,\n",
       " 0.17267103214742502,\n",
       " 0.20260482322003895,\n",
       " 0.13714623956963293,\n",
       " 0.16458587458047982,\n",
       " 0.24552383484889884,\n",
       " 0.4946701708635878,\n",
       " 0.17802637040290956,\n",
       " 0.15800856740008107,\n",
       " 0.06651643397893248,\n",
       " 0.13683775465296527,\n",
       " 0.18304503994455065,\n",
       " 0.10575423512214993,\n",
       " 0.17819891585165543,\n",
       " 0.22665264499240978,\n",
       " 0.2366479823464647,\n",
       " 0.11203440673006448,\n",
       " 0.16867831902682104,\n",
       " 0.45304450908214433,\n",
       " 0.16277444171265434,\n",
       " 0.13314617558062566,\n",
       " 0.13161354853331952,\n",
       " 0.24439800064077122,\n",
       " 0.18823224378204156,\n",
       " 0.2994577303062109,\n",
       " 0.16062519102522702,\n",
       " 0.19705756249442477,\n",
       " 0.23153528199589915,\n",
       " 0.328245277032028,\n",
       " 0.1476216005884823,\n",
       " 0.13191136416245336,\n",
       " 0.22684189841201763,\n",
       " 0.09476775199513493,\n",
       " 0.15332206260324047,\n",
       " 0.24435218395155922,\n",
       " 0.12619590487277085,\n",
       " 0.42111407960803754,\n",
       " 0.08399847024193806,\n",
       " 0.27458299255283675,\n",
       " 0.19764062957252915,\n",
       " 0.4641064432688498,\n",
       " 0.4583922765089735,\n",
       " 0.20360184814257778,\n",
       " 0.06159839635167319,\n",
       " 0.16531508482343477,\n",
       " 0.19389466960275412,\n",
       " 0.13117399143009612,\n",
       " 0.21380819861905154,\n",
       " 0.19862920267434178,\n",
       " 0.20527029498367802,\n",
       " 0.097126105049816,\n",
       " 0.14770427609877484,\n",
       " 0.3068119646839701,\n",
       " 0.10061817780255343,\n",
       " 0.19136480067950987,\n",
       " 0.19714603902880112,\n",
       " 0.25252441119798197,\n",
       " 0.2226860015181982,\n",
       " 0.016322042588020613,\n",
       " 0.26005727688351554,\n",
       " 0.25311676308318115,\n",
       " 0.41517780797214077,\n",
       " 0.13276139747042245,\n",
       " 0.2554876325191824,\n",
       " 0.2518214046063903,\n",
       " 0.25819919390755947,\n",
       " 0.31419914042232316,\n",
       " 0.2802336921303352,\n",
       " 0.07347851683152137,\n",
       " 0.24384295023427638,\n",
       " 0.19734320924192014,\n",
       " 0.3277103872607843,\n",
       " 0.24845481261102392,\n",
       " 0.17600903421190991,\n",
       " 0.26337201289333617,\n",
       " 0.23829362182803404,\n",
       " 0.29968390226820607,\n",
       " 0.10899944025070647,\n",
       " 0.12644208876042606,\n",
       " 0.47332424410595836,\n",
       " 0.2056985477133298,\n",
       " 0.2093840443750803,\n",
       " 0.14462297167824162,\n",
       " 0.12824462225652472,\n",
       " 0.19968244654765652,\n",
       " 0.11929366870740175,\n",
       " 0.1897586322013487,\n",
       " 0.07673665647996407,\n",
       " 0.3764889064072572,\n",
       " 0.44500076099028585,\n",
       " 0.17780675090180853,\n",
       " 0.215665097050874,\n",
       " 0.3324622119868915,\n",
       " 0.1824808738771419,\n",
       " ...]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>T0</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.812576</td>\n",
       "      <td>0.781302</td>\n",
       "      <td>0.806498</td>\n",
       "      <td>0.808156</td>\n",
       "      <td>0.812576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.440264</td>\n",
       "      <td>0.407662</td>\n",
       "      <td>0.405029</td>\n",
       "      <td>0.448539</td>\n",
       "      <td>0.440264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statistical_parity_difference</th>\n",
       "      <td>-0.066572</td>\n",
       "      <td>-0.063687</td>\n",
       "      <td>-0.059564</td>\n",
       "      <td>-0.074546</td>\n",
       "      <td>-0.066572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal_opportunity_difference</th>\n",
       "      <td>-0.032983</td>\n",
       "      <td>-0.002994</td>\n",
       "      <td>-0.027154</td>\n",
       "      <td>-0.039369</td>\n",
       "      <td>-0.032983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_abs_odds_difference</th>\n",
       "      <td>0.020277</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>0.027194</td>\n",
       "      <td>0.020277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disparate_impact</th>\n",
       "      <td>-1.079553</td>\n",
       "      <td>-0.643027</td>\n",
       "      <td>-1.106966</td>\n",
       "      <td>-1.022627</td>\n",
       "      <td>-1.079553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theil_index</th>\n",
       "      <td>0.203083</td>\n",
       "      <td>0.214830</td>\n",
       "      <td>0.211923</td>\n",
       "      <td>0.201688</td>\n",
       "      <td>0.203083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               classifier        T0        T1        T2  \\\n",
       "accuracy                         0.812576  0.781302  0.806498  0.808156   \n",
       "f1                               0.440264  0.407662  0.405029  0.448539   \n",
       "statistical_parity_difference   -0.066572 -0.063687 -0.059564 -0.074546   \n",
       "equal_opportunity_difference    -0.032983 -0.002994 -0.027154 -0.039369   \n",
       "average_abs_odds_difference      0.020277  0.010964  0.017036  0.027194   \n",
       "disparate_impact                -1.079553 -0.643027 -1.106966 -1.022627   \n",
       "theil_index                      0.203083  0.214830  0.211923  0.201688   \n",
       "\n",
       "                                     T3  \n",
       "accuracy                       0.812576  \n",
       "f1                             0.440264  \n",
       "statistical_parity_difference -0.066572  \n",
       "equal_opportunity_difference  -0.032983  \n",
       "average_abs_odds_difference    0.020277  \n",
       "disparate_impact              -1.079553  \n",
       "theil_index                    0.203083  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "final_result = pd.DataFrame(final_metrics)\n",
    "final_result[3] = np.log(final_result[3])\n",
    "final_result = final_result.transpose()\n",
    "acc_f1 = pd.DataFrame(accuracy)\n",
    "acc_f1['f1'] = f1\n",
    "acc_f1 = pd.DataFrame(acc_f1).transpose()\n",
    "acc = acc_f1.rename(index={0: 'accuracy', 1: 'f1'})\n",
    "final_result = final_result.rename(index={0: 'statistical_parity_difference', 1: 'equal_opportunity_difference', 2: 'average_abs_odds_difference', 3: 'disparate_impact', 4: 'theil_index'})\n",
    "final_result = pd.concat([acc,final_result])\n",
    "final_result.columns = ['T' + str(col) for col in final_result.columns]\n",
    "final_result.insert(0, \"classifier\", final_result['T' + str(len(list_estimators) - 1)])   ##Add final metrics add the beginning of the df\n",
    "#final_result.to_csv('../../Results/VotingClassifier/' + nb_fname + '.csv')\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
