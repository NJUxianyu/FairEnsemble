{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "import matplotlib.patches as patches\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "#from packages import *\n",
    "#from ml_fairness import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "nb_path = ipynbname.path()\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
    "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
    "\n",
    "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
    "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:\n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "    \n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4,0.25]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8,0]\n",
    "    bottom = [-1,-1,-1,0,0]\n",
    "    top = [1,1,1,2,1]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fair_metrics_and_plot(data, model, plot=False, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    # fair_metrics function available in the metrics.py file\n",
    "    fair = fair_metrics(data, pred)\n",
    "\n",
    "    if plot:\n",
    "        # plot_fair_metrics function available in the visualisations.py file\n",
    "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
    "        plot_fair_metrics(fair)\n",
    "        display(fair)\n",
    "    \n",
    "    return fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig_train = pd.read_pickle('../../Results/XGBoost/3-a-statistical-analysis-ml-workflow-of-titanic_Train.pkl')\n",
    "data_orig_test = pd.read_pickle('../../Results/XGBoost/3-a-statistical-analysis-ml-workflow-of-titanic_Test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\GermanCredit\\Results\\XGBoost\\\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "i = 0\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "TM_dict = dict()\n",
    "paths = [\"..\\\\..\\\\Titanic\\\\Results\\\\XGBoost\\\\\"]\n",
    "for path in paths:\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "    for f in csv_files:\n",
    "        if i % 3 == 0 or i == 0:\n",
    "            i = i + 1\n",
    "            continue\n",
    "        TM_dict[f] = pd.read_pickle(f)\n",
    "        i = i + 1\n",
    "        \n",
    "i = 0\n",
    "AC_dict = dict()    \n",
    "paths = [\"..\\\\..\\\\AdultNotebook\\\\Results\\\\XGBoost\\\\\"]\n",
    "for path in paths:\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "\n",
    "    for f in csv_files:\n",
    "        if i % 3 == 0 or i == 0:\n",
    "            i = i + 1\n",
    "            continue\n",
    "        AC_dict[f] = pd.read_pickle(f)\n",
    "        i = i + 1\n",
    "        \n",
    "i = 0\n",
    "BM_dict = dict()\n",
    "paths = [\"..\\\\..\\\\BankMarketingNotebook\\\\Results\\\\XGBoost\\\\\"]\n",
    "for path in paths:\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "    for f in csv_files:\n",
    "        if i % 3 == 0 or i == 0:\n",
    "            i = i + 1\n",
    "            continue\n",
    "            i = i + 1\n",
    "        BM_dict[f] = pd.read_pickle(f)\n",
    "        i = i + 1\n",
    "        \n",
    "i = 0\n",
    "GC_dict = dict()\n",
    "paths = [\"..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\\"]\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "    for f in csv_files:\n",
    "        if i % 3 == 0 or i == 0:\n",
    "            i = i + 1\n",
    "            continue\n",
    "        GC_dict[f] = pd.read_pickle(f)\n",
    "        i = i + 1\n",
    "    \n",
    "    \n",
    "models = [TM_dict, AC_dict, BM_dict, GC_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\0-predicting-credit-risk-model-pipeline_Test.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\0-predicting-credit-risk-model-pipeline_Train.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\1-credit-risk-evaluation_Test.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\1-credit-risk-evaluation_Train.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\10-credit-risk-assessment_Test.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\10-credit-risk-assessment_Train.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\3-bank-credit-data-clustering-modelling_Test.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\3-bank-credit-data-clustering-modelling_Train.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\6-credit-risk-modelling-german-data_Test.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\6-credit-risk-modelling-german-data_Train.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\7-predicting-credit-risk-model-pipeline-60deb1_Test.pkl', '..\\\\..\\\\GermanCredit\\\\Results\\\\XGBoost\\\\7-predicting-credit-risk-model-pipeline-60deb1_Train.pkl'])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GC_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "nb_path = ipynbname.path()\n",
    "\n",
    "Overall_metrics = []\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for dataset in models:\n",
    "    \n",
    "    for i in range (0, len(dataset.keys()),2):\n",
    "        print(i)\n",
    "        data_orig_train = pd.read_pickle(list(dataset.keys())[i+1])\n",
    "        data_orig_test = pd.read_pickle(list(dataset.keys())[i])\n",
    "        \n",
    "        X_train = data_orig_train.features\n",
    "        y_train = data_orig_train.labels.ravel()\n",
    "\n",
    "        X_test = data_orig_test.features\n",
    "        y_test = data_orig_test.labels.ravel()\n",
    "        \n",
    "        \n",
    "        final_metrics = []\n",
    "        accuracy = []\n",
    "        f1= []\n",
    "\n",
    "        for i in range(0,10):\n",
    "\n",
    "            model = AdaBoostClassifier(n_estimators= 200, base_estimator=DecisionTreeClassifier(max_depth = 6))\n",
    "\n",
    "            mdl = model.fit(X_train, y_train)\n",
    "            yy = mdl.predict(X_test)\n",
    "            accuracy.append(accuracy_score(y_test, yy))\n",
    "            f1.append(f1_score(y_test, yy))\n",
    "            fair = get_fair_metrics_and_plot(data_orig_test, mdl)                           \n",
    "            fair_list = fair.iloc[1].tolist()\n",
    "            fair_list[3] = np.log(fair_list[3])\n",
    "            final_metrics.append(fair_list)\n",
    "\n",
    "        Overall_metrics.append(list(sum(x) for x in final_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "nb_path = ipynbname.path()\n",
    "\n",
    "Overall_metrics = []\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for dataset in models:\n",
    "    \n",
    "    for i in range (0, len(dataset.keys()),2):\n",
    "        print(i)\n",
    "        data_orig_train = pd.read_pickle(list(dataset.keys())[i+1])\n",
    "        data_orig_test = pd.read_pickle(list(dataset.keys())[i])\n",
    "        \n",
    "        X_train = data_orig_train.features\n",
    "        y_train = data_orig_train.labels.ravel()\n",
    "\n",
    "        X_test = data_orig_test.features\n",
    "        y_test = data_orig_test.labels.ravel()\n",
    "        \n",
    "        \n",
    "        final_metrics = []\n",
    "        accuracy = []\n",
    "        f1= []\n",
    "\n",
    "\n",
    "        model =  XGBClassifier(n_estimators= 100, max_depth = 12)\n",
    "\n",
    "        mdl = model.fit(X_train, y_train)\n",
    "        yy = mdl.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, yy))\n",
    "        f1.append(f1_score(y_test, yy))\n",
    "        fair = get_fair_metrics_and_plot(data_orig_test, mdl)                           \n",
    "        fair_list = fair.iloc[1].tolist()\n",
    "        fair_list[3] = np.log(fair_list[3])\n",
    "        final_metrics.append(fair_list)\n",
    "\n",
    "        Overall_metrics.append(sum(final_metrics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.6232558954364125,\n",
       " -2.768991219915285,\n",
       " -1.90241331974715,\n",
       " -2.5542659259507157,\n",
       " -1.8998885505291934,\n",
       " -2.0470509539848933,\n",
       " -1.16852915889595,\n",
       " -1.1196638544511306,\n",
       " -1.1657535293805215,\n",
       " -1.226294233515039,\n",
       " -1.1164914958279295,\n",
       " -1.1492301020397553,\n",
       " 1.4193874325987281,\n",
       " 0.12605755386485482,\n",
       " 1.6340773242888629,\n",
       " 0.74508017075025,\n",
       " 1.4464837514843225,\n",
       " 1.3609889451926762,\n",
       " 0.2007528221188386,\n",
       " 0.25006484704655285,\n",
       " 0.033907917448325525,\n",
       " 0.8508798814271157,\n",
       " -0.26064390563617634,\n",
       " 0.015049577821735965]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overall_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"output_XGB.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(Overall_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "final_metrics = []\n",
    "accuracy = []\n",
    "f1= []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    model = AdaBoostClassifier(n_estimators= 100, base_estimator=DecisionTreeClassifier(max_depth = 6))\n",
    "    \n",
    "    mdl = model.fit(X_train, y_train)\n",
    "    yy = mdl.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, yy))\n",
    "    f1.append(f1_score(y_test, yy))\n",
    "    fair = get_fair_metrics_and_plot(data_orig_test, mdl)                           \n",
    "    fair_list = fair.iloc[1].tolist()\n",
    "    fair_list[3] = np.log(fair_list[3])\n",
    "    final_metrics.append(fair_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "total_bias = []\n",
    "total_accuracy = []\n",
    "total_f1 = []\n",
    "\n",
    "for i in range(0, len(final_metrics)):\n",
    "    total_bias.append(sum(final_metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011549209690540144\n",
      "0.7353333333333334\n",
      "0.8252532750235162\n"
     ]
    }
   ],
   "source": [
    "print(statistics.mean(total_bias))\n",
    "print(statistics.mean(accuracy))\n",
    "print(statistics.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.06686382349233941,\n",
       " -0.032197113378255704,\n",
       " 0.13329970781716197,\n",
       " -0.04039106355168129,\n",
       " -0.0903614754630932,\n",
       " -0.005973588955697912,\n",
       " -0.1342408232274386,\n",
       " -0.032078168116783656,\n",
       " -0.046484605931384565,\n",
       " 0.027204245871282562]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "final_metrics = []\n",
    "accuracy = []\n",
    "f1= []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    model = XGBClassifier(n_estimators= 100)\n",
    "    \n",
    "    mdl = model.fit(X_train, y_train)\n",
    "    yy = mdl.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, yy))\n",
    "    f1.append(f1_score(y_test, yy))\n",
    "    fair = get_fair_metrics_and_plot(data_orig_test, mdl)                           \n",
    "    fair_list = fair.iloc[1].tolist()\n",
    "    fair_list[3] = np.log(fair_list[3])\n",
    "    final_metrics.append(fair_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "final_metrics = []\n",
    "accuracy = []\n",
    "f1= []\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    model = GradientBoostingClassifier(n_estimators=500, max_depth = 6)\n",
    "    \n",
    "    mdl = model.fit(X_train, y_train)\n",
    "    yy = mdl.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_test, yy))\n",
    "    f1.append(f1_score(y_test, yy))\n",
    "    fair = get_fair_metrics_and_plot(data_orig_test, mdl)                           \n",
    "    fair_list = fair.iloc[1].tolist()\n",
    "    fair_list[3] = np.log(fair_list[3])\n",
    "    final_metrics.append(fair_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.07232441471571904,\n",
       "  -0.01570897064902843,\n",
       "  0.045733273203302116,\n",
       "  -0.09745956268446926,\n",
       "  0.18284246248733227],\n",
       " [-0.04703177257525082,\n",
       "  0.013435303844563928,\n",
       "  0.03323280343743351,\n",
       "  -0.06349995754152825,\n",
       "  0.1861513618079388],\n",
       " [-0.13858695652173914,\n",
       "  -0.0062009094667218045,\n",
       "  0.1508277274606336,\n",
       "  -0.18701482722570426,\n",
       "  0.15822187611741734],\n",
       " [-0.10827759197324416,\n",
       "  -0.05363786688714345,\n",
       "  0.08363711526175355,\n",
       "  -0.13861317717128535,\n",
       "  0.14662365468305202],\n",
       " [-0.0842391304347826,\n",
       "  -0.05839189747829676,\n",
       "  0.03677170631490595,\n",
       "  -0.10945659287982967,\n",
       "  0.15688678012369994],\n",
       " [-0.06751672240802675,\n",
       "  -0.003513848697808908,\n",
       "  0.05099934859132871,\n",
       "  -0.0912675924365482,\n",
       "  0.19135974138866416],\n",
       " [-0.08068561872909707,\n",
       "  0.0059942124844977185,\n",
       "  0.0787546819998246,\n",
       "  -0.10658354367784008,\n",
       "  0.16784233655869898],\n",
       " [-0.027591973244147083,\n",
       "  -0.01705250103348488,\n",
       "  0.0426171596076515,\n",
       "  -0.03562168312256171,\n",
       "  0.1690363168548286],\n",
       " [-0.05539297658862874,\n",
       "  -0.004857379082265356,\n",
       "  0.02515596226840544,\n",
       "  -0.07330807876351852,\n",
       "  0.17562186677348324],\n",
       " [-0.07817725752508364,\n",
       "  -0.035345183960314164,\n",
       "  0.04797562228318736,\n",
       "  -0.1005412292218774,\n",
       "  0.1580537563786151]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
