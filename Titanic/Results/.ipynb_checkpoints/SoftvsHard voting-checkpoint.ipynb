{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "import matplotlib.patches as patches\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "#from packages import *\n",
    "#from ml_fairness import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "nb_path = ipynbname.path()\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\n",
    "algo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n",
    "\n",
    "def add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n",
    "    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_metrics(dataset, pred, pred_is_dataset=False):\n",
    "    if pred_is_dataset:\n",
    "        dataset_pred = pred\n",
    "    else:\n",
    "        dataset_pred = dataset.copy()\n",
    "        dataset_pred.labels = pred\n",
    "    \n",
    "    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n",
    "    obj_fairness = [[0,0,0,1,0]]\n",
    "    \n",
    "    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n",
    "    \n",
    "    for attr in dataset_pred.protected_attribute_names:\n",
    "        idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "        \n",
    "        classified_metric = ClassificationMetric(dataset, \n",
    "                                                     dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "\n",
    "        acc = classified_metric.accuracy()\n",
    "\n",
    "        row = pd.DataFrame([[metric_pred.mean_difference(),\n",
    "                                classified_metric.equal_opportunity_difference(),\n",
    "                                classified_metric.average_abs_odds_difference(),\n",
    "                                metric_pred.disparate_impact(),\n",
    "                                classified_metric.theil_index()]],\n",
    "                           columns  = cols,\n",
    "                           index = [attr]\n",
    "                          )\n",
    "        fair_metrics = fair_metrics.append(row)    \n",
    "    \n",
    "    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n",
    "        \n",
    "    return fair_metrics\n",
    "\n",
    "def plot_fair_metrics(fair_metrics):\n",
    "    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left    =  0.125, \n",
    "        bottom  =  0.1, \n",
    "        right   =  0.9, \n",
    "        top     =  0.9, \n",
    "        wspace  =  .5, \n",
    "        hspace  =  1.1\n",
    "    )\n",
    "\n",
    "    y_title_margin = 1.2\n",
    "\n",
    "    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n",
    "    sns.set(style=\"dark\")\n",
    "\n",
    "    cols = fair_metrics.columns.values\n",
    "    obj = fair_metrics.loc['objective']\n",
    "    size_rect = [0.2,0.2,0.2,0.4,0.25]\n",
    "    rect = [-0.1,-0.1,-0.1,0.8,0]\n",
    "    bottom = [-1,-1,-1,0,0]\n",
    "    top = [1,1,1,2,1]\n",
    "    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n",
    "\n",
    "    display(Markdown(\"### Check bias metrics :\"))\n",
    "    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n",
    "    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n",
    "        display(Markdown(\"#### For the %s attribute :\"%attr))\n",
    "        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n",
    "        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n",
    "\n",
    "    for i in range(0,5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n",
    "        \n",
    "        for j in range(0,len(fair_metrics)-1):\n",
    "            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n",
    "            marg = -0.2 if val < 0 else 0.1\n",
    "            ax.text(a.get_x()+a.get_width()/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n",
    "\n",
    "        plt.ylim(bottom[i], top[i])\n",
    "        plt.setp(ax.patches, linewidth=0)\n",
    "        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n",
    "        plt.axhline(obj[i], color='black', alpha=0.3)\n",
    "        plt.title(cols[i])\n",
    "        ax.set_ylabel('')    \n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fair_metrics_and_plot(data, model, plot=False, model_aif=False):\n",
    "    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n",
    "    # fair_metrics function available in the metrics.py file\n",
    "    fair = fair_metrics(data, pred)\n",
    "\n",
    "    if plot:\n",
    "        # plot_fair_metrics function available in the visualisations.py file\n",
    "        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n",
    "        plot_fair_metrics(fair)\n",
    "        display(fair)\n",
    "    \n",
    "    return fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\GermanCredit\\Results\\VotingClassifier\\\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "i = 0\n",
    "# use glob to get all the csv files \n",
    "# in the folder\n",
    "TM_dict = dict()\n",
    "paths = [\"..\\\\..\\\\Titanic\\\\Results\\\\VotingClassifier\\\\\"]\n",
    "for path in paths:\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "    for f in csv_files:\n",
    "        TM_dict[f] = pd.read_pickle(f)\n",
    "        \n",
    "\n",
    "AC_dict = dict()    \n",
    "paths = [\"..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\\"]\n",
    "for path in paths:\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "\n",
    "    for f in csv_files:\n",
    "        AC_dict[f] = pd.read_pickle(f)\n",
    "        \n",
    "\n",
    "BM_dict = dict()\n",
    "paths = [\"..\\\\..\\\\BankMarketingNotebook\\\\Results\\\\VotingClassifier\\\\\"]\n",
    "for path in paths:\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "    for f in csv_files:\n",
    "        BM_dict[f] = pd.read_pickle(f)\n",
    "        \n",
    "GC_dict = dict()\n",
    "paths = [\"..\\\\..\\\\GermanCredit\\\\Results\\\\VotingClassifier\\\\\"]\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "    for f in csv_files:\n",
    "        GC_dict[f] = pd.read_pickle(f)\n",
    "    \n",
    "    \n",
    "models = [TM_dict, AC_dict, BM_dict, GC_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\0-income-prediction-84-369-accuracy.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\0-income-prediction-84-369-accuracy_Test.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\0-income-prediction-84-369-accuracy_Train.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\1-eda-and-income-predictions-86-78-accuracy.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\1-eda-and-income-predictions-86-78-accuracy_Test.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\1-eda-and-income-predictions-86-78-accuracy_Train.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\2-income-census-prediction-gradient-boosting-algos.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\2-income-census-prediction-gradient-boosting-algos_Test.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\2-income-census-prediction-gradient-boosting-algos_Train.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\3-beginner-s-income-prediction-89-score.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\3-beginner-s-income-prediction-89-score_Test.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\3-beginner-s-income-prediction-89-score_Train.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\4-prediction-ensemble-methods-multiple-ml-s-acc-86.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\4-prediction-ensemble-methods-multiple-ml-s-acc-86_Test.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\4-prediction-ensemble-methods-multiple-ml-s-acc-86_Train.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\5-income-prediction-xgbclassifier-auc-0-926.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\5-income-prediction-xgbclassifier-auc-0-926_Test.pkl', '..\\\\..\\\\AdultNotebook\\\\Results\\\\VotingClassifier\\\\5-income-prediction-xgbclassifier-auc-0-926_Train.pkl'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "soft\n",
      "3\n",
      "soft\n",
      "6\n",
      "soft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "soft\n",
      "12\n",
      "soft\n",
      "15\n",
      "soft\n",
      "0\n",
      "soft\n",
      "3\n",
      "soft\n",
      "[15:20:17] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { n_estimator } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "soft\n",
      "9\n",
      "soft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "soft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "soft\n",
      "0\n",
      "soft\n",
      "3\n",
      "soft\n",
      "6\n",
      "soft\n",
      "9\n",
      "soft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "soft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "soft\n",
      "0\n",
      "soft\n",
      "3\n",
      "soft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "soft\n",
      "9\n",
      "soft\n",
      "12\n",
      "soft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "soft\n"
     ]
    }
   ],
   "source": [
    "import ipynbname\n",
    "nb_fname = ipynbname.name()\n",
    "nb_path = ipynbname.path()\n",
    "\n",
    "Overall_metrics = []\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from csv import writer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for dataset in models:\n",
    "    \n",
    "    for i in range (0, len(dataset.keys()),3):\n",
    "        print(i)\n",
    "        data_orig_train = pd.read_pickle(list(dataset.keys())[i+2])\n",
    "        data_orig_test = pd.read_pickle(list(dataset.keys())[i+1])\n",
    "        model = pd.read_pickle(list(dataset.keys())[i])\n",
    "        model.set_params(**{'voting': 'soft'})\n",
    "        print(model.voting)\n",
    "        \n",
    "        X_train = data_orig_train.features\n",
    "        y_train = data_orig_train.labels.ravel()\n",
    "\n",
    "        X_test = data_orig_test.features\n",
    "        y_test = data_orig_test.labels.ravel()\n",
    "        \n",
    "        \n",
    "        final_metrics = []\n",
    "        accuracy = []\n",
    "        f1= []\n",
    "\n",
    "        \n",
    "        mdl = model.fit(X_train, y_train)\n",
    "        try:\n",
    "            yy = mdl.predict(X_test)\n",
    "        except Exception:\n",
    "            continue\n",
    "        accuracy.append(accuracy_score(y_test, yy))\n",
    "        f1.append(f1_score(y_test, yy))\n",
    "        fair = get_fair_metrics_and_plot(data_orig_test, mdl)                           \n",
    "        fair_list = fair.iloc[1].tolist()\n",
    "        fair_list[3] = np.log(fair_list[3])\n",
    "        final_metrics.append(fair_list)\n",
    "\n",
    "        Overall_metrics.append(list(sum(x) for x in final_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.950100897595755],\n",
       " [-3.069187238539814],\n",
       " [-2.5649280737829283],\n",
       " [-2.7289290890882674],\n",
       " [-2.742688783983291],\n",
       " [-2.383969668414516],\n",
       " [-1.1136321583993403],\n",
       " [-2.200685870428438],\n",
       " [-1.2700246545327036],\n",
       " [-1.442539325925548],\n",
       " [-1.4113798335785939],\n",
       " [-1.3043910501196991],\n",
       " [1.454318962403024],\n",
       " [1.6547155205779986],\n",
       " [0.7766088056544911],\n",
       " [0.08810634989978808],\n",
       " [1.5364375712287035],\n",
       " [0.7514995298969995],\n",
       " [0.021425591380663606],\n",
       " [-0.3156891524534373],\n",
       " [-0.08275798175722539],\n",
       " [-0.06965252217381004],\n",
       " [-0.13856655568922915]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"SoftVoting.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(Overall_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
